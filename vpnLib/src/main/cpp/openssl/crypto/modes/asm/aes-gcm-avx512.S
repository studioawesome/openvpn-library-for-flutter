
.globl	ossl_vaes_vpclmulqdq_capable
.type	ossl_vaes_vpclmulqdq_capable,@function
.align	32
ossl_vaes_vpclmulqdq_capable:
	movq	OPENSSL_ia32cap_P+8(%rip),%rcx

	movq	$6600291188736,%rdx
	xorl	%eax,%eax
	andq	%rdx,%rcx
	cmpq	%rdx,%rcx
	cmoveq	%rcx,%rax
	.byte	0xf3,0xc3
.size	ossl_vaes_vpclmulqdq_capable, .-ossl_vaes_vpclmulqdq_capable
.text	
.globl	ossl_aes_gcm_init_avx512
.type	ossl_aes_gcm_init_avx512,@function
.align	32
ossl_aes_gcm_init_avx512:
.cfi_startproc	
.byte	243,15,30,250
	vpxorq	%xmm16,%xmm16,%xmm16


	movl	240(%rdi),%eax
	cmpl	$9,%eax
	je	.Laes_128_otrDyyvmtrjBjDk
	cmpl	$11,%eax
	je	.Laes_192_otrDyyvmtrjBjDk
	cmpl	$13,%eax
	je	.Laes_256_otrDyyvmtrjBjDk
	jmp	.Lexit_aes_otrDyyvmtrjBjDk
.align	32
.Laes_128_otrDyyvmtrjBjDk:
	vpxorq	0(%rdi),%xmm16,%xmm16

	vaesenc	16(%rdi),%xmm16,%xmm16

	vaesenc	32(%rdi),%xmm16,%xmm16

	vaesenc	48(%rdi),%xmm16,%xmm16

	vaesenc	64(%rdi),%xmm16,%xmm16

	vaesenc	80(%rdi),%xmm16,%xmm16

	vaesenc	96(%rdi),%xmm16,%xmm16

	vaesenc	112(%rdi),%xmm16,%xmm16

	vaesenc	128(%rdi),%xmm16,%xmm16

	vaesenc	144(%rdi),%xmm16,%xmm16

	vaesenclast	160(%rdi),%xmm16,%xmm16
	jmp	.Lexit_aes_otrDyyvmtrjBjDk
.align	32
.Laes_192_otrDyyvmtrjBjDk:
	vpxorq	0(%rdi),%xmm16,%xmm16

	vaesenc	16(%rdi),%xmm16,%xmm16

	vaesenc	32(%rdi),%xmm16,%xmm16

	vaesenc	48(%rdi),%xmm16,%xmm16

	vaesenc	64(%rdi),%xmm16,%xmm16

	vaesenc	80(%rdi),%xmm16,%xmm16

	vaesenc	96(%rdi),%xmm16,%xmm16

	vaesenc	112(%rdi),%xmm16,%xmm16

	vaesenc	128(%rdi),%xmm16,%xmm16

	vaesenc	144(%rdi),%xmm16,%xmm16

	vaesenc	160(%rdi),%xmm16,%xmm16

	vaesenc	176(%rdi),%xmm16,%xmm16

	vaesenclast	192(%rdi),%xmm16,%xmm16
	jmp	.Lexit_aes_otrDyyvmtrjBjDk
.align	32
.Laes_256_otrDyyvmtrjBjDk:
	vpxorq	0(%rdi),%xmm16,%xmm16

	vaesenc	16(%rdi),%xmm16,%xmm16

	vaesenc	32(%rdi),%xmm16,%xmm16

	vaesenc	48(%rdi),%xmm16,%xmm16

	vaesenc	64(%rdi),%xmm16,%xmm16

	vaesenc	80(%rdi),%xmm16,%xmm16

	vaesenc	96(%rdi),%xmm16,%xmm16

	vaesenc	112(%rdi),%xmm16,%xmm16

	vaesenc	128(%rdi),%xmm16,%xmm16

	vaesenc	144(%rdi),%xmm16,%xmm16

	vaesenc	160(%rdi),%xmm16,%xmm16

	vaesenc	176(%rdi),%xmm16,%xmm16

	vaesenc	192(%rdi),%xmm16,%xmm16

	vaesenc	208(%rdi),%xmm16,%xmm16

	vaesenclast	224(%rdi),%xmm16,%xmm16
	jmp	.Lexit_aes_otrDyyvmtrjBjDk
.Lexit_aes_otrDyyvmtrjBjDk:

	vpshufb	SHUF_MASK(%rip),%xmm16,%xmm16

	vmovdqa64	%xmm16,%xmm2
	vpsllq	$1,%xmm16,%xmm16
	vpsrlq	$63,%xmm2,%xmm2
	vmovdqa	%xmm2,%xmm1
	vpslldq	$8,%xmm2,%xmm2
	vpsrldq	$8,%xmm1,%xmm1
	vporq	%xmm2,%xmm16,%xmm16

	vpshufd	$36,%xmm1,%xmm2
	vpcmpeqd	TWOONE(%rip),%xmm2,%xmm2
	vpand	POLY(%rip),%xmm2,%xmm2
	vpxorq	%xmm2,%xmm16,%xmm16

	vmovdqu64	%xmm16,336(%rsi)
	vshufi32x4	$0x00,%ymm16,%ymm16,%ymm4
	vmovdqa	%ymm4,%ymm3

	vpclmulqdq	$0x11,%ymm4,%ymm3,%ymm0
	vpclmulqdq	$0x00,%ymm4,%ymm3,%ymm1
	vpclmulqdq	$0x01,%ymm4,%ymm3,%ymm2
	vpclmulqdq	$0x10,%ymm4,%ymm3,%ymm3
	vpxorq	%ymm2,%ymm3,%ymm3

	vpsrldq	$8,%ymm3,%ymm2
	vpslldq	$8,%ymm3,%ymm3
	vpxorq	%ymm2,%ymm0,%ymm0
	vpxorq	%ymm1,%ymm3,%ymm3



	vmovdqu64	POLY2(%rip),%ymm2

	vpclmulqdq	$0x01,%ymm3,%ymm2,%ymm1
	vpslldq	$8,%ymm1,%ymm1
	vpxorq	%ymm1,%ymm3,%ymm3



	vpclmulqdq	$0x00,%ymm3,%ymm2,%ymm1
	vpsrldq	$4,%ymm1,%ymm1
	vpclmulqdq	$0x10,%ymm3,%ymm2,%ymm3
	vpslldq	$4,%ymm3,%ymm3

	vpternlogq	$0x96,%ymm1,%ymm0,%ymm3

	vmovdqu64	%xmm3,320(%rsi)
	vinserti64x2	$1,%xmm16,%ymm3,%ymm4
	vmovdqa64	%ymm4,%ymm5

	vpclmulqdq	$0x11,%ymm3,%ymm4,%ymm0
	vpclmulqdq	$0x00,%ymm3,%ymm4,%ymm1
	vpclmulqdq	$0x01,%ymm3,%ymm4,%ymm2
	vpclmulqdq	$0x10,%ymm3,%ymm4,%ymm4
	vpxorq	%ymm2,%ymm4,%ymm4

	vpsrldq	$8,%ymm4,%ymm2
	vpslldq	$8,%ymm4,%ymm4
	vpxorq	%ymm2,%ymm0,%ymm0
	vpxorq	%ymm1,%ymm4,%ymm4



	vmovdqu64	POLY2(%rip),%ymm2

	vpclmulqdq	$0x01,%ymm4,%ymm2,%ymm1
	vpslldq	$8,%ymm1,%ymm1
	vpxorq	%ymm1,%ymm4,%ymm4



	vpclmulqdq	$0x00,%ymm4,%ymm2,%ymm1
	vpsrldq	$4,%ymm1,%ymm1
	vpclmulqdq	$0x10,%ymm4,%ymm2,%ymm4
	vpslldq	$4,%ymm4,%ymm4

	vpternlogq	$0x96,%ymm1,%ymm0,%ymm4

	vmovdqu64	%ymm4,288(%rsi)

	vinserti64x4	$1,%ymm5,%zmm4,%zmm4


	vshufi64x2	$0x00,%zmm4,%zmm4,%zmm3
	vmovdqa64	%zmm4,%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm0
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm1
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm2
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm2
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm0,%zmm0
	vpxorq	%zmm1,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm2

	vpclmulqdq	$0x01,%zmm4,%zmm2,%zmm1
	vpslldq	$8,%zmm1,%zmm1
	vpxorq	%zmm1,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm2,%zmm1
	vpsrldq	$4,%zmm1,%zmm1
	vpclmulqdq	$0x10,%zmm4,%zmm2,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm1,%zmm0,%zmm4

	vmovdqu64	%zmm4,224(%rsi)
	vshufi64x2	$0x00,%zmm4,%zmm4,%zmm3

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm0
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm1
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm2
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm2,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm2
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm2,%zmm0,%zmm0
	vpxorq	%zmm1,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm2

	vpclmulqdq	$0x01,%zmm5,%zmm2,%zmm1
	vpslldq	$8,%zmm1,%zmm1
	vpxorq	%zmm1,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm2,%zmm1
	vpsrldq	$4,%zmm1,%zmm1
	vpclmulqdq	$0x10,%zmm5,%zmm2,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm1,%zmm0,%zmm5

	vmovdqu64	%zmm5,160(%rsi)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm0
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm1
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm2
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm2
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm0,%zmm0
	vpxorq	%zmm1,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm2

	vpclmulqdq	$0x01,%zmm4,%zmm2,%zmm1
	vpslldq	$8,%zmm1,%zmm1
	vpxorq	%zmm1,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm2,%zmm1
	vpsrldq	$4,%zmm1,%zmm1
	vpclmulqdq	$0x10,%zmm4,%zmm2,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm1,%zmm0,%zmm4

	vmovdqu64	%zmm4,96(%rsi)
	vzeroupper
.Labort_init:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	ossl_aes_gcm_init_avx512, .-ossl_aes_gcm_init_avx512
.globl	ossl_aes_gcm_setiv_avx512
.type	ossl_aes_gcm_setiv_avx512,@function
.align	32
ossl_aes_gcm_setiv_avx512:
.cfi_startproc	
.Lsetiv_seh_begin:
.byte	243,15,30,250
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
.Lsetiv_seh_push_rbx:
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
.Lsetiv_seh_push_rbp:
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
.Lsetiv_seh_push_r12:
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
.Lsetiv_seh_push_r13:
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
.Lsetiv_seh_push_r14:
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
.Lsetiv_seh_push_r15:










	leaq	0(%rsp),%rbp
.cfi_def_cfa_register	%rbp
.Lsetiv_seh_setfp:

.Lsetiv_seh_prolog_end:
	subq	$820,%rsp
	andq	$(-64),%rsp
	cmpq	$12,%rcx
	je	iv_len_12_init_IV
	vpxor	%xmm2,%xmm2,%xmm2
	movq	%rdx,%r10
	movq	%rcx,%r11
	orq	%r11,%r11
	jz	.L_CALC_AAD_done_jwDgyariknlwpAi

	xorq	%rbx,%rbx
	vmovdqa64	SHUF_MASK(%rip),%zmm16

.L_get_AAD_loop48x16_jwDgyariknlwpAi:
	cmpq	$768,%r11
	jl	.L_exit_AAD_loop48x16_jwDgyariknlwpAi
	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	testq	%rbx,%rbx
	jnz	.L_skip_hkeys_precomputation_AihyocGyslDBpel

	vmovdqu64	288(%rsi),%zmm1
	vmovdqu64	%zmm1,704(%rsp)

	vmovdqu64	224(%rsi),%zmm9
	vmovdqu64	%zmm9,640(%rsp)


	vshufi64x2	$0x00,%zmm9,%zmm9,%zmm9

	vmovdqu64	160(%rsi),%zmm10
	vmovdqu64	%zmm10,576(%rsp)

	vmovdqu64	96(%rsi),%zmm12
	vmovdqu64	%zmm12,512(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,448(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,384(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,320(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,256(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,192(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,128(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,64(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,0(%rsp)
.L_skip_hkeys_precomputation_AihyocGyslDBpel:
	movq	$1,%rbx
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	0(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	64(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	128(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	192(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	256(%r10),%zmm11
	vmovdqu64	320(%r10),%zmm3
	vmovdqu64	384(%r10),%zmm4
	vmovdqu64	448(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	256(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	320(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	384(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	448(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	512(%r10),%zmm11
	vmovdqu64	576(%r10),%zmm3
	vmovdqu64	640(%r10),%zmm4
	vmovdqu64	704(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	512(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	576(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	640(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	704(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm2

	subq	$768,%r11
	je	.L_CALC_AAD_done_jwDgyariknlwpAi

	addq	$768,%r10
	jmp	.L_get_AAD_loop48x16_jwDgyariknlwpAi

.L_exit_AAD_loop48x16_jwDgyariknlwpAi:

	cmpq	$512,%r11
	jl	.L_less_than_32x16_jwDgyariknlwpAi

	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	testq	%rbx,%rbx
	jnz	.L_skip_hkeys_precomputation_yxuacpnEbrcpEpy

	vmovdqu64	288(%rsi),%zmm1
	vmovdqu64	%zmm1,704(%rsp)

	vmovdqu64	224(%rsi),%zmm9
	vmovdqu64	%zmm9,640(%rsp)


	vshufi64x2	$0x00,%zmm9,%zmm9,%zmm9

	vmovdqu64	160(%rsi),%zmm10
	vmovdqu64	%zmm10,576(%rsp)

	vmovdqu64	96(%rsi),%zmm12
	vmovdqu64	%zmm12,512(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,448(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,384(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,320(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,256(%rsp)
.L_skip_hkeys_precomputation_yxuacpnEbrcpEpy:
	movq	$1,%rbx
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	256(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	320(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	384(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	448(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	256(%r10),%zmm11
	vmovdqu64	320(%r10),%zmm3
	vmovdqu64	384(%r10),%zmm4
	vmovdqu64	448(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	512(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	576(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	640(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	704(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm2

	subq	$512,%r11
	je	.L_CALC_AAD_done_jwDgyariknlwpAi

	addq	$512,%r10
	jmp	.L_less_than_16x16_jwDgyariknlwpAi

.L_less_than_32x16_jwDgyariknlwpAi:
	cmpq	$256,%r11
	jl	.L_less_than_16x16_jwDgyariknlwpAi

	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	96(%rsi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	160(%rsi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	224(%rsi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	288(%rsi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm2

	subq	$256,%r11
	je	.L_CALC_AAD_done_jwDgyariknlwpAi

	addq	$256,%r10

.L_less_than_16x16_jwDgyariknlwpAi:

	leaq	byte64_len_to_mask_table(%rip),%r12
	leaq	(%r12,%r11,8),%r12


	addl	$15,%r11d
	shrl	$4,%r11d
	cmpl	$2,%r11d
	jb	.L_AAD_blocks_1_jwDgyariknlwpAi
	je	.L_AAD_blocks_2_jwDgyariknlwpAi
	cmpl	$4,%r11d
	jb	.L_AAD_blocks_3_jwDgyariknlwpAi
	je	.L_AAD_blocks_4_jwDgyariknlwpAi
	cmpl	$6,%r11d
	jb	.L_AAD_blocks_5_jwDgyariknlwpAi
	je	.L_AAD_blocks_6_jwDgyariknlwpAi
	cmpl	$8,%r11d
	jb	.L_AAD_blocks_7_jwDgyariknlwpAi
	je	.L_AAD_blocks_8_jwDgyariknlwpAi
	cmpl	$10,%r11d
	jb	.L_AAD_blocks_9_jwDgyariknlwpAi
	je	.L_AAD_blocks_10_jwDgyariknlwpAi
	cmpl	$12,%r11d
	jb	.L_AAD_blocks_11_jwDgyariknlwpAi
	je	.L_AAD_blocks_12_jwDgyariknlwpAi
	cmpl	$14,%r11d
	jb	.L_AAD_blocks_13_jwDgyariknlwpAi
	je	.L_AAD_blocks_14_jwDgyariknlwpAi
	cmpl	$15,%r11d
	je	.L_AAD_blocks_15_jwDgyariknlwpAi
.L_AAD_blocks_16_jwDgyariknlwpAi:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%zmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	96(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	160(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	224(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm9,%zmm11,%zmm1
	vpternlogq	$0x96,%zmm10,%zmm3,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm12,%zmm11,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm3,%zmm8
	vmovdqu64	288(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm5,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm5,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm5,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm5,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_jwDgyariknlwpAi
.L_AAD_blocks_15_jwDgyariknlwpAi:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%zmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	112(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	176(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	240(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	304(%rsi),%ymm15
	vinserti64x2	$2,336(%rsi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm5,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm5,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm5,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm5,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_jwDgyariknlwpAi
.L_AAD_blocks_14_jwDgyariknlwpAi:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%ymm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%ymm16,%ymm5,%ymm5
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	128(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	192(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	256(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	320(%rsi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm5,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm5,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm5,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm5,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_jwDgyariknlwpAi
.L_AAD_blocks_13_jwDgyariknlwpAi:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%xmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%xmm16,%xmm5,%xmm5
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	144(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	208(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	272(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	336(%rsi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm5,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm5,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm5,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm5,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_jwDgyariknlwpAi
.L_AAD_blocks_12_jwDgyariknlwpAi:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	160(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	224(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	288(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_jwDgyariknlwpAi
.L_AAD_blocks_11_jwDgyariknlwpAi:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	176(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	240(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	304(%rsi),%ymm15
	vinserti64x2	$2,336(%rsi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_jwDgyariknlwpAi
.L_AAD_blocks_10_jwDgyariknlwpAi:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%ymm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%ymm16,%ymm4,%ymm4
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	192(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	256(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	320(%rsi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm4,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm4,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm4,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm4,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_jwDgyariknlwpAi
.L_AAD_blocks_9_jwDgyariknlwpAi:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%xmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%xmm16,%xmm4,%xmm4
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	208(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	272(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	336(%rsi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm4,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm4,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm4,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm4,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_jwDgyariknlwpAi
.L_AAD_blocks_8_jwDgyariknlwpAi:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	224(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	288(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_jwDgyariknlwpAi
.L_AAD_blocks_7_jwDgyariknlwpAi:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	240(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	304(%rsi),%ymm15
	vinserti64x2	$2,336(%rsi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_jwDgyariknlwpAi
.L_AAD_blocks_6_jwDgyariknlwpAi:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%ymm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%ymm16,%ymm3,%ymm3
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	256(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	320(%rsi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm3,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm3,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm3,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm3,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_jwDgyariknlwpAi
.L_AAD_blocks_5_jwDgyariknlwpAi:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%xmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%xmm16,%xmm3,%xmm3
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	272(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	336(%rsi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm3,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm3,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm3,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm3,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_jwDgyariknlwpAi
.L_AAD_blocks_4_jwDgyariknlwpAi:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	288(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_jwDgyariknlwpAi
.L_AAD_blocks_3_jwDgyariknlwpAi:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	304(%rsi),%ymm15
	vinserti64x2	$2,336(%rsi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_jwDgyariknlwpAi
.L_AAD_blocks_2_jwDgyariknlwpAi:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%ymm11{%k1}{z}
	vpshufb	%ymm16,%ymm11,%ymm11
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	320(%rsi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm11,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm11,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm11,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm11,%ymm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_jwDgyariknlwpAi
.L_AAD_blocks_1_jwDgyariknlwpAi:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%xmm11{%k1}{z}
	vpshufb	%xmm16,%xmm11,%xmm11
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	336(%rsi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm11,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm11,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm11,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm11,%xmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

.L_CALC_AAD_done_jwDgyariknlwpAi:
	movq	%rcx,%r10
	shlq	$3,%r10
	vmovq	%r10,%xmm3


	vpxorq	%xmm2,%xmm3,%xmm2

	vmovdqu64	336(%rsi),%xmm1

	vpclmulqdq	$0x11,%xmm1,%xmm2,%xmm11
	vpclmulqdq	$0x00,%xmm1,%xmm2,%xmm3
	vpclmulqdq	$0x01,%xmm1,%xmm2,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm2,%xmm2
	vpxorq	%xmm4,%xmm2,%xmm2

	vpsrldq	$8,%xmm2,%xmm4
	vpslldq	$8,%xmm2,%xmm2
	vpxorq	%xmm4,%xmm11,%xmm11
	vpxorq	%xmm3,%xmm2,%xmm2



	vmovdqu64	POLY2(%rip),%xmm4

	vpclmulqdq	$0x01,%xmm2,%xmm4,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm2,%xmm2



	vpclmulqdq	$0x00,%xmm2,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm2,%xmm4,%xmm2
	vpslldq	$4,%xmm2,%xmm2

	vpternlogq	$0x96,%xmm3,%xmm11,%xmm2

	vpshufb	SHUF_MASK(%rip),%xmm2,%xmm2
	jmp	skip_iv_len_12_init_IV
iv_len_12_init_IV:

	vmovdqu8	ONEf(%rip),%xmm2
	movq	%rdx,%r11
	movl	$0x0000000000000fff,%r10d
	kmovq	%r10,%k1
	vmovdqu8	(%r11),%xmm2{%k1}
skip_iv_len_12_init_IV:
	vmovdqu	%xmm2,%xmm1


	movl	240(%rdi),%r10d
	cmpl	$9,%r10d
	je	.Laes_128_diucycifbhxqjre
	cmpl	$11,%r10d
	je	.Laes_192_diucycifbhxqjre
	cmpl	$13,%r10d
	je	.Laes_256_diucycifbhxqjre
	jmp	.Lexit_aes_diucycifbhxqjre
.align	32
.Laes_128_diucycifbhxqjre:
	vpxorq	0(%rdi),%xmm1,%xmm1

	vaesenc	16(%rdi),%xmm1,%xmm1

	vaesenc	32(%rdi),%xmm1,%xmm1

	vaesenc	48(%rdi),%xmm1,%xmm1

	vaesenc	64(%rdi),%xmm1,%xmm1

	vaesenc	80(%rdi),%xmm1,%xmm1

	vaesenc	96(%rdi),%xmm1,%xmm1

	vaesenc	112(%rdi),%xmm1,%xmm1

	vaesenc	128(%rdi),%xmm1,%xmm1

	vaesenc	144(%rdi),%xmm1,%xmm1

	vaesenclast	160(%rdi),%xmm1,%xmm1
	jmp	.Lexit_aes_diucycifbhxqjre
.align	32
.Laes_192_diucycifbhxqjre:
	vpxorq	0(%rdi),%xmm1,%xmm1

	vaesenc	16(%rdi),%xmm1,%xmm1

	vaesenc	32(%rdi),%xmm1,%xmm1

	vaesenc	48(%rdi),%xmm1,%xmm1

	vaesenc	64(%rdi),%xmm1,%xmm1

	vaesenc	80(%rdi),%xmm1,%xmm1

	vaesenc	96(%rdi),%xmm1,%xmm1

	vaesenc	112(%rdi),%xmm1,%xmm1

	vaesenc	128(%rdi),%xmm1,%xmm1

	vaesenc	144(%rdi),%xmm1,%xmm1

	vaesenc	160(%rdi),%xmm1,%xmm1

	vaesenc	176(%rdi),%xmm1,%xmm1

	vaesenclast	192(%rdi),%xmm1,%xmm1
	jmp	.Lexit_aes_diucycifbhxqjre
.align	32
.Laes_256_diucycifbhxqjre:
	vpxorq	0(%rdi),%xmm1,%xmm1

	vaesenc	16(%rdi),%xmm1,%xmm1

	vaesenc	32(%rdi),%xmm1,%xmm1

	vaesenc	48(%rdi),%xmm1,%xmm1

	vaesenc	64(%rdi),%xmm1,%xmm1

	vaesenc	80(%rdi),%xmm1,%xmm1

	vaesenc	96(%rdi),%xmm1,%xmm1

	vaesenc	112(%rdi),%xmm1,%xmm1

	vaesenc	128(%rdi),%xmm1,%xmm1

	vaesenc	144(%rdi),%xmm1,%xmm1

	vaesenc	160(%rdi),%xmm1,%xmm1

	vaesenc	176(%rdi),%xmm1,%xmm1

	vaesenc	192(%rdi),%xmm1,%xmm1

	vaesenc	208(%rdi),%xmm1,%xmm1

	vaesenclast	224(%rdi),%xmm1,%xmm1
	jmp	.Lexit_aes_diucycifbhxqjre
.Lexit_aes_diucycifbhxqjre:

	vmovdqu	%xmm1,32(%rsi)


	vpshufb	SHUF_MASK(%rip),%xmm2,%xmm2
	vmovdqu	%xmm2,0(%rsi)
	cmpq	$256,%rcx
	jbe	.Lskip_hkeys_cleanup_ybinBclziAfknuA
	vpxor	%xmm0,%xmm0,%xmm0
	vmovdqa64	%zmm0,0(%rsp)
	vmovdqa64	%zmm0,64(%rsp)
	vmovdqa64	%zmm0,128(%rsp)
	vmovdqa64	%zmm0,192(%rsp)
	vmovdqa64	%zmm0,256(%rsp)
	vmovdqa64	%zmm0,320(%rsp)
	vmovdqa64	%zmm0,384(%rsp)
	vmovdqa64	%zmm0,448(%rsp)
	vmovdqa64	%zmm0,512(%rsp)
	vmovdqa64	%zmm0,576(%rsp)
	vmovdqa64	%zmm0,640(%rsp)
	vmovdqa64	%zmm0,704(%rsp)
.Lskip_hkeys_cleanup_ybinBclziAfknuA:
	vzeroupper
	leaq	(%rbp),%rsp
.cfi_def_cfa_register	%rsp
	popq	%r15
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r15
	popq	%r14
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r14
	popq	%r13
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r13
	popq	%r12
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r12
	popq	%rbp
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbp
	popq	%rbx
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbx
.Labort_setiv:
	.byte	0xf3,0xc3
.Lsetiv_seh_end:
.cfi_endproc	
.size	ossl_aes_gcm_setiv_avx512, .-ossl_aes_gcm_setiv_avx512
.globl	ossl_aes_gcm_update_aad_avx512
.type	ossl_aes_gcm_update_aad_avx512,@function
.align	32
ossl_aes_gcm_update_aad_avx512:
.cfi_startproc	
.Lghash_seh_begin:
.byte	243,15,30,250
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
.Lghash_seh_push_rbx:
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
.Lghash_seh_push_rbp:
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
.Lghash_seh_push_r12:
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
.Lghash_seh_push_r13:
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
.Lghash_seh_push_r14:
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
.Lghash_seh_push_r15:










	leaq	0(%rsp),%rbp
.cfi_def_cfa_register	%rbp
.Lghash_seh_setfp:

.Lghash_seh_prolog_end:
	subq	$820,%rsp
	andq	$(-64),%rsp
	vmovdqu64	64(%rdi),%xmm14
	movq	%rsi,%r10
	movq	%rdx,%r11
	orq	%r11,%r11
	jz	.L_CALC_AAD_done_rfGeAAAiGctqyaA

	xorq	%rbx,%rbx
	vmovdqa64	SHUF_MASK(%rip),%zmm16

.L_get_AAD_loop48x16_rfGeAAAiGctqyaA:
	cmpq	$768,%r11
	jl	.L_exit_AAD_loop48x16_rfGeAAAiGctqyaA
	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	testq	%rbx,%rbx
	jnz	.L_skip_hkeys_precomputation_uyEvkvBcdnthxgc

	vmovdqu64	288(%rdi),%zmm1
	vmovdqu64	%zmm1,704(%rsp)

	vmovdqu64	224(%rdi),%zmm9
	vmovdqu64	%zmm9,640(%rsp)


	vshufi64x2	$0x00,%zmm9,%zmm9,%zmm9

	vmovdqu64	160(%rdi),%zmm10
	vmovdqu64	%zmm10,576(%rsp)

	vmovdqu64	96(%rdi),%zmm12
	vmovdqu64	%zmm12,512(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,448(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,384(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,320(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,256(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,192(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,128(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,64(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,0(%rsp)
.L_skip_hkeys_precomputation_uyEvkvBcdnthxgc:
	movq	$1,%rbx
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	0(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	64(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	128(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	192(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	256(%r10),%zmm11
	vmovdqu64	320(%r10),%zmm3
	vmovdqu64	384(%r10),%zmm4
	vmovdqu64	448(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	256(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	320(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	384(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	448(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	512(%r10),%zmm11
	vmovdqu64	576(%r10),%zmm3
	vmovdqu64	640(%r10),%zmm4
	vmovdqu64	704(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	512(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	576(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	640(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	704(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm14

	subq	$768,%r11
	je	.L_CALC_AAD_done_rfGeAAAiGctqyaA

	addq	$768,%r10
	jmp	.L_get_AAD_loop48x16_rfGeAAAiGctqyaA

.L_exit_AAD_loop48x16_rfGeAAAiGctqyaA:

	cmpq	$512,%r11
	jl	.L_less_than_32x16_rfGeAAAiGctqyaA

	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	testq	%rbx,%rbx
	jnz	.L_skip_hkeys_precomputation_idhBhrrCkbtziny

	vmovdqu64	288(%rdi),%zmm1
	vmovdqu64	%zmm1,704(%rsp)

	vmovdqu64	224(%rdi),%zmm9
	vmovdqu64	%zmm9,640(%rsp)


	vshufi64x2	$0x00,%zmm9,%zmm9,%zmm9

	vmovdqu64	160(%rdi),%zmm10
	vmovdqu64	%zmm10,576(%rsp)

	vmovdqu64	96(%rdi),%zmm12
	vmovdqu64	%zmm12,512(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,448(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,384(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,320(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,256(%rsp)
.L_skip_hkeys_precomputation_idhBhrrCkbtziny:
	movq	$1,%rbx
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	256(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	320(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	384(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	448(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	256(%r10),%zmm11
	vmovdqu64	320(%r10),%zmm3
	vmovdqu64	384(%r10),%zmm4
	vmovdqu64	448(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	512(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	576(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	640(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	704(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm14

	subq	$512,%r11
	je	.L_CALC_AAD_done_rfGeAAAiGctqyaA

	addq	$512,%r10
	jmp	.L_less_than_16x16_rfGeAAAiGctqyaA

.L_less_than_32x16_rfGeAAAiGctqyaA:
	cmpq	$256,%r11
	jl	.L_less_than_16x16_rfGeAAAiGctqyaA

	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	96(%rdi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	160(%rdi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	224(%rdi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	288(%rdi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm14

	subq	$256,%r11
	je	.L_CALC_AAD_done_rfGeAAAiGctqyaA

	addq	$256,%r10

.L_less_than_16x16_rfGeAAAiGctqyaA:

	leaq	byte64_len_to_mask_table(%rip),%r12
	leaq	(%r12,%r11,8),%r12


	addl	$15,%r11d
	shrl	$4,%r11d
	cmpl	$2,%r11d
	jb	.L_AAD_blocks_1_rfGeAAAiGctqyaA
	je	.L_AAD_blocks_2_rfGeAAAiGctqyaA
	cmpl	$4,%r11d
	jb	.L_AAD_blocks_3_rfGeAAAiGctqyaA
	je	.L_AAD_blocks_4_rfGeAAAiGctqyaA
	cmpl	$6,%r11d
	jb	.L_AAD_blocks_5_rfGeAAAiGctqyaA
	je	.L_AAD_blocks_6_rfGeAAAiGctqyaA
	cmpl	$8,%r11d
	jb	.L_AAD_blocks_7_rfGeAAAiGctqyaA
	je	.L_AAD_blocks_8_rfGeAAAiGctqyaA
	cmpl	$10,%r11d
	jb	.L_AAD_blocks_9_rfGeAAAiGctqyaA
	je	.L_AAD_blocks_10_rfGeAAAiGctqyaA
	cmpl	$12,%r11d
	jb	.L_AAD_blocks_11_rfGeAAAiGctqyaA
	je	.L_AAD_blocks_12_rfGeAAAiGctqyaA
	cmpl	$14,%r11d
	jb	.L_AAD_blocks_13_rfGeAAAiGctqyaA
	je	.L_AAD_blocks_14_rfGeAAAiGctqyaA
	cmpl	$15,%r11d
	je	.L_AAD_blocks_15_rfGeAAAiGctqyaA
.L_AAD_blocks_16_rfGeAAAiGctqyaA:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%zmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	96(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	160(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	224(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm9,%zmm11,%zmm1
	vpternlogq	$0x96,%zmm10,%zmm3,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm12,%zmm11,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm3,%zmm8
	vmovdqu64	288(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm5,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm5,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm5,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm5,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_rfGeAAAiGctqyaA
.L_AAD_blocks_15_rfGeAAAiGctqyaA:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%zmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	112(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	176(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	240(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	304(%rdi),%ymm15
	vinserti64x2	$2,336(%rdi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm5,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm5,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm5,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm5,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_rfGeAAAiGctqyaA
.L_AAD_blocks_14_rfGeAAAiGctqyaA:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%ymm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%ymm16,%ymm5,%ymm5
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	128(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	192(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	256(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	320(%rdi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm5,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm5,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm5,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm5,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_rfGeAAAiGctqyaA
.L_AAD_blocks_13_rfGeAAAiGctqyaA:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%xmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%xmm16,%xmm5,%xmm5
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	144(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	208(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	272(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	336(%rdi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm5,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm5,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm5,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm5,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_rfGeAAAiGctqyaA
.L_AAD_blocks_12_rfGeAAAiGctqyaA:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	160(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	224(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	288(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_rfGeAAAiGctqyaA
.L_AAD_blocks_11_rfGeAAAiGctqyaA:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	176(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	240(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	304(%rdi),%ymm15
	vinserti64x2	$2,336(%rdi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_rfGeAAAiGctqyaA
.L_AAD_blocks_10_rfGeAAAiGctqyaA:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%ymm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%ymm16,%ymm4,%ymm4
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	192(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	256(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	320(%rdi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm4,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm4,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm4,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm4,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_rfGeAAAiGctqyaA
.L_AAD_blocks_9_rfGeAAAiGctqyaA:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%xmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%xmm16,%xmm4,%xmm4
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	208(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	272(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	336(%rdi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm4,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm4,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm4,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm4,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_rfGeAAAiGctqyaA
.L_AAD_blocks_8_rfGeAAAiGctqyaA:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	224(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	288(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_rfGeAAAiGctqyaA
.L_AAD_blocks_7_rfGeAAAiGctqyaA:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	240(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	304(%rdi),%ymm15
	vinserti64x2	$2,336(%rdi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_rfGeAAAiGctqyaA
.L_AAD_blocks_6_rfGeAAAiGctqyaA:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%ymm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%ymm16,%ymm3,%ymm3
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	256(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	320(%rdi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm3,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm3,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm3,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm3,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_rfGeAAAiGctqyaA
.L_AAD_blocks_5_rfGeAAAiGctqyaA:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%xmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%xmm16,%xmm3,%xmm3
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	272(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	336(%rdi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm3,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm3,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm3,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm3,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_rfGeAAAiGctqyaA
.L_AAD_blocks_4_rfGeAAAiGctqyaA:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	288(%rdi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_rfGeAAAiGctqyaA
.L_AAD_blocks_3_rfGeAAAiGctqyaA:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	304(%rdi),%ymm15
	vinserti64x2	$2,336(%rdi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_rfGeAAAiGctqyaA
.L_AAD_blocks_2_rfGeAAAiGctqyaA:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%ymm11{%k1}{z}
	vpshufb	%ymm16,%ymm11,%ymm11
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	320(%rdi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm11,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm11,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm11,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm11,%ymm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_rfGeAAAiGctqyaA
.L_AAD_blocks_1_rfGeAAAiGctqyaA:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%xmm11{%k1}{z}
	vpshufb	%xmm16,%xmm11,%xmm11
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	336(%rdi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm11,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm11,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm11,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm11,%xmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

.L_CALC_AAD_done_rfGeAAAiGctqyaA:
	vmovdqu64	%xmm14,64(%rdi)
	cmpq	$256,%rdx
	jbe	.Lskip_hkeys_cleanup_bAajcrtqskztvsm
	vpxor	%xmm0,%xmm0,%xmm0
	vmovdqa64	%zmm0,0(%rsp)
	vmovdqa64	%zmm0,64(%rsp)
	vmovdqa64	%zmm0,128(%rsp)
	vmovdqa64	%zmm0,192(%rsp)
	vmovdqa64	%zmm0,256(%rsp)
	vmovdqa64	%zmm0,320(%rsp)
	vmovdqa64	%zmm0,384(%rsp)
	vmovdqa64	%zmm0,448(%rsp)
	vmovdqa64	%zmm0,512(%rsp)
	vmovdqa64	%zmm0,576(%rsp)
	vmovdqa64	%zmm0,640(%rsp)
	vmovdqa64	%zmm0,704(%rsp)
.Lskip_hkeys_cleanup_bAajcrtqskztvsm:
	vzeroupper
	leaq	(%rbp),%rsp
.cfi_def_cfa_register	%rsp
	popq	%r15
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r15
	popq	%r14
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r14
	popq	%r13
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r13
	popq	%r12
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r12
	popq	%rbp
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbp
	popq	%rbx
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbx
.Lexit_update_aad:
	.byte	0xf3,0xc3
.Lghash_seh_end:
.cfi_endproc	
.size	ossl_aes_gcm_update_aad_avx512, .-ossl_aes_gcm_update_aad_avx512
.globl	ossl_aes_gcm_encrypt_avx512
.type	ossl_aes_gcm_encrypt_avx512,@function
.align	32
ossl_aes_gcm_encrypt_avx512:
.cfi_startproc	
.Lencrypt_seh_begin:
.byte	243,15,30,250
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
.Lencrypt_seh_push_rbx:
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
.Lencrypt_seh_push_rbp:
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
.Lencrypt_seh_push_r12:
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
.Lencrypt_seh_push_r13:
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
.Lencrypt_seh_push_r14:
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
.Lencrypt_seh_push_r15:










	leaq	0(%rsp),%rbp
.cfi_def_cfa_register	%rbp
.Lencrypt_seh_setfp:

.Lencrypt_seh_prolog_end:
	subq	$1588,%rsp
	andq	$(-64),%rsp


	movl	240(%rdi),%eax
	cmpl	$9,%eax
	je	.Laes_gcm_encrypt_128_avx512
	cmpl	$11,%eax
	je	.Laes_gcm_encrypt_192_avx512
	cmpl	$13,%eax
	je	.Laes_gcm_encrypt_256_avx512
	xorl	%eax,%eax
	jmp	.Lexit_gcm_encrypt
.align	32
.Laes_gcm_encrypt_128_avx512:
	orq	%r8,%r8
	je	.L_enc_dec_done_AgmocwsDmrmqxia
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14

	movq	(%rdx),%r11
	orq	%r11,%r11
	je	.L_partial_block_done_rwjiurfcmmlEfad
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3
	vmovdqu64	336(%rsi),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%r11,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%r11,1),%r13
	subq	$16,%r13
	jge	.L_no_extra_mask_rwjiurfcmmlEfad
	subq	%r13,%r12
.L_no_extra_mask_rwjiurfcmmlEfad:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	.L_partial_incomplete_rwjiurfcmmlEfad

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movq	$0,(%rdx)

	movq	%r11,%r12
	movq	$16,%r11
	subq	%r12,%r11
	jmp	.L_enc_dec_done_rwjiurfcmmlEfad

.L_partial_incomplete_rwjiurfcmmlEfad:
	addq	%r8,(%rdx)
	movq	%r8,%r11

.L_enc_dec_done_rwjiurfcmmlEfad:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%r11,2),%k1
	vmovdqu64	%xmm14,64(%rsi)

	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
.L_partial_block_done_rwjiurfcmmlEfad:
	vmovdqu64	0(%rsi),%xmm2
	subq	%r11,%r8
	je	.L_enc_dec_done_AgmocwsDmrmqxia
	cmpq	$256,%r8
	jbe	.L_message_below_equal_16_blocks_AgmocwsDmrmqxia

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_kCxdpCootxEBCGF
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_kCxdpCootxEBCGF
.L_next_16_overflow_kCxdpCootxEBCGF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_kCxdpCootxEBCGF:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%r11,1),%zmm0
	vmovdqu8	64(%rcx,%r11,1),%zmm3
	vmovdqu8	128(%rcx,%r11,1),%zmm4
	vmovdqu8	192(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%r11,1)
	vmovdqu8	%zmm10,64(%r10,%r11,1)
	vmovdqu8	%zmm11,128(%r10,%r11,1)
	vmovdqu8	%zmm12,192(%r10,%r11,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_CcwvAEwBBauvnio

	vmovdqu64	288(%rsi),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	224(%rsi),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	160(%rsi),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	96(%rsi),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
.L_skip_hkeys_precomputation_CcwvAEwBBauvnio:
	cmpq	$512,%r8
	jb	.L_message_below_32_blocks_AgmocwsDmrmqxia



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_vjxnuycmzsqjftq
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_vjxnuycmzsqjftq
.L_next_16_overflow_vjxnuycmzsqjftq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_vjxnuycmzsqjftq:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%r11,1),%zmm0
	vmovdqu8	320(%rcx,%r11,1),%zmm3
	vmovdqu8	384(%rcx,%r11,1),%zmm4
	vmovdqu8	448(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%r11,1)
	vmovdqu8	%zmm10,320(%r10,%r11,1)
	vmovdqu8	%zmm11,384(%r10,%r11,1)
	vmovdqu8	%zmm12,448(%r10,%r11,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_qrjClfCGoubjndw
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
.L_skip_hkeys_precomputation_qrjClfCGoubjndw:
	movq	$1,%r14
	addq	$512,%r11
	subq	$512,%r8

	cmpq	$768,%r8
	jb	.L_no_more_big_nblocks_AgmocwsDmrmqxia
.L_encrypt_big_nblocks_AgmocwsDmrmqxia:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_EbGjlCgnoankgfu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_EbGjlCgnoankgfu
.L_16_blocks_overflow_EbGjlCgnoankgfu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_EbGjlCgnoankgfu:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_kpdBbBrDBhztshs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_kpdBbBrDBhztshs
.L_16_blocks_overflow_kpdBbBrDBhztshs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_kpdBbBrDBhztshs:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_ywDolzCFwADjskw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_ywDolzCFwADjskw
.L_16_blocks_overflow_ywDolzCFwADjskw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_ywDolzCFwADjskw:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%r11,1),%zmm17
	vmovdqu8	576(%rcx,%r11,1),%zmm19
	vmovdqu8	640(%rcx,%r11,1),%zmm20
	vmovdqu8	704(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%r11,1)
	vmovdqu8	%zmm3,576(%r10,%r11,1)
	vmovdqu8	%zmm4,640(%r10,%r11,1)
	vmovdqu8	%zmm5,704(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%r11
	subq	$768,%r8
	cmpq	$768,%r8
	jae	.L_encrypt_big_nblocks_AgmocwsDmrmqxia

.L_no_more_big_nblocks_AgmocwsDmrmqxia:

	cmpq	$512,%r8
	jae	.L_encrypt_32_blocks_AgmocwsDmrmqxia

	cmpq	$256,%r8
	jae	.L_encrypt_16_blocks_AgmocwsDmrmqxia
.L_encrypt_0_blocks_ghash_32_AgmocwsDmrmqxia:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_bferpfwABBkjryF

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_bferpfwABBkjryF
	jb	.L_last_num_blocks_is_7_1_bferpfwABBkjryF


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_bferpfwABBkjryF
	jb	.L_last_num_blocks_is_11_9_bferpfwABBkjryF


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_bferpfwABBkjryF
	ja	.L_last_num_blocks_is_16_bferpfwABBkjryF
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_bferpfwABBkjryF
	jmp	.L_last_num_blocks_is_13_bferpfwABBkjryF

.L_last_num_blocks_is_11_9_bferpfwABBkjryF:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_bferpfwABBkjryF
	ja	.L_last_num_blocks_is_11_bferpfwABBkjryF
	jmp	.L_last_num_blocks_is_9_bferpfwABBkjryF

.L_last_num_blocks_is_7_1_bferpfwABBkjryF:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_bferpfwABBkjryF
	jb	.L_last_num_blocks_is_3_1_bferpfwABBkjryF

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_bferpfwABBkjryF
	je	.L_last_num_blocks_is_6_bferpfwABBkjryF
	jmp	.L_last_num_blocks_is_5_bferpfwABBkjryF

.L_last_num_blocks_is_3_1_bferpfwABBkjryF:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_bferpfwABBkjryF
	je	.L_last_num_blocks_is_2_bferpfwABBkjryF
.L_last_num_blocks_is_1_bferpfwABBkjryF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_kzEBaBxqykBnijA
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_kzEBaBxqykBnijA

.L_16_blocks_overflow_kzEBaBxqykBnijA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_kzEBaBxqykBnijA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nrixtAqkGwjzdEu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nrixtAqkGwjzdEu
.L_small_initial_partial_block_nrixtAqkGwjzdEu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_nrixtAqkGwjzdEu
.L_small_initial_compute_done_nrixtAqkGwjzdEu:
.L_after_reduction_nrixtAqkGwjzdEu:
	jmp	.L_last_blocks_done_bferpfwABBkjryF
.L_last_num_blocks_is_2_bferpfwABBkjryF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_tkmBkAzwxBiyEhd
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_tkmBkAzwxBiyEhd

.L_16_blocks_overflow_tkmBkAzwxBiyEhd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_tkmBkAzwxBiyEhd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fffrzmuvepqjddw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fffrzmuvepqjddw
.L_small_initial_partial_block_fffrzmuvepqjddw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_fffrzmuvepqjddw:

	orq	%r8,%r8
	je	.L_after_reduction_fffrzmuvepqjddw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_fffrzmuvepqjddw:
	jmp	.L_last_blocks_done_bferpfwABBkjryF
.L_last_num_blocks_is_3_bferpfwABBkjryF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_uzAyindigedDiFz
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_uzAyindigedDiFz

.L_16_blocks_overflow_uzAyindigedDiFz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_uzAyindigedDiFz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FDflpAaAkzkBABE





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FDflpAaAkzkBABE
.L_small_initial_partial_block_FDflpAaAkzkBABE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FDflpAaAkzkBABE:

	orq	%r8,%r8
	je	.L_after_reduction_FDflpAaAkzkBABE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FDflpAaAkzkBABE:
	jmp	.L_last_blocks_done_bferpfwABBkjryF
.L_last_num_blocks_is_4_bferpfwABBkjryF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_fihmyfCEvufkhpm
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_fihmyfCEvufkhpm

.L_16_blocks_overflow_fihmyfCEvufkhpm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_fihmyfCEvufkhpm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xFiEexFwBwwbazF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xFiEexFwBwwbazF
.L_small_initial_partial_block_xFiEexFwBwwbazF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xFiEexFwBwwbazF:

	orq	%r8,%r8
	je	.L_after_reduction_xFiEexFwBwwbazF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xFiEexFwBwwbazF:
	jmp	.L_last_blocks_done_bferpfwABBkjryF
.L_last_num_blocks_is_5_bferpfwABBkjryF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_pxspvriiuGEgFsd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_pxspvriiuGEgFsd

.L_16_blocks_overflow_pxspvriiuGEgFsd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_pxspvriiuGEgFsd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CcwfAlyhjjorqrn





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CcwfAlyhjjorqrn
.L_small_initial_partial_block_CcwfAlyhjjorqrn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CcwfAlyhjjorqrn:

	orq	%r8,%r8
	je	.L_after_reduction_CcwfAlyhjjorqrn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CcwfAlyhjjorqrn:
	jmp	.L_last_blocks_done_bferpfwABBkjryF
.L_last_num_blocks_is_6_bferpfwABBkjryF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_lGgpnyElBvyAGsc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_lGgpnyElBvyAGsc

.L_16_blocks_overflow_lGgpnyElBvyAGsc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_lGgpnyElBvyAGsc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lsmBhvgGxonGvAF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lsmBhvgGxonGvAF
.L_small_initial_partial_block_lsmBhvgGxonGvAF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lsmBhvgGxonGvAF:

	orq	%r8,%r8
	je	.L_after_reduction_lsmBhvgGxonGvAF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lsmBhvgGxonGvAF:
	jmp	.L_last_blocks_done_bferpfwABBkjryF
.L_last_num_blocks_is_7_bferpfwABBkjryF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_qhulnvkGgytcnkp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_qhulnvkGgytcnkp

.L_16_blocks_overflow_qhulnvkGgytcnkp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_qhulnvkGgytcnkp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cipEtqevsbfiAfC





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cipEtqevsbfiAfC
.L_small_initial_partial_block_cipEtqevsbfiAfC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cipEtqevsbfiAfC:

	orq	%r8,%r8
	je	.L_after_reduction_cipEtqevsbfiAfC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cipEtqevsbfiAfC:
	jmp	.L_last_blocks_done_bferpfwABBkjryF
.L_last_num_blocks_is_8_bferpfwABBkjryF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_fpndpqghliccACF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_fpndpqghliccACF

.L_16_blocks_overflow_fpndpqghliccACF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_fpndpqghliccACF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jdtBryrrqflxbyp





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jdtBryrrqflxbyp
.L_small_initial_partial_block_jdtBryrrqflxbyp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jdtBryrrqflxbyp:

	orq	%r8,%r8
	je	.L_after_reduction_jdtBryrrqflxbyp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jdtBryrrqflxbyp:
	jmp	.L_last_blocks_done_bferpfwABBkjryF
.L_last_num_blocks_is_9_bferpfwABBkjryF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_bnEkbkzntkCwBph
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_bnEkbkzntkCwBph

.L_16_blocks_overflow_bnEkbkzntkCwBph:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_bnEkbkzntkCwBph:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pFFFpnGavtfxtxd





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pFFFpnGavtfxtxd
.L_small_initial_partial_block_pFFFpnGavtfxtxd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pFFFpnGavtfxtxd:

	orq	%r8,%r8
	je	.L_after_reduction_pFFFpnGavtfxtxd
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_pFFFpnGavtfxtxd:
	jmp	.L_last_blocks_done_bferpfwABBkjryF
.L_last_num_blocks_is_10_bferpfwABBkjryF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_zEgwBkoikfyxFjc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_zEgwBkoikfyxFjc

.L_16_blocks_overflow_zEgwBkoikfyxFjc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_zEgwBkoikfyxFjc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cdmmzezumsiGpdi





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cdmmzezumsiGpdi
.L_small_initial_partial_block_cdmmzezumsiGpdi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cdmmzezumsiGpdi:

	orq	%r8,%r8
	je	.L_after_reduction_cdmmzezumsiGpdi
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cdmmzezumsiGpdi:
	jmp	.L_last_blocks_done_bferpfwABBkjryF
.L_last_num_blocks_is_11_bferpfwABBkjryF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_twscqvcnbqjutFe
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_twscqvcnbqjutFe

.L_16_blocks_overflow_twscqvcnbqjutFe:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_twscqvcnbqjutFe:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bkwwyvCGCpfDijx





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bkwwyvCGCpfDijx
.L_small_initial_partial_block_bkwwyvCGCpfDijx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bkwwyvCGCpfDijx:

	orq	%r8,%r8
	je	.L_after_reduction_bkwwyvCGCpfDijx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bkwwyvCGCpfDijx:
	jmp	.L_last_blocks_done_bferpfwABBkjryF
.L_last_num_blocks_is_12_bferpfwABBkjryF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_umgqlcderlBrcey
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_umgqlcderlBrcey

.L_16_blocks_overflow_umgqlcderlBrcey:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_umgqlcderlBrcey:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kvGEvszqhvxrveg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kvGEvszqhvxrveg
.L_small_initial_partial_block_kvGEvszqhvxrveg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kvGEvszqhvxrveg:

	orq	%r8,%r8
	je	.L_after_reduction_kvGEvszqhvxrveg
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kvGEvszqhvxrveg:
	jmp	.L_last_blocks_done_bferpfwABBkjryF
.L_last_num_blocks_is_13_bferpfwABBkjryF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_bxFbwpxaFnehncG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_bxFbwpxaFnehncG

.L_16_blocks_overflow_bxFbwpxaFnehncG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_bxFbwpxaFnehncG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dEcwnAbAwdAmfAu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dEcwnAbAwdAmfAu
.L_small_initial_partial_block_dEcwnAbAwdAmfAu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dEcwnAbAwdAmfAu:

	orq	%r8,%r8
	je	.L_after_reduction_dEcwnAbAwdAmfAu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dEcwnAbAwdAmfAu:
	jmp	.L_last_blocks_done_bferpfwABBkjryF
.L_last_num_blocks_is_14_bferpfwABBkjryF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_sosdsdytcqEvwab
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_sosdsdytcqEvwab

.L_16_blocks_overflow_sosdsdytcqEvwab:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_sosdsdytcqEvwab:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bgCpBqiAEjhCoEx





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bgCpBqiAEjhCoEx
.L_small_initial_partial_block_bgCpBqiAEjhCoEx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bgCpBqiAEjhCoEx:

	orq	%r8,%r8
	je	.L_after_reduction_bgCpBqiAEjhCoEx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bgCpBqiAEjhCoEx:
	jmp	.L_last_blocks_done_bferpfwABBkjryF
.L_last_num_blocks_is_15_bferpfwABBkjryF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_vGFCyADcqvDgFsB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_vGFCyADcqvDgFsB

.L_16_blocks_overflow_vGFCyADcqvDgFsB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_vGFCyADcqvDgFsB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_eaeilnokmxgnhoq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_eaeilnokmxgnhoq
.L_small_initial_partial_block_eaeilnokmxgnhoq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_eaeilnokmxgnhoq:

	orq	%r8,%r8
	je	.L_after_reduction_eaeilnokmxgnhoq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_eaeilnokmxgnhoq:
	jmp	.L_last_blocks_done_bferpfwABBkjryF
.L_last_num_blocks_is_16_bferpfwABBkjryF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_GcEfDsahFmphFmt
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_GcEfDsahFmphFmt

.L_16_blocks_overflow_GcEfDsahFmphFmt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_GcEfDsahFmphFmt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_cbEvdtctsgfziBj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cbEvdtctsgfziBj:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cbEvdtctsgfziBj:
	jmp	.L_last_blocks_done_bferpfwABBkjryF
.L_last_num_blocks_is_0_bferpfwABBkjryF:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_bferpfwABBkjryF:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_AgmocwsDmrmqxia
.L_encrypt_32_blocks_AgmocwsDmrmqxia:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_mpprsuzfnwwFgir
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_mpprsuzfnwwFgir
.L_16_blocks_overflow_mpprsuzfnwwFgir:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_mpprsuzfnwwFgir:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_bmgidupCyqEydhk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_bmgidupCyqEydhk
.L_16_blocks_overflow_bmgidupCyqEydhk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_bmgidupCyqEydhk:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%r11
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_iuahECnAqdulDkc

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_iuahECnAqdulDkc
	jb	.L_last_num_blocks_is_7_1_iuahECnAqdulDkc


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_iuahECnAqdulDkc
	jb	.L_last_num_blocks_is_11_9_iuahECnAqdulDkc


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_iuahECnAqdulDkc
	ja	.L_last_num_blocks_is_16_iuahECnAqdulDkc
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_iuahECnAqdulDkc
	jmp	.L_last_num_blocks_is_13_iuahECnAqdulDkc

.L_last_num_blocks_is_11_9_iuahECnAqdulDkc:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_iuahECnAqdulDkc
	ja	.L_last_num_blocks_is_11_iuahECnAqdulDkc
	jmp	.L_last_num_blocks_is_9_iuahECnAqdulDkc

.L_last_num_blocks_is_7_1_iuahECnAqdulDkc:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_iuahECnAqdulDkc
	jb	.L_last_num_blocks_is_3_1_iuahECnAqdulDkc

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_iuahECnAqdulDkc
	je	.L_last_num_blocks_is_6_iuahECnAqdulDkc
	jmp	.L_last_num_blocks_is_5_iuahECnAqdulDkc

.L_last_num_blocks_is_3_1_iuahECnAqdulDkc:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_iuahECnAqdulDkc
	je	.L_last_num_blocks_is_2_iuahECnAqdulDkc
.L_last_num_blocks_is_1_iuahECnAqdulDkc:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_kqqFrmiGhokckrq
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_kqqFrmiGhokckrq

.L_16_blocks_overflow_kqqFrmiGhokckrq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_kqqFrmiGhokckrq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CyzeilChcugpesA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CyzeilChcugpesA
.L_small_initial_partial_block_CyzeilChcugpesA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_CyzeilChcugpesA
.L_small_initial_compute_done_CyzeilChcugpesA:
.L_after_reduction_CyzeilChcugpesA:
	jmp	.L_last_blocks_done_iuahECnAqdulDkc
.L_last_num_blocks_is_2_iuahECnAqdulDkc:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_wyChgBrkewwfzcf
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_wyChgBrkewwfzcf

.L_16_blocks_overflow_wyChgBrkewwfzcf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_wyChgBrkewwfzcf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wiaxEtfAbpnbsAv





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wiaxEtfAbpnbsAv
.L_small_initial_partial_block_wiaxEtfAbpnbsAv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wiaxEtfAbpnbsAv:

	orq	%r8,%r8
	je	.L_after_reduction_wiaxEtfAbpnbsAv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_wiaxEtfAbpnbsAv:
	jmp	.L_last_blocks_done_iuahECnAqdulDkc
.L_last_num_blocks_is_3_iuahECnAqdulDkc:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_CgCyxxcEfGgsqzj
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_CgCyxxcEfGgsqzj

.L_16_blocks_overflow_CgCyxxcEfGgsqzj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_CgCyxxcEfGgsqzj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_udFyldlExdAnaje





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_udFyldlExdAnaje
.L_small_initial_partial_block_udFyldlExdAnaje:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_udFyldlExdAnaje:

	orq	%r8,%r8
	je	.L_after_reduction_udFyldlExdAnaje
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_udFyldlExdAnaje:
	jmp	.L_last_blocks_done_iuahECnAqdulDkc
.L_last_num_blocks_is_4_iuahECnAqdulDkc:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_ujDdbvkgnvazEyz
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_ujDdbvkgnvazEyz

.L_16_blocks_overflow_ujDdbvkgnvazEyz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_ujDdbvkgnvazEyz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vDjxcAbzkAltovc





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vDjxcAbzkAltovc
.L_small_initial_partial_block_vDjxcAbzkAltovc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vDjxcAbzkAltovc:

	orq	%r8,%r8
	je	.L_after_reduction_vDjxcAbzkAltovc
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vDjxcAbzkAltovc:
	jmp	.L_last_blocks_done_iuahECnAqdulDkc
.L_last_num_blocks_is_5_iuahECnAqdulDkc:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_khzdpkjpegAElbF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_khzdpkjpegAElbF

.L_16_blocks_overflow_khzdpkjpegAElbF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_khzdpkjpegAElbF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GgsCgrsEfArktbn





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GgsCgrsEfArktbn
.L_small_initial_partial_block_GgsCgrsEfArktbn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GgsCgrsEfArktbn:

	orq	%r8,%r8
	je	.L_after_reduction_GgsCgrsEfArktbn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GgsCgrsEfArktbn:
	jmp	.L_last_blocks_done_iuahECnAqdulDkc
.L_last_num_blocks_is_6_iuahECnAqdulDkc:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_oqvCorqjhxbcBxm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_oqvCorqjhxbcBxm

.L_16_blocks_overflow_oqvCorqjhxbcBxm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_oqvCorqjhxbcBxm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_eACvpoBiwlBkgxs





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_eACvpoBiwlBkgxs
.L_small_initial_partial_block_eACvpoBiwlBkgxs:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_eACvpoBiwlBkgxs:

	orq	%r8,%r8
	je	.L_after_reduction_eACvpoBiwlBkgxs
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_eACvpoBiwlBkgxs:
	jmp	.L_last_blocks_done_iuahECnAqdulDkc
.L_last_num_blocks_is_7_iuahECnAqdulDkc:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_Ctvmzfebvzbpswb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_Ctvmzfebvzbpswb

.L_16_blocks_overflow_Ctvmzfebvzbpswb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_Ctvmzfebvzbpswb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nhlwfogfCdekEkf





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nhlwfogfCdekEkf
.L_small_initial_partial_block_nhlwfogfCdekEkf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nhlwfogfCdekEkf:

	orq	%r8,%r8
	je	.L_after_reduction_nhlwfogfCdekEkf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_nhlwfogfCdekEkf:
	jmp	.L_last_blocks_done_iuahECnAqdulDkc
.L_last_num_blocks_is_8_iuahECnAqdulDkc:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_qmdxhopakCkbDil
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_qmdxhopakCkbDil

.L_16_blocks_overflow_qmdxhopakCkbDil:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_qmdxhopakCkbDil:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GkDoidjgrBnlytw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GkDoidjgrBnlytw
.L_small_initial_partial_block_GkDoidjgrBnlytw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GkDoidjgrBnlytw:

	orq	%r8,%r8
	je	.L_after_reduction_GkDoidjgrBnlytw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GkDoidjgrBnlytw:
	jmp	.L_last_blocks_done_iuahECnAqdulDkc
.L_last_num_blocks_is_9_iuahECnAqdulDkc:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_EeqzmmzhgcBsewy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_EeqzmmzhgcBsewy

.L_16_blocks_overflow_EeqzmmzhgcBsewy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_EeqzmmzhgcBsewy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DsqubglCsmEBAbw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DsqubglCsmEBAbw
.L_small_initial_partial_block_DsqubglCsmEBAbw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DsqubglCsmEBAbw:

	orq	%r8,%r8
	je	.L_after_reduction_DsqubglCsmEBAbw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DsqubglCsmEBAbw:
	jmp	.L_last_blocks_done_iuahECnAqdulDkc
.L_last_num_blocks_is_10_iuahECnAqdulDkc:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_zvyohmttdchafbE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_zvyohmttdchafbE

.L_16_blocks_overflow_zvyohmttdchafbE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_zvyohmttdchafbE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fjgfyiryAnwmvfl





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fjgfyiryAnwmvfl
.L_small_initial_partial_block_fjgfyiryAnwmvfl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_fjgfyiryAnwmvfl:

	orq	%r8,%r8
	je	.L_after_reduction_fjgfyiryAnwmvfl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_fjgfyiryAnwmvfl:
	jmp	.L_last_blocks_done_iuahECnAqdulDkc
.L_last_num_blocks_is_11_iuahECnAqdulDkc:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_bdbrvojbkusxoar
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_bdbrvojbkusxoar

.L_16_blocks_overflow_bdbrvojbkusxoar:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_bdbrvojbkusxoar:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qEvrxjuouDoCFmw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qEvrxjuouDoCFmw
.L_small_initial_partial_block_qEvrxjuouDoCFmw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qEvrxjuouDoCFmw:

	orq	%r8,%r8
	je	.L_after_reduction_qEvrxjuouDoCFmw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_qEvrxjuouDoCFmw:
	jmp	.L_last_blocks_done_iuahECnAqdulDkc
.L_last_num_blocks_is_12_iuahECnAqdulDkc:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_yzdGylvwfjjvDGs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_yzdGylvwfjjvDGs

.L_16_blocks_overflow_yzdGylvwfjjvDGs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_yzdGylvwfjjvDGs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wsjvarGzrnjuemc





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wsjvarGzrnjuemc
.L_small_initial_partial_block_wsjvarGzrnjuemc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wsjvarGzrnjuemc:

	orq	%r8,%r8
	je	.L_after_reduction_wsjvarGzrnjuemc
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_wsjvarGzrnjuemc:
	jmp	.L_last_blocks_done_iuahECnAqdulDkc
.L_last_num_blocks_is_13_iuahECnAqdulDkc:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_wDxvfpexevznlvG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_wDxvfpexevznlvG

.L_16_blocks_overflow_wDxvfpexevznlvG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_wDxvfpexevznlvG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zulbbjAsflAvupi





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zulbbjAsflAvupi
.L_small_initial_partial_block_zulbbjAsflAvupi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zulbbjAsflAvupi:

	orq	%r8,%r8
	je	.L_after_reduction_zulbbjAsflAvupi
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zulbbjAsflAvupi:
	jmp	.L_last_blocks_done_iuahECnAqdulDkc
.L_last_num_blocks_is_14_iuahECnAqdulDkc:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_DgyfxsDpljgitBs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_DgyfxsDpljgitBs

.L_16_blocks_overflow_DgyfxsDpljgitBs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_DgyfxsDpljgitBs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ykjDhfGFkregrce





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ykjDhfGFkregrce
.L_small_initial_partial_block_ykjDhfGFkregrce:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ykjDhfGFkregrce:

	orq	%r8,%r8
	je	.L_after_reduction_ykjDhfGFkregrce
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ykjDhfGFkregrce:
	jmp	.L_last_blocks_done_iuahECnAqdulDkc
.L_last_num_blocks_is_15_iuahECnAqdulDkc:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_qpvuBoeeoAigsaz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_qpvuBoeeoAigsaz

.L_16_blocks_overflow_qpvuBoeeoAigsaz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_qpvuBoeeoAigsaz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jDkGAfpAGjcCEfF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jDkGAfpAGjcCEfF
.L_small_initial_partial_block_jDkGAfpAGjcCEfF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jDkGAfpAGjcCEfF:

	orq	%r8,%r8
	je	.L_after_reduction_jDkGAfpAGjcCEfF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jDkGAfpAGjcCEfF:
	jmp	.L_last_blocks_done_iuahECnAqdulDkc
.L_last_num_blocks_is_16_iuahECnAqdulDkc:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_EAAvDjlfvBhovEl
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_EAAvDjlfvBhovEl

.L_16_blocks_overflow_EAAvDjlfvBhovEl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_EAAvDjlfvBhovEl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_AjBDjxdkekbwFwr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AjBDjxdkekbwFwr:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_AjBDjxdkekbwFwr:
	jmp	.L_last_blocks_done_iuahECnAqdulDkc
.L_last_num_blocks_is_0_iuahECnAqdulDkc:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_iuahECnAqdulDkc:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_AgmocwsDmrmqxia
.L_encrypt_16_blocks_AgmocwsDmrmqxia:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_mcxvogegvmqCgjl
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_mcxvogegvmqCgjl
.L_16_blocks_overflow_mcxvogegvmqCgjl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_mcxvogegvmqCgjl:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_FmppfDfiFrqrunD

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_FmppfDfiFrqrunD
	jb	.L_last_num_blocks_is_7_1_FmppfDfiFrqrunD


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_FmppfDfiFrqrunD
	jb	.L_last_num_blocks_is_11_9_FmppfDfiFrqrunD


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_FmppfDfiFrqrunD
	ja	.L_last_num_blocks_is_16_FmppfDfiFrqrunD
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_FmppfDfiFrqrunD
	jmp	.L_last_num_blocks_is_13_FmppfDfiFrqrunD

.L_last_num_blocks_is_11_9_FmppfDfiFrqrunD:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_FmppfDfiFrqrunD
	ja	.L_last_num_blocks_is_11_FmppfDfiFrqrunD
	jmp	.L_last_num_blocks_is_9_FmppfDfiFrqrunD

.L_last_num_blocks_is_7_1_FmppfDfiFrqrunD:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_FmppfDfiFrqrunD
	jb	.L_last_num_blocks_is_3_1_FmppfDfiFrqrunD

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_FmppfDfiFrqrunD
	je	.L_last_num_blocks_is_6_FmppfDfiFrqrunD
	jmp	.L_last_num_blocks_is_5_FmppfDfiFrqrunD

.L_last_num_blocks_is_3_1_FmppfDfiFrqrunD:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_FmppfDfiFrqrunD
	je	.L_last_num_blocks_is_2_FmppfDfiFrqrunD
.L_last_num_blocks_is_1_FmppfDfiFrqrunD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_ldnCbgjqFFzeGkE
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_ldnCbgjqFFzeGkE

.L_16_blocks_overflow_ldnCbgjqFFzeGkE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_ldnCbgjqFFzeGkE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_moDuBsnrhCduAbf





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_moDuBsnrhCduAbf
.L_small_initial_partial_block_moDuBsnrhCduAbf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_moDuBsnrhCduAbf
.L_small_initial_compute_done_moDuBsnrhCduAbf:
.L_after_reduction_moDuBsnrhCduAbf:
	jmp	.L_last_blocks_done_FmppfDfiFrqrunD
.L_last_num_blocks_is_2_FmppfDfiFrqrunD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_aliuFCltFsjolFE
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_aliuFCltFsjolFE

.L_16_blocks_overflow_aliuFCltFsjolFE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_aliuFCltFsjolFE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EfhwrDwomqygiyg





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EfhwrDwomqygiyg
.L_small_initial_partial_block_EfhwrDwomqygiyg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EfhwrDwomqygiyg:

	orq	%r8,%r8
	je	.L_after_reduction_EfhwrDwomqygiyg
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_EfhwrDwomqygiyg:
	jmp	.L_last_blocks_done_FmppfDfiFrqrunD
.L_last_num_blocks_is_3_FmppfDfiFrqrunD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_sxiadEDbujzaFdv
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_sxiadEDbujzaFdv

.L_16_blocks_overflow_sxiadEDbujzaFdv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_sxiadEDbujzaFdv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_beDsxFiAgwmuGeu





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_beDsxFiAgwmuGeu
.L_small_initial_partial_block_beDsxFiAgwmuGeu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_beDsxFiAgwmuGeu:

	orq	%r8,%r8
	je	.L_after_reduction_beDsxFiAgwmuGeu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_beDsxFiAgwmuGeu:
	jmp	.L_last_blocks_done_FmppfDfiFrqrunD
.L_last_num_blocks_is_4_FmppfDfiFrqrunD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_xadpnysddqAwGmG
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_xadpnysddqAwGmG

.L_16_blocks_overflow_xadpnysddqAwGmG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_xadpnysddqAwGmG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EBadkpjzffkliko





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EBadkpjzffkliko
.L_small_initial_partial_block_EBadkpjzffkliko:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EBadkpjzffkliko:

	orq	%r8,%r8
	je	.L_after_reduction_EBadkpjzffkliko
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_EBadkpjzffkliko:
	jmp	.L_last_blocks_done_FmppfDfiFrqrunD
.L_last_num_blocks_is_5_FmppfDfiFrqrunD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_hksmAxFpGirGmjv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_hksmAxFpGirGmjv

.L_16_blocks_overflow_hksmAxFpGirGmjv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_hksmAxFpGirGmjv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jixejnpEsGcelrf





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jixejnpEsGcelrf
.L_small_initial_partial_block_jixejnpEsGcelrf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jixejnpEsGcelrf:

	orq	%r8,%r8
	je	.L_after_reduction_jixejnpEsGcelrf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jixejnpEsGcelrf:
	jmp	.L_last_blocks_done_FmppfDfiFrqrunD
.L_last_num_blocks_is_6_FmppfDfiFrqrunD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_iCDwkEqpCeByAmv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_iCDwkEqpCeByAmv

.L_16_blocks_overflow_iCDwkEqpCeByAmv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_iCDwkEqpCeByAmv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vgmoCpyoebdsykz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vgmoCpyoebdsykz
.L_small_initial_partial_block_vgmoCpyoebdsykz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vgmoCpyoebdsykz:

	orq	%r8,%r8
	je	.L_after_reduction_vgmoCpyoebdsykz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vgmoCpyoebdsykz:
	jmp	.L_last_blocks_done_FmppfDfiFrqrunD
.L_last_num_blocks_is_7_FmppfDfiFrqrunD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_qqtmkarhonickAs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_qqtmkarhonickAs

.L_16_blocks_overflow_qqtmkarhonickAs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_qqtmkarhonickAs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_mfFkFCGzhdriGlf





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_mfFkFCGzhdriGlf
.L_small_initial_partial_block_mfFkFCGzhdriGlf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mfFkFCGzhdriGlf:

	orq	%r8,%r8
	je	.L_after_reduction_mfFkFCGzhdriGlf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_mfFkFCGzhdriGlf:
	jmp	.L_last_blocks_done_FmppfDfiFrqrunD
.L_last_num_blocks_is_8_FmppfDfiFrqrunD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_rkptdrerxbdboae
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_rkptdrerxbdboae

.L_16_blocks_overflow_rkptdrerxbdboae:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_rkptdrerxbdboae:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_oGtypulltGpidfc





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_oGtypulltGpidfc
.L_small_initial_partial_block_oGtypulltGpidfc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_oGtypulltGpidfc:

	orq	%r8,%r8
	je	.L_after_reduction_oGtypulltGpidfc
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_oGtypulltGpidfc:
	jmp	.L_last_blocks_done_FmppfDfiFrqrunD
.L_last_num_blocks_is_9_FmppfDfiFrqrunD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_cibFbuboytxixsp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_cibFbuboytxixsp

.L_16_blocks_overflow_cibFbuboytxixsp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_cibFbuboytxixsp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BbFfbzpAeCyrtAm





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BbFfbzpAeCyrtAm
.L_small_initial_partial_block_BbFfbzpAeCyrtAm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BbFfbzpAeCyrtAm:

	orq	%r8,%r8
	je	.L_after_reduction_BbFfbzpAeCyrtAm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BbFfbzpAeCyrtAm:
	jmp	.L_last_blocks_done_FmppfDfiFrqrunD
.L_last_num_blocks_is_10_FmppfDfiFrqrunD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_cnnfhlmgaBnjDAr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_cnnfhlmgaBnjDAr

.L_16_blocks_overflow_cnnfhlmgaBnjDAr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_cnnfhlmgaBnjDAr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cbxlsqjackCnzuc





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cbxlsqjackCnzuc
.L_small_initial_partial_block_cbxlsqjackCnzuc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cbxlsqjackCnzuc:

	orq	%r8,%r8
	je	.L_after_reduction_cbxlsqjackCnzuc
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cbxlsqjackCnzuc:
	jmp	.L_last_blocks_done_FmppfDfiFrqrunD
.L_last_num_blocks_is_11_FmppfDfiFrqrunD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_ehiiapflbcoEbce
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_ehiiapflbcoEbce

.L_16_blocks_overflow_ehiiapflbcoEbce:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_ehiiapflbcoEbce:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fDDcFaygvvuyAwG





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fDDcFaygvvuyAwG
.L_small_initial_partial_block_fDDcFaygvvuyAwG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_fDDcFaygvvuyAwG:

	orq	%r8,%r8
	je	.L_after_reduction_fDDcFaygvvuyAwG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_fDDcFaygvvuyAwG:
	jmp	.L_last_blocks_done_FmppfDfiFrqrunD
.L_last_num_blocks_is_12_FmppfDfiFrqrunD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_cmdoadEwwthzcAC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_cmdoadEwwthzcAC

.L_16_blocks_overflow_cmdoadEwwthzcAC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_cmdoadEwwthzcAC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DFajAlxDuyffEqe





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DFajAlxDuyffEqe
.L_small_initial_partial_block_DFajAlxDuyffEqe:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DFajAlxDuyffEqe:

	orq	%r8,%r8
	je	.L_after_reduction_DFajAlxDuyffEqe
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DFajAlxDuyffEqe:
	jmp	.L_last_blocks_done_FmppfDfiFrqrunD
.L_last_num_blocks_is_13_FmppfDfiFrqrunD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_EqwfywcwABxpdeG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_EqwfywcwABxpdeG

.L_16_blocks_overflow_EqwfywcwABxpdeG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_EqwfywcwABxpdeG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cmufevqGAqbhCFc





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cmufevqGAqbhCFc
.L_small_initial_partial_block_cmufevqGAqbhCFc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cmufevqGAqbhCFc:

	orq	%r8,%r8
	je	.L_after_reduction_cmufevqGAqbhCFc
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cmufevqGAqbhCFc:
	jmp	.L_last_blocks_done_FmppfDfiFrqrunD
.L_last_num_blocks_is_14_FmppfDfiFrqrunD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_ohkBnDjyekjctbn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_ohkBnDjyekjctbn

.L_16_blocks_overflow_ohkBnDjyekjctbn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_ohkBnDjyekjctbn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GAdykyBiGdtbFGk





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GAdykyBiGdtbFGk
.L_small_initial_partial_block_GAdykyBiGdtbFGk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GAdykyBiGdtbFGk:

	orq	%r8,%r8
	je	.L_after_reduction_GAdykyBiGdtbFGk
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GAdykyBiGdtbFGk:
	jmp	.L_last_blocks_done_FmppfDfiFrqrunD
.L_last_num_blocks_is_15_FmppfDfiFrqrunD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_cfyfEDbbAezFzuc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_cfyfEDbbAezFzuc

.L_16_blocks_overflow_cfyfEDbbAezFzuc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_cfyfEDbbAezFzuc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kbnnFaqjlpiGhEc





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kbnnFaqjlpiGhEc
.L_small_initial_partial_block_kbnnFaqjlpiGhEc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kbnnFaqjlpiGhEc:

	orq	%r8,%r8
	je	.L_after_reduction_kbnnFaqjlpiGhEc
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kbnnFaqjlpiGhEc:
	jmp	.L_last_blocks_done_FmppfDfiFrqrunD
.L_last_num_blocks_is_16_FmppfDfiFrqrunD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_ADCloudtuaqEigx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_ADCloudtuaqEigx

.L_16_blocks_overflow_ADCloudtuaqEigx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_ADCloudtuaqEigx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_DexFnwnGEvDiBxy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DexFnwnGEvDiBxy:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DexFnwnGEvDiBxy:
	jmp	.L_last_blocks_done_FmppfDfiFrqrunD
.L_last_num_blocks_is_0_FmppfDfiFrqrunD:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_FmppfDfiFrqrunD:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_AgmocwsDmrmqxia

.L_message_below_32_blocks_AgmocwsDmrmqxia:


	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_hbvggauFhBhnjdz
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
.L_skip_hkeys_precomputation_hbvggauFhBhnjdz:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_FwmqnGjABafdyqs

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_FwmqnGjABafdyqs
	jb	.L_last_num_blocks_is_7_1_FwmqnGjABafdyqs


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_FwmqnGjABafdyqs
	jb	.L_last_num_blocks_is_11_9_FwmqnGjABafdyqs


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_FwmqnGjABafdyqs
	ja	.L_last_num_blocks_is_16_FwmqnGjABafdyqs
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_FwmqnGjABafdyqs
	jmp	.L_last_num_blocks_is_13_FwmqnGjABafdyqs

.L_last_num_blocks_is_11_9_FwmqnGjABafdyqs:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_FwmqnGjABafdyqs
	ja	.L_last_num_blocks_is_11_FwmqnGjABafdyqs
	jmp	.L_last_num_blocks_is_9_FwmqnGjABafdyqs

.L_last_num_blocks_is_7_1_FwmqnGjABafdyqs:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_FwmqnGjABafdyqs
	jb	.L_last_num_blocks_is_3_1_FwmqnGjABafdyqs

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_FwmqnGjABafdyqs
	je	.L_last_num_blocks_is_6_FwmqnGjABafdyqs
	jmp	.L_last_num_blocks_is_5_FwmqnGjABafdyqs

.L_last_num_blocks_is_3_1_FwmqnGjABafdyqs:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_FwmqnGjABafdyqs
	je	.L_last_num_blocks_is_2_FwmqnGjABafdyqs
.L_last_num_blocks_is_1_FwmqnGjABafdyqs:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_hkcaEcembkgfCEd
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_hkcaEcembkgfCEd

.L_16_blocks_overflow_hkcaEcembkgfCEd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_hkcaEcembkgfCEd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cylEhBilBykefzx





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cylEhBilBykefzx
.L_small_initial_partial_block_cylEhBilBykefzx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_cylEhBilBykefzx
.L_small_initial_compute_done_cylEhBilBykefzx:
.L_after_reduction_cylEhBilBykefzx:
	jmp	.L_last_blocks_done_FwmqnGjABafdyqs
.L_last_num_blocks_is_2_FwmqnGjABafdyqs:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_boCyDtbFdcokFCo
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_boCyDtbFdcokFCo

.L_16_blocks_overflow_boCyDtbFdcokFCo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_boCyDtbFdcokFCo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yBvmDppAvrEionm





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yBvmDppAvrEionm
.L_small_initial_partial_block_yBvmDppAvrEionm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yBvmDppAvrEionm:

	orq	%r8,%r8
	je	.L_after_reduction_yBvmDppAvrEionm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_yBvmDppAvrEionm:
	jmp	.L_last_blocks_done_FwmqnGjABafdyqs
.L_last_num_blocks_is_3_FwmqnGjABafdyqs:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_sGuohFicDybGjzc
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_sGuohFicDybGjzc

.L_16_blocks_overflow_sGuohFicDybGjzc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_sGuohFicDybGjzc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_uBwkimkbzbsesyl





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_uBwkimkbzbsesyl
.L_small_initial_partial_block_uBwkimkbzbsesyl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_uBwkimkbzbsesyl:

	orq	%r8,%r8
	je	.L_after_reduction_uBwkimkbzbsesyl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_uBwkimkbzbsesyl:
	jmp	.L_last_blocks_done_FwmqnGjABafdyqs
.L_last_num_blocks_is_4_FwmqnGjABafdyqs:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_pujgnrvwcjjGumi
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_pujgnrvwcjjGumi

.L_16_blocks_overflow_pujgnrvwcjjGumi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_pujgnrvwcjjGumi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vfsgttpFphblDhk





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vfsgttpFphblDhk
.L_small_initial_partial_block_vfsgttpFphblDhk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vfsgttpFphblDhk:

	orq	%r8,%r8
	je	.L_after_reduction_vfsgttpFphblDhk
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vfsgttpFphblDhk:
	jmp	.L_last_blocks_done_FwmqnGjABafdyqs
.L_last_num_blocks_is_5_FwmqnGjABafdyqs:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_eukFozuxEgEvmqh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_eukFozuxEgEvmqh

.L_16_blocks_overflow_eukFozuxEgEvmqh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_eukFozuxEgEvmqh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dejoehrEGwxwpkA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dejoehrEGwxwpkA
.L_small_initial_partial_block_dejoehrEGwxwpkA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dejoehrEGwxwpkA:

	orq	%r8,%r8
	je	.L_after_reduction_dejoehrEGwxwpkA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dejoehrEGwxwpkA:
	jmp	.L_last_blocks_done_FwmqnGjABafdyqs
.L_last_num_blocks_is_6_FwmqnGjABafdyqs:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_nkiikbbwshxbbwj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_nkiikbbwshxbbwj

.L_16_blocks_overflow_nkiikbbwshxbbwj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_nkiikbbwshxbbwj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_uDhjFFGAmcDeuoj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_uDhjFFGAmcDeuoj
.L_small_initial_partial_block_uDhjFFGAmcDeuoj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_uDhjFFGAmcDeuoj:

	orq	%r8,%r8
	je	.L_after_reduction_uDhjFFGAmcDeuoj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_uDhjFFGAmcDeuoj:
	jmp	.L_last_blocks_done_FwmqnGjABafdyqs
.L_last_num_blocks_is_7_FwmqnGjABafdyqs:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_fnpukpqqfkBteko
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_fnpukpqqfkBteko

.L_16_blocks_overflow_fnpukpqqfkBteko:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_fnpukpqqfkBteko:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CqrfrBribqbpdfe





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CqrfrBribqbpdfe
.L_small_initial_partial_block_CqrfrBribqbpdfe:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CqrfrBribqbpdfe:

	orq	%r8,%r8
	je	.L_after_reduction_CqrfrBribqbpdfe
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CqrfrBribqbpdfe:
	jmp	.L_last_blocks_done_FwmqnGjABafdyqs
.L_last_num_blocks_is_8_FwmqnGjABafdyqs:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_jhvtyeevCDpEEgE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_jhvtyeevCDpEEgE

.L_16_blocks_overflow_jhvtyeevCDpEEgE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_jhvtyeevCDpEEgE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pkdlkCuhormojoo





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pkdlkCuhormojoo
.L_small_initial_partial_block_pkdlkCuhormojoo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pkdlkCuhormojoo:

	orq	%r8,%r8
	je	.L_after_reduction_pkdlkCuhormojoo
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_pkdlkCuhormojoo:
	jmp	.L_last_blocks_done_FwmqnGjABafdyqs
.L_last_num_blocks_is_9_FwmqnGjABafdyqs:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_uamurBjBEihqvcD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_uamurBjBEihqvcD

.L_16_blocks_overflow_uamurBjBEihqvcD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_uamurBjBEihqvcD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_mdwdgsjxBtycjbB





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_mdwdgsjxBtycjbB
.L_small_initial_partial_block_mdwdgsjxBtycjbB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mdwdgsjxBtycjbB:

	orq	%r8,%r8
	je	.L_after_reduction_mdwdgsjxBtycjbB
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_mdwdgsjxBtycjbB:
	jmp	.L_last_blocks_done_FwmqnGjABafdyqs
.L_last_num_blocks_is_10_FwmqnGjABafdyqs:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_tzhvyGGrlulvEFv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_tzhvyGGrlulvEFv

.L_16_blocks_overflow_tzhvyGGrlulvEFv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_tzhvyGGrlulvEFv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tAeFcteyAFxjCvi





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tAeFcteyAFxjCvi
.L_small_initial_partial_block_tAeFcteyAFxjCvi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tAeFcteyAFxjCvi:

	orq	%r8,%r8
	je	.L_after_reduction_tAeFcteyAFxjCvi
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tAeFcteyAFxjCvi:
	jmp	.L_last_blocks_done_FwmqnGjABafdyqs
.L_last_num_blocks_is_11_FwmqnGjABafdyqs:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_hoFtwjaqteCrGGf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_hoFtwjaqteCrGGf

.L_16_blocks_overflow_hoFtwjaqteCrGGf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_hoFtwjaqteCrGGf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_sxAxqpjqoyoBqrB





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_sxAxqpjqoyoBqrB
.L_small_initial_partial_block_sxAxqpjqoyoBqrB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_sxAxqpjqoyoBqrB:

	orq	%r8,%r8
	je	.L_after_reduction_sxAxqpjqoyoBqrB
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_sxAxqpjqoyoBqrB:
	jmp	.L_last_blocks_done_FwmqnGjABafdyqs
.L_last_num_blocks_is_12_FwmqnGjABafdyqs:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_vGmgsDEriEgBlnD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_vGmgsDEriEgBlnD

.L_16_blocks_overflow_vGmgsDEriEgBlnD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_vGmgsDEriEgBlnD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cdlwBzjmDnfqwGx





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cdlwBzjmDnfqwGx
.L_small_initial_partial_block_cdlwBzjmDnfqwGx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cdlwBzjmDnfqwGx:

	orq	%r8,%r8
	je	.L_after_reduction_cdlwBzjmDnfqwGx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cdlwBzjmDnfqwGx:
	jmp	.L_last_blocks_done_FwmqnGjABafdyqs
.L_last_num_blocks_is_13_FwmqnGjABafdyqs:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_ctpsypwrqiCkmbx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_ctpsypwrqiCkmbx

.L_16_blocks_overflow_ctpsypwrqiCkmbx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_ctpsypwrqiCkmbx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dyevbpxCzlebbdG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dyevbpxCzlebbdG
.L_small_initial_partial_block_dyevbpxCzlebbdG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dyevbpxCzlebbdG:

	orq	%r8,%r8
	je	.L_after_reduction_dyevbpxCzlebbdG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dyevbpxCzlebbdG:
	jmp	.L_last_blocks_done_FwmqnGjABafdyqs
.L_last_num_blocks_is_14_FwmqnGjABafdyqs:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_AlCCxkymjujFpea
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_AlCCxkymjujFpea

.L_16_blocks_overflow_AlCCxkymjujFpea:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_AlCCxkymjujFpea:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_opGawevlfanEuyi





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_opGawevlfanEuyi
.L_small_initial_partial_block_opGawevlfanEuyi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_opGawevlfanEuyi:

	orq	%r8,%r8
	je	.L_after_reduction_opGawevlfanEuyi
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_opGawevlfanEuyi:
	jmp	.L_last_blocks_done_FwmqnGjABafdyqs
.L_last_num_blocks_is_15_FwmqnGjABafdyqs:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_xhoAFwmhvFqsllh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_xhoAFwmhvFqsllh

.L_16_blocks_overflow_xhoAFwmhvFqsllh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_xhoAFwmhvFqsllh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_obxzbAmlBdfEDxk





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_obxzbAmlBdfEDxk
.L_small_initial_partial_block_obxzbAmlBdfEDxk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_obxzbAmlBdfEDxk:

	orq	%r8,%r8
	je	.L_after_reduction_obxzbAmlBdfEDxk
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_obxzbAmlBdfEDxk:
	jmp	.L_last_blocks_done_FwmqnGjABafdyqs
.L_last_num_blocks_is_16_FwmqnGjABafdyqs:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_xnAvkkgDcvGwfhA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_xnAvkkgDcvGwfhA

.L_16_blocks_overflow_xnAvkkgDcvGwfhA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_xnAvkkgDcvGwfhA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_lAhrdGpvyxjGtCD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lAhrdGpvyxjGtCD:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lAhrdGpvyxjGtCD:
	jmp	.L_last_blocks_done_FwmqnGjABafdyqs
.L_last_num_blocks_is_0_FwmqnGjABafdyqs:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_FwmqnGjABafdyqs:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_AgmocwsDmrmqxia

.L_message_below_equal_16_blocks_AgmocwsDmrmqxia:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	.L_small_initial_num_blocks_is_8_apEeDkAppehCuuj
	jl	.L_small_initial_num_blocks_is_7_1_apEeDkAppehCuuj


	cmpq	$12,%r12
	je	.L_small_initial_num_blocks_is_12_apEeDkAppehCuuj
	jl	.L_small_initial_num_blocks_is_11_9_apEeDkAppehCuuj


	cmpq	$16,%r12
	je	.L_small_initial_num_blocks_is_16_apEeDkAppehCuuj
	cmpq	$15,%r12
	je	.L_small_initial_num_blocks_is_15_apEeDkAppehCuuj
	cmpq	$14,%r12
	je	.L_small_initial_num_blocks_is_14_apEeDkAppehCuuj
	jmp	.L_small_initial_num_blocks_is_13_apEeDkAppehCuuj

.L_small_initial_num_blocks_is_11_9_apEeDkAppehCuuj:

	cmpq	$11,%r12
	je	.L_small_initial_num_blocks_is_11_apEeDkAppehCuuj
	cmpq	$10,%r12
	je	.L_small_initial_num_blocks_is_10_apEeDkAppehCuuj
	jmp	.L_small_initial_num_blocks_is_9_apEeDkAppehCuuj

.L_small_initial_num_blocks_is_7_1_apEeDkAppehCuuj:
	cmpq	$4,%r12
	je	.L_small_initial_num_blocks_is_4_apEeDkAppehCuuj
	jl	.L_small_initial_num_blocks_is_3_1_apEeDkAppehCuuj

	cmpq	$7,%r12
	je	.L_small_initial_num_blocks_is_7_apEeDkAppehCuuj
	cmpq	$6,%r12
	je	.L_small_initial_num_blocks_is_6_apEeDkAppehCuuj
	jmp	.L_small_initial_num_blocks_is_5_apEeDkAppehCuuj

.L_small_initial_num_blocks_is_3_1_apEeDkAppehCuuj:

	cmpq	$3,%r12
	je	.L_small_initial_num_blocks_is_3_apEeDkAppehCuuj
	cmpq	$2,%r12
	je	.L_small_initial_num_blocks_is_2_apEeDkAppehCuuj





.L_small_initial_num_blocks_is_1_apEeDkAppehCuuj:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%r11,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qhpAzgodrEFBoAh





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qhpAzgodrEFBoAh
.L_small_initial_partial_block_qhpAzgodrEFBoAh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	.L_after_reduction_qhpAzgodrEFBoAh
.L_small_initial_compute_done_qhpAzgodrEFBoAh:
.L_after_reduction_qhpAzgodrEFBoAh:
	jmp	.L_small_initial_blocks_encrypted_apEeDkAppehCuuj
.L_small_initial_num_blocks_is_2_apEeDkAppehCuuj:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%r11,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_sicyfyBwlelBaDp





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_sicyfyBwlelBaDp
.L_small_initial_partial_block_sicyfyBwlelBaDp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_sicyfyBwlelBaDp:

	orq	%r8,%r8
	je	.L_after_reduction_sicyfyBwlelBaDp
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_sicyfyBwlelBaDp:
	jmp	.L_small_initial_blocks_encrypted_apEeDkAppehCuuj
.L_small_initial_num_blocks_is_3_apEeDkAppehCuuj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_hvfEubpwEwckzem





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_hvfEubpwEwckzem
.L_small_initial_partial_block_hvfEubpwEwckzem:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_hvfEubpwEwckzem:

	orq	%r8,%r8
	je	.L_after_reduction_hvfEubpwEwckzem
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_hvfEubpwEwckzem:
	jmp	.L_small_initial_blocks_encrypted_apEeDkAppehCuuj
.L_small_initial_num_blocks_is_4_apEeDkAppehCuuj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qaDdppyxtEifmuy





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qaDdppyxtEifmuy
.L_small_initial_partial_block_qaDdppyxtEifmuy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qaDdppyxtEifmuy:

	orq	%r8,%r8
	je	.L_after_reduction_qaDdppyxtEifmuy
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_qaDdppyxtEifmuy:
	jmp	.L_small_initial_blocks_encrypted_apEeDkAppehCuuj
.L_small_initial_num_blocks_is_5_apEeDkAppehCuuj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%xmm29,%xmm3,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CuomhubikyAmmAb





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CuomhubikyAmmAb
.L_small_initial_partial_block_CuomhubikyAmmAb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CuomhubikyAmmAb:

	orq	%r8,%r8
	je	.L_after_reduction_CuomhubikyAmmAb
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_CuomhubikyAmmAb:
	jmp	.L_small_initial_blocks_encrypted_apEeDkAppehCuuj
.L_small_initial_num_blocks_is_6_apEeDkAppehCuuj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%ymm29,%ymm3,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yhdziFkFaGtruAd





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yhdziFkFaGtruAd
.L_small_initial_partial_block_yhdziFkFaGtruAd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yhdziFkFaGtruAd:

	orq	%r8,%r8
	je	.L_after_reduction_yhdziFkFaGtruAd
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_yhdziFkFaGtruAd:
	jmp	.L_small_initial_blocks_encrypted_apEeDkAppehCuuj
.L_small_initial_num_blocks_is_7_apEeDkAppehCuuj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AxdpvvmfFkbdemp





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AxdpvvmfFkbdemp
.L_small_initial_partial_block_AxdpvvmfFkbdemp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AxdpvvmfFkbdemp:

	orq	%r8,%r8
	je	.L_after_reduction_AxdpvvmfFkbdemp
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_AxdpvvmfFkbdemp:
	jmp	.L_small_initial_blocks_encrypted_apEeDkAppehCuuj
.L_small_initial_num_blocks_is_8_apEeDkAppehCuuj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CxuqECDeqEGpknz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CxuqECDeqEGpknz
.L_small_initial_partial_block_CxuqECDeqEGpknz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CxuqECDeqEGpknz:

	orq	%r8,%r8
	je	.L_after_reduction_CxuqECDeqEGpknz
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_CxuqECDeqEGpknz:
	jmp	.L_small_initial_blocks_encrypted_apEeDkAppehCuuj
.L_small_initial_num_blocks_is_9_apEeDkAppehCuuj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%xmm29,%xmm4,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EnkmqfGkCvgjaCc





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EnkmqfGkCvgjaCc
.L_small_initial_partial_block_EnkmqfGkCvgjaCc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EnkmqfGkCvgjaCc:

	orq	%r8,%r8
	je	.L_after_reduction_EnkmqfGkCvgjaCc
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_EnkmqfGkCvgjaCc:
	jmp	.L_small_initial_blocks_encrypted_apEeDkAppehCuuj
.L_small_initial_num_blocks_is_10_apEeDkAppehCuuj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%ymm29,%ymm4,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gacmGblxAGBemGD





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gacmGblxAGBemGD
.L_small_initial_partial_block_gacmGblxAGBemGD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gacmGblxAGBemGD:

	orq	%r8,%r8
	je	.L_after_reduction_gacmGblxAGBemGD
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_gacmGblxAGBemGD:
	jmp	.L_small_initial_blocks_encrypted_apEeDkAppehCuuj
.L_small_initial_num_blocks_is_11_apEeDkAppehCuuj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iGxstjpzhtCktFs





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iGxstjpzhtCktFs
.L_small_initial_partial_block_iGxstjpzhtCktFs:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iGxstjpzhtCktFs:

	orq	%r8,%r8
	je	.L_after_reduction_iGxstjpzhtCktFs
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_iGxstjpzhtCktFs:
	jmp	.L_small_initial_blocks_encrypted_apEeDkAppehCuuj
.L_small_initial_num_blocks_is_12_apEeDkAppehCuuj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fFoqCfwFAFpqcab





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fFoqCfwFAFpqcab
.L_small_initial_partial_block_fFoqCfwFAFpqcab:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_fFoqCfwFAFpqcab:

	orq	%r8,%r8
	je	.L_after_reduction_fFoqCfwFAFpqcab
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_fFoqCfwFAFpqcab:
	jmp	.L_small_initial_blocks_encrypted_apEeDkAppehCuuj
.L_small_initial_num_blocks_is_13_apEeDkAppehCuuj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%xmm29,%xmm5,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pvrbpzbdreiBBkk





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pvrbpzbdreiBBkk
.L_small_initial_partial_block_pvrbpzbdreiBBkk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pvrbpzbdreiBBkk:

	orq	%r8,%r8
	je	.L_after_reduction_pvrbpzbdreiBBkk
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_pvrbpzbdreiBBkk:
	jmp	.L_small_initial_blocks_encrypted_apEeDkAppehCuuj
.L_small_initial_num_blocks_is_14_apEeDkAppehCuuj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%ymm29,%ymm5,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DcbzGEAigblrvyG





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DcbzGEAigblrvyG
.L_small_initial_partial_block_DcbzGEAigblrvyG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DcbzGEAigblrvyG:

	orq	%r8,%r8
	je	.L_after_reduction_DcbzGEAigblrvyG
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_DcbzGEAigblrvyG:
	jmp	.L_small_initial_blocks_encrypted_apEeDkAppehCuuj
.L_small_initial_num_blocks_is_15_apEeDkAppehCuuj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qwFpzqsmluklekr





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qwFpzqsmluklekr
.L_small_initial_partial_block_qwFpzqsmluklekr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qwFpzqsmluklekr:

	orq	%r8,%r8
	je	.L_after_reduction_qwFpzqsmluklekr
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_qwFpzqsmluklekr:
	jmp	.L_small_initial_blocks_encrypted_apEeDkAppehCuuj
.L_small_initial_num_blocks_is_16_apEeDkAppehCuuj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_AagApBBfaBtijsr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AagApBBfaBtijsr:
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_AagApBBfaBtijsr:
.L_small_initial_blocks_encrypted_apEeDkAppehCuuj:
.L_ghash_done_AgmocwsDmrmqxia:
	vmovdqu64	%xmm2,0(%rsi)
	vmovdqu64	%xmm14,64(%rsi)
.L_enc_dec_done_AgmocwsDmrmqxia:
	jmp	.Lexit_gcm_encrypt
.align	32
.Laes_gcm_encrypt_192_avx512:
	orq	%r8,%r8
	je	.L_enc_dec_done_qjDDlvkftsqvFxs
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14

	movq	(%rdx),%r11
	orq	%r11,%r11
	je	.L_partial_block_done_bidBBgzpCkcuxxq
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3
	vmovdqu64	336(%rsi),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%r11,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%r11,1),%r13
	subq	$16,%r13
	jge	.L_no_extra_mask_bidBBgzpCkcuxxq
	subq	%r13,%r12
.L_no_extra_mask_bidBBgzpCkcuxxq:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	.L_partial_incomplete_bidBBgzpCkcuxxq

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movq	$0,(%rdx)

	movq	%r11,%r12
	movq	$16,%r11
	subq	%r12,%r11
	jmp	.L_enc_dec_done_bidBBgzpCkcuxxq

.L_partial_incomplete_bidBBgzpCkcuxxq:
	addq	%r8,(%rdx)
	movq	%r8,%r11

.L_enc_dec_done_bidBBgzpCkcuxxq:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%r11,2),%k1
	vmovdqu64	%xmm14,64(%rsi)

	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
.L_partial_block_done_bidBBgzpCkcuxxq:
	vmovdqu64	0(%rsi),%xmm2
	subq	%r11,%r8
	je	.L_enc_dec_done_qjDDlvkftsqvFxs
	cmpq	$256,%r8
	jbe	.L_message_below_equal_16_blocks_qjDDlvkftsqvFxs

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_GFFtEvihrebstEo
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_GFFtEvihrebstEo
.L_next_16_overflow_GFFtEvihrebstEo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_GFFtEvihrebstEo:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%r11,1),%zmm0
	vmovdqu8	64(%rcx,%r11,1),%zmm3
	vmovdqu8	128(%rcx,%r11,1),%zmm4
	vmovdqu8	192(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%r11,1)
	vmovdqu8	%zmm10,64(%r10,%r11,1)
	vmovdqu8	%zmm11,128(%r10,%r11,1)
	vmovdqu8	%zmm12,192(%r10,%r11,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_rdsjlkqDibFGFtt

	vmovdqu64	288(%rsi),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	224(%rsi),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	160(%rsi),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	96(%rsi),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
.L_skip_hkeys_precomputation_rdsjlkqDibFGFtt:
	cmpq	$512,%r8
	jb	.L_message_below_32_blocks_qjDDlvkftsqvFxs



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_vbFjphBewvnfusd
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_vbFjphBewvnfusd
.L_next_16_overflow_vbFjphBewvnfusd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_vbFjphBewvnfusd:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%r11,1),%zmm0
	vmovdqu8	320(%rcx,%r11,1),%zmm3
	vmovdqu8	384(%rcx,%r11,1),%zmm4
	vmovdqu8	448(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%r11,1)
	vmovdqu8	%zmm10,320(%r10,%r11,1)
	vmovdqu8	%zmm11,384(%r10,%r11,1)
	vmovdqu8	%zmm12,448(%r10,%r11,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_bEowDDBhimcaGzy
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
.L_skip_hkeys_precomputation_bEowDDBhimcaGzy:
	movq	$1,%r14
	addq	$512,%r11
	subq	$512,%r8

	cmpq	$768,%r8
	jb	.L_no_more_big_nblocks_qjDDlvkftsqvFxs
.L_encrypt_big_nblocks_qjDDlvkftsqvFxs:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_tjcFgiqGxynhrqj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_tjcFgiqGxynhrqj
.L_16_blocks_overflow_tjcFgiqGxynhrqj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_tjcFgiqGxynhrqj:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_hohEicoleFslkjk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_hohEicoleFslkjk
.L_16_blocks_overflow_hohEicoleFslkjk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_hohEicoleFslkjk:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_jdgwhzDloEwttDu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_jdgwhzDloEwttDu
.L_16_blocks_overflow_jdgwhzDloEwttDu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_jdgwhzDloEwttDu:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%r11,1),%zmm17
	vmovdqu8	576(%rcx,%r11,1),%zmm19
	vmovdqu8	640(%rcx,%r11,1),%zmm20
	vmovdqu8	704(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%r11,1)
	vmovdqu8	%zmm3,576(%r10,%r11,1)
	vmovdqu8	%zmm4,640(%r10,%r11,1)
	vmovdqu8	%zmm5,704(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%r11
	subq	$768,%r8
	cmpq	$768,%r8
	jae	.L_encrypt_big_nblocks_qjDDlvkftsqvFxs

.L_no_more_big_nblocks_qjDDlvkftsqvFxs:

	cmpq	$512,%r8
	jae	.L_encrypt_32_blocks_qjDDlvkftsqvFxs

	cmpq	$256,%r8
	jae	.L_encrypt_16_blocks_qjDDlvkftsqvFxs
.L_encrypt_0_blocks_ghash_32_qjDDlvkftsqvFxs:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_agovhaymEzxkEvr

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_agovhaymEzxkEvr
	jb	.L_last_num_blocks_is_7_1_agovhaymEzxkEvr


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_agovhaymEzxkEvr
	jb	.L_last_num_blocks_is_11_9_agovhaymEzxkEvr


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_agovhaymEzxkEvr
	ja	.L_last_num_blocks_is_16_agovhaymEzxkEvr
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_agovhaymEzxkEvr
	jmp	.L_last_num_blocks_is_13_agovhaymEzxkEvr

.L_last_num_blocks_is_11_9_agovhaymEzxkEvr:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_agovhaymEzxkEvr
	ja	.L_last_num_blocks_is_11_agovhaymEzxkEvr
	jmp	.L_last_num_blocks_is_9_agovhaymEzxkEvr

.L_last_num_blocks_is_7_1_agovhaymEzxkEvr:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_agovhaymEzxkEvr
	jb	.L_last_num_blocks_is_3_1_agovhaymEzxkEvr

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_agovhaymEzxkEvr
	je	.L_last_num_blocks_is_6_agovhaymEzxkEvr
	jmp	.L_last_num_blocks_is_5_agovhaymEzxkEvr

.L_last_num_blocks_is_3_1_agovhaymEzxkEvr:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_agovhaymEzxkEvr
	je	.L_last_num_blocks_is_2_agovhaymEzxkEvr
.L_last_num_blocks_is_1_agovhaymEzxkEvr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_wctudvrezeyGvpd
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_wctudvrezeyGvpd

.L_16_blocks_overflow_wctudvrezeyGvpd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_wctudvrezeyGvpd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_skGaaheBvrmywqm





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_skGaaheBvrmywqm
.L_small_initial_partial_block_skGaaheBvrmywqm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_skGaaheBvrmywqm
.L_small_initial_compute_done_skGaaheBvrmywqm:
.L_after_reduction_skGaaheBvrmywqm:
	jmp	.L_last_blocks_done_agovhaymEzxkEvr
.L_last_num_blocks_is_2_agovhaymEzxkEvr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_BewGcGFlFBshGws
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_BewGcGFlFBshGws

.L_16_blocks_overflow_BewGcGFlFBshGws:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_BewGcGFlFBshGws:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CtDwBkzrbAvbgwu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CtDwBkzrbAvbgwu
.L_small_initial_partial_block_CtDwBkzrbAvbgwu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CtDwBkzrbAvbgwu:

	orq	%r8,%r8
	je	.L_after_reduction_CtDwBkzrbAvbgwu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CtDwBkzrbAvbgwu:
	jmp	.L_last_blocks_done_agovhaymEzxkEvr
.L_last_num_blocks_is_3_agovhaymEzxkEvr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_xgosabFBABmnqnc
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_xgosabFBABmnqnc

.L_16_blocks_overflow_xgosabFBABmnqnc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_xgosabFBABmnqnc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_omqBxuGliGGlAsg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_omqBxuGliGGlAsg
.L_small_initial_partial_block_omqBxuGliGGlAsg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_omqBxuGliGGlAsg:

	orq	%r8,%r8
	je	.L_after_reduction_omqBxuGliGGlAsg
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_omqBxuGliGGlAsg:
	jmp	.L_last_blocks_done_agovhaymEzxkEvr
.L_last_num_blocks_is_4_agovhaymEzxkEvr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_juCtAfarEwDDjEv
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_juCtAfarEwDDjEv

.L_16_blocks_overflow_juCtAfarEwDDjEv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_juCtAfarEwDDjEv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EqoAjuykhxBboqt





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EqoAjuykhxBboqt
.L_small_initial_partial_block_EqoAjuykhxBboqt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EqoAjuykhxBboqt:

	orq	%r8,%r8
	je	.L_after_reduction_EqoAjuykhxBboqt
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_EqoAjuykhxBboqt:
	jmp	.L_last_blocks_done_agovhaymEzxkEvr
.L_last_num_blocks_is_5_agovhaymEzxkEvr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_vuoxyvyzpdvlgaF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_vuoxyvyzpdvlgaF

.L_16_blocks_overflow_vuoxyvyzpdvlgaF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_vuoxyvyzpdvlgaF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iDgwwcovbxrxyxf





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iDgwwcovbxrxyxf
.L_small_initial_partial_block_iDgwwcovbxrxyxf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iDgwwcovbxrxyxf:

	orq	%r8,%r8
	je	.L_after_reduction_iDgwwcovbxrxyxf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_iDgwwcovbxrxyxf:
	jmp	.L_last_blocks_done_agovhaymEzxkEvr
.L_last_num_blocks_is_6_agovhaymEzxkEvr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_iArnpewfskhdksF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_iArnpewfskhdksF

.L_16_blocks_overflow_iArnpewfskhdksF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_iArnpewfskhdksF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_svqBAowhcozrdkh





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_svqBAowhcozrdkh
.L_small_initial_partial_block_svqBAowhcozrdkh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_svqBAowhcozrdkh:

	orq	%r8,%r8
	je	.L_after_reduction_svqBAowhcozrdkh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_svqBAowhcozrdkh:
	jmp	.L_last_blocks_done_agovhaymEzxkEvr
.L_last_num_blocks_is_7_agovhaymEzxkEvr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_cDcxEhvhjAFysGE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_cDcxEhvhjAFysGE

.L_16_blocks_overflow_cDcxEhvhjAFysGE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_cDcxEhvhjAFysGE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EipdjlxpsdgFuvd





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EipdjlxpsdgFuvd
.L_small_initial_partial_block_EipdjlxpsdgFuvd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EipdjlxpsdgFuvd:

	orq	%r8,%r8
	je	.L_after_reduction_EipdjlxpsdgFuvd
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_EipdjlxpsdgFuvd:
	jmp	.L_last_blocks_done_agovhaymEzxkEvr
.L_last_num_blocks_is_8_agovhaymEzxkEvr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_GwnzogDBabFfqbc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_GwnzogDBabFfqbc

.L_16_blocks_overflow_GwnzogDBabFfqbc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_GwnzogDBabFfqbc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DFiwtrBiBaotBhq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DFiwtrBiBaotBhq
.L_small_initial_partial_block_DFiwtrBiBaotBhq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DFiwtrBiBaotBhq:

	orq	%r8,%r8
	je	.L_after_reduction_DFiwtrBiBaotBhq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DFiwtrBiBaotBhq:
	jmp	.L_last_blocks_done_agovhaymEzxkEvr
.L_last_num_blocks_is_9_agovhaymEzxkEvr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_crcGFfliCdvtfcy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_crcGFfliCdvtfcy

.L_16_blocks_overflow_crcGFfliCdvtfcy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_crcGFfliCdvtfcy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ElnqfBFrEkznqgw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ElnqfBFrEkznqgw
.L_small_initial_partial_block_ElnqfBFrEkznqgw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ElnqfBFrEkznqgw:

	orq	%r8,%r8
	je	.L_after_reduction_ElnqfBFrEkznqgw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ElnqfBFrEkznqgw:
	jmp	.L_last_blocks_done_agovhaymEzxkEvr
.L_last_num_blocks_is_10_agovhaymEzxkEvr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_BeevFzvgCklqgED
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_BeevFzvgCklqgED

.L_16_blocks_overflow_BeevFzvgCklqgED:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_BeevFzvgCklqgED:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DAghtxEAstBrFcm





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DAghtxEAstBrFcm
.L_small_initial_partial_block_DAghtxEAstBrFcm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DAghtxEAstBrFcm:

	orq	%r8,%r8
	je	.L_after_reduction_DAghtxEAstBrFcm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DAghtxEAstBrFcm:
	jmp	.L_last_blocks_done_agovhaymEzxkEvr
.L_last_num_blocks_is_11_agovhaymEzxkEvr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_hdlxjEGCohvejgx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_hdlxjEGCohvejgx

.L_16_blocks_overflow_hdlxjEGCohvejgx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_hdlxjEGCohvejgx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wuacFCDaBzdwbCc





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wuacFCDaBzdwbCc
.L_small_initial_partial_block_wuacFCDaBzdwbCc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wuacFCDaBzdwbCc:

	orq	%r8,%r8
	je	.L_after_reduction_wuacFCDaBzdwbCc
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_wuacFCDaBzdwbCc:
	jmp	.L_last_blocks_done_agovhaymEzxkEvr
.L_last_num_blocks_is_12_agovhaymEzxkEvr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_bFznskfGExkptjD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_bFznskfGExkptjD

.L_16_blocks_overflow_bFznskfGExkptjD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_bFznskfGExkptjD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_uBuikFpkqoutedD





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_uBuikFpkqoutedD
.L_small_initial_partial_block_uBuikFpkqoutedD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_uBuikFpkqoutedD:

	orq	%r8,%r8
	je	.L_after_reduction_uBuikFpkqoutedD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_uBuikFpkqoutedD:
	jmp	.L_last_blocks_done_agovhaymEzxkEvr
.L_last_num_blocks_is_13_agovhaymEzxkEvr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_DxiscotCfqmDipn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_DxiscotCfqmDipn

.L_16_blocks_overflow_DxiscotCfqmDipn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_DxiscotCfqmDipn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_eFupqcCzGGwurcj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_eFupqcCzGGwurcj
.L_small_initial_partial_block_eFupqcCzGGwurcj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_eFupqcCzGGwurcj:

	orq	%r8,%r8
	je	.L_after_reduction_eFupqcCzGGwurcj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_eFupqcCzGGwurcj:
	jmp	.L_last_blocks_done_agovhaymEzxkEvr
.L_last_num_blocks_is_14_agovhaymEzxkEvr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_BGmhhDtuelrfvyu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_BGmhhDtuelrfvyu

.L_16_blocks_overflow_BGmhhDtuelrfvyu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_BGmhhDtuelrfvyu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qiDzbmFnsFGrpig





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qiDzbmFnsFGrpig
.L_small_initial_partial_block_qiDzbmFnsFGrpig:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qiDzbmFnsFGrpig:

	orq	%r8,%r8
	je	.L_after_reduction_qiDzbmFnsFGrpig
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_qiDzbmFnsFGrpig:
	jmp	.L_last_blocks_done_agovhaymEzxkEvr
.L_last_num_blocks_is_15_agovhaymEzxkEvr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_zDwCogmGAljAubr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_zDwCogmGAljAubr

.L_16_blocks_overflow_zDwCogmGAljAubr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_zDwCogmGAljAubr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cuekyudwBgedqve





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cuekyudwBgedqve
.L_small_initial_partial_block_cuekyudwBgedqve:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cuekyudwBgedqve:

	orq	%r8,%r8
	je	.L_after_reduction_cuekyudwBgedqve
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cuekyudwBgedqve:
	jmp	.L_last_blocks_done_agovhaymEzxkEvr
.L_last_num_blocks_is_16_agovhaymEzxkEvr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_AphrmCuvcDnuqwB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_AphrmCuvcDnuqwB

.L_16_blocks_overflow_AphrmCuvcDnuqwB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_AphrmCuvcDnuqwB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_styitsbEBerlCtc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_styitsbEBerlCtc:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_styitsbEBerlCtc:
	jmp	.L_last_blocks_done_agovhaymEzxkEvr
.L_last_num_blocks_is_0_agovhaymEzxkEvr:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_agovhaymEzxkEvr:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_qjDDlvkftsqvFxs
.L_encrypt_32_blocks_qjDDlvkftsqvFxs:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_lkabsmguCgawmbf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_lkabsmguCgawmbf
.L_16_blocks_overflow_lkabsmguCgawmbf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_lkabsmguCgawmbf:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_uExmtecnCsxeqfz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_uExmtecnCsxeqfz
.L_16_blocks_overflow_uExmtecnCsxeqfz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_uExmtecnCsxeqfz:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%r11
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_yjdGqwFEoCGCncj

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_yjdGqwFEoCGCncj
	jb	.L_last_num_blocks_is_7_1_yjdGqwFEoCGCncj


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_yjdGqwFEoCGCncj
	jb	.L_last_num_blocks_is_11_9_yjdGqwFEoCGCncj


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_yjdGqwFEoCGCncj
	ja	.L_last_num_blocks_is_16_yjdGqwFEoCGCncj
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_yjdGqwFEoCGCncj
	jmp	.L_last_num_blocks_is_13_yjdGqwFEoCGCncj

.L_last_num_blocks_is_11_9_yjdGqwFEoCGCncj:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_yjdGqwFEoCGCncj
	ja	.L_last_num_blocks_is_11_yjdGqwFEoCGCncj
	jmp	.L_last_num_blocks_is_9_yjdGqwFEoCGCncj

.L_last_num_blocks_is_7_1_yjdGqwFEoCGCncj:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_yjdGqwFEoCGCncj
	jb	.L_last_num_blocks_is_3_1_yjdGqwFEoCGCncj

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_yjdGqwFEoCGCncj
	je	.L_last_num_blocks_is_6_yjdGqwFEoCGCncj
	jmp	.L_last_num_blocks_is_5_yjdGqwFEoCGCncj

.L_last_num_blocks_is_3_1_yjdGqwFEoCGCncj:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_yjdGqwFEoCGCncj
	je	.L_last_num_blocks_is_2_yjdGqwFEoCGCncj
.L_last_num_blocks_is_1_yjdGqwFEoCGCncj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_irasxggGjkkjcBd
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_irasxggGjkkjcBd

.L_16_blocks_overflow_irasxggGjkkjcBd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_irasxggGjkkjcBd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kvBnpsdtFzCrkuq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kvBnpsdtFzCrkuq
.L_small_initial_partial_block_kvBnpsdtFzCrkuq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_kvBnpsdtFzCrkuq
.L_small_initial_compute_done_kvBnpsdtFzCrkuq:
.L_after_reduction_kvBnpsdtFzCrkuq:
	jmp	.L_last_blocks_done_yjdGqwFEoCGCncj
.L_last_num_blocks_is_2_yjdGqwFEoCGCncj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_bwyFyuiuECvwmfA
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_bwyFyuiuECvwmfA

.L_16_blocks_overflow_bwyFyuiuECvwmfA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_bwyFyuiuECvwmfA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AscjyqrmpjdFrjC





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AscjyqrmpjdFrjC
.L_small_initial_partial_block_AscjyqrmpjdFrjC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AscjyqrmpjdFrjC:

	orq	%r8,%r8
	je	.L_after_reduction_AscjyqrmpjdFrjC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_AscjyqrmpjdFrjC:
	jmp	.L_last_blocks_done_yjdGqwFEoCGCncj
.L_last_num_blocks_is_3_yjdGqwFEoCGCncj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_actxAtwszwyitmu
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_actxAtwszwyitmu

.L_16_blocks_overflow_actxAtwszwyitmu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_actxAtwszwyitmu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bFvFaCBnFeltifA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bFvFaCBnFeltifA
.L_small_initial_partial_block_bFvFaCBnFeltifA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bFvFaCBnFeltifA:

	orq	%r8,%r8
	je	.L_after_reduction_bFvFaCBnFeltifA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bFvFaCBnFeltifA:
	jmp	.L_last_blocks_done_yjdGqwFEoCGCncj
.L_last_num_blocks_is_4_yjdGqwFEoCGCncj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_oqeFgsFtpstEpxE
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_oqeFgsFtpstEpxE

.L_16_blocks_overflow_oqeFgsFtpstEpxE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_oqeFgsFtpstEpxE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zdbjnCDiflBuEcE





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zdbjnCDiflBuEcE
.L_small_initial_partial_block_zdbjnCDiflBuEcE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zdbjnCDiflBuEcE:

	orq	%r8,%r8
	je	.L_after_reduction_zdbjnCDiflBuEcE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zdbjnCDiflBuEcE:
	jmp	.L_last_blocks_done_yjdGqwFEoCGCncj
.L_last_num_blocks_is_5_yjdGqwFEoCGCncj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_oAjuyAklCkcxGCz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_oAjuyAklCkcxGCz

.L_16_blocks_overflow_oAjuyAklCkcxGCz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_oAjuyAklCkcxGCz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lcgnffGsevxuDth





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lcgnffGsevxuDth
.L_small_initial_partial_block_lcgnffGsevxuDth:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lcgnffGsevxuDth:

	orq	%r8,%r8
	je	.L_after_reduction_lcgnffGsevxuDth
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lcgnffGsevxuDth:
	jmp	.L_last_blocks_done_yjdGqwFEoCGCncj
.L_last_num_blocks_is_6_yjdGqwFEoCGCncj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_BDzywdGyCCksqci
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_BDzywdGyCCksqci

.L_16_blocks_overflow_BDzywdGyCCksqci:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_BDzywdGyCCksqci:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_snchaEacaamcusC





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_snchaEacaamcusC
.L_small_initial_partial_block_snchaEacaamcusC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_snchaEacaamcusC:

	orq	%r8,%r8
	je	.L_after_reduction_snchaEacaamcusC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_snchaEacaamcusC:
	jmp	.L_last_blocks_done_yjdGqwFEoCGCncj
.L_last_num_blocks_is_7_yjdGqwFEoCGCncj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_vkploggpchsxBpn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_vkploggpchsxBpn

.L_16_blocks_overflow_vkploggpchsxBpn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_vkploggpchsxBpn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ysomruiEjqaszfG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ysomruiEjqaszfG
.L_small_initial_partial_block_ysomruiEjqaszfG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ysomruiEjqaszfG:

	orq	%r8,%r8
	je	.L_after_reduction_ysomruiEjqaszfG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ysomruiEjqaszfG:
	jmp	.L_last_blocks_done_yjdGqwFEoCGCncj
.L_last_num_blocks_is_8_yjdGqwFEoCGCncj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_zGrplqDBqomakfj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_zGrplqDBqomakfj

.L_16_blocks_overflow_zGrplqDBqomakfj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_zGrplqDBqomakfj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qCuthrwxuixwajj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qCuthrwxuixwajj
.L_small_initial_partial_block_qCuthrwxuixwajj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qCuthrwxuixwajj:

	orq	%r8,%r8
	je	.L_after_reduction_qCuthrwxuixwajj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_qCuthrwxuixwajj:
	jmp	.L_last_blocks_done_yjdGqwFEoCGCncj
.L_last_num_blocks_is_9_yjdGqwFEoCGCncj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_EgswlppysddyBcm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_EgswlppysddyBcm

.L_16_blocks_overflow_EgswlppysddyBcm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_EgswlppysddyBcm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_plhCpujrpgfxwth





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_plhCpujrpgfxwth
.L_small_initial_partial_block_plhCpujrpgfxwth:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_plhCpujrpgfxwth:

	orq	%r8,%r8
	je	.L_after_reduction_plhCpujrpgfxwth
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_plhCpujrpgfxwth:
	jmp	.L_last_blocks_done_yjdGqwFEoCGCncj
.L_last_num_blocks_is_10_yjdGqwFEoCGCncj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_GtwbysdukfjCvhv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_GtwbysdukfjCvhv

.L_16_blocks_overflow_GtwbysdukfjCvhv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_GtwbysdukfjCvhv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yvfxGerzEywmiAh





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yvfxGerzEywmiAh
.L_small_initial_partial_block_yvfxGerzEywmiAh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yvfxGerzEywmiAh:

	orq	%r8,%r8
	je	.L_after_reduction_yvfxGerzEywmiAh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_yvfxGerzEywmiAh:
	jmp	.L_last_blocks_done_yjdGqwFEoCGCncj
.L_last_num_blocks_is_11_yjdGqwFEoCGCncj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_qnacartglGgApgo
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_qnacartglGgApgo

.L_16_blocks_overflow_qnacartglGgApgo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_qnacartglGgApgo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_leAArfswiyfoeFr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_leAArfswiyfoeFr
.L_small_initial_partial_block_leAArfswiyfoeFr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_leAArfswiyfoeFr:

	orq	%r8,%r8
	je	.L_after_reduction_leAArfswiyfoeFr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_leAArfswiyfoeFr:
	jmp	.L_last_blocks_done_yjdGqwFEoCGCncj
.L_last_num_blocks_is_12_yjdGqwFEoCGCncj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_qwffvwFaGxahAbG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_qwffvwFaGxahAbG

.L_16_blocks_overflow_qwffvwFaGxahAbG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_qwffvwFaGxahAbG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zkAczkEvuwGhosb





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zkAczkEvuwGhosb
.L_small_initial_partial_block_zkAczkEvuwGhosb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zkAczkEvuwGhosb:

	orq	%r8,%r8
	je	.L_after_reduction_zkAczkEvuwGhosb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zkAczkEvuwGhosb:
	jmp	.L_last_blocks_done_yjdGqwFEoCGCncj
.L_last_num_blocks_is_13_yjdGqwFEoCGCncj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_kxkGubgpdmdEfBk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_kxkGubgpdmdEfBk

.L_16_blocks_overflow_kxkGubgpdmdEfBk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_kxkGubgpdmdEfBk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BxkdvcgsybdDqiy





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BxkdvcgsybdDqiy
.L_small_initial_partial_block_BxkdvcgsybdDqiy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BxkdvcgsybdDqiy:

	orq	%r8,%r8
	je	.L_after_reduction_BxkdvcgsybdDqiy
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BxkdvcgsybdDqiy:
	jmp	.L_last_blocks_done_yjdGqwFEoCGCncj
.L_last_num_blocks_is_14_yjdGqwFEoCGCncj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_jvprghirDfCtFGr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_jvprghirDfCtFGr

.L_16_blocks_overflow_jvprghirDfCtFGr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_jvprghirDfCtFGr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DneajfrvfBDyoro





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DneajfrvfBDyoro
.L_small_initial_partial_block_DneajfrvfBDyoro:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DneajfrvfBDyoro:

	orq	%r8,%r8
	je	.L_after_reduction_DneajfrvfBDyoro
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DneajfrvfBDyoro:
	jmp	.L_last_blocks_done_yjdGqwFEoCGCncj
.L_last_num_blocks_is_15_yjdGqwFEoCGCncj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_pzAddCEbvuiuddt
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_pzAddCEbvuiuddt

.L_16_blocks_overflow_pzAddCEbvuiuddt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_pzAddCEbvuiuddt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gjeabtGvmFsbBiE





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gjeabtGvmFsbBiE
.L_small_initial_partial_block_gjeabtGvmFsbBiE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gjeabtGvmFsbBiE:

	orq	%r8,%r8
	je	.L_after_reduction_gjeabtGvmFsbBiE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gjeabtGvmFsbBiE:
	jmp	.L_last_blocks_done_yjdGqwFEoCGCncj
.L_last_num_blocks_is_16_yjdGqwFEoCGCncj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_DyjsbtppirCtpBi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_DyjsbtppirCtpBi

.L_16_blocks_overflow_DyjsbtppirCtpBi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_DyjsbtppirCtpBi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_seDFzdjsAzkoEBo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_seDFzdjsAzkoEBo:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_seDFzdjsAzkoEBo:
	jmp	.L_last_blocks_done_yjdGqwFEoCGCncj
.L_last_num_blocks_is_0_yjdGqwFEoCGCncj:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_yjdGqwFEoCGCncj:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_qjDDlvkftsqvFxs
.L_encrypt_16_blocks_qjDDlvkftsqvFxs:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_qqcaEFvuqesqhle
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_qqcaEFvuqesqhle
.L_16_blocks_overflow_qqcaEFvuqesqhle:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_qqcaEFvuqesqhle:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_kjvrpwlajuCzDlB

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_kjvrpwlajuCzDlB
	jb	.L_last_num_blocks_is_7_1_kjvrpwlajuCzDlB


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_kjvrpwlajuCzDlB
	jb	.L_last_num_blocks_is_11_9_kjvrpwlajuCzDlB


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_kjvrpwlajuCzDlB
	ja	.L_last_num_blocks_is_16_kjvrpwlajuCzDlB
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_kjvrpwlajuCzDlB
	jmp	.L_last_num_blocks_is_13_kjvrpwlajuCzDlB

.L_last_num_blocks_is_11_9_kjvrpwlajuCzDlB:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_kjvrpwlajuCzDlB
	ja	.L_last_num_blocks_is_11_kjvrpwlajuCzDlB
	jmp	.L_last_num_blocks_is_9_kjvrpwlajuCzDlB

.L_last_num_blocks_is_7_1_kjvrpwlajuCzDlB:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_kjvrpwlajuCzDlB
	jb	.L_last_num_blocks_is_3_1_kjvrpwlajuCzDlB

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_kjvrpwlajuCzDlB
	je	.L_last_num_blocks_is_6_kjvrpwlajuCzDlB
	jmp	.L_last_num_blocks_is_5_kjvrpwlajuCzDlB

.L_last_num_blocks_is_3_1_kjvrpwlajuCzDlB:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_kjvrpwlajuCzDlB
	je	.L_last_num_blocks_is_2_kjvrpwlajuCzDlB
.L_last_num_blocks_is_1_kjvrpwlajuCzDlB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_gjGEuyuvzstugoz
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_gjGEuyuvzstugoz

.L_16_blocks_overflow_gjGEuyuvzstugoz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_gjGEuyuvzstugoz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wCDbeaEgoytqxdu





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wCDbeaEgoytqxdu
.L_small_initial_partial_block_wCDbeaEgoytqxdu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_wCDbeaEgoytqxdu
.L_small_initial_compute_done_wCDbeaEgoytqxdu:
.L_after_reduction_wCDbeaEgoytqxdu:
	jmp	.L_last_blocks_done_kjvrpwlajuCzDlB
.L_last_num_blocks_is_2_kjvrpwlajuCzDlB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_zClsFsknfedtqxp
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_zClsFsknfedtqxp

.L_16_blocks_overflow_zClsFsknfedtqxp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_zClsFsknfedtqxp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ddktskApFvtaGBt





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ddktskApFvtaGBt
.L_small_initial_partial_block_ddktskApFvtaGBt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ddktskApFvtaGBt:

	orq	%r8,%r8
	je	.L_after_reduction_ddktskApFvtaGBt
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ddktskApFvtaGBt:
	jmp	.L_last_blocks_done_kjvrpwlajuCzDlB
.L_last_num_blocks_is_3_kjvrpwlajuCzDlB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_cyzfgzefzAfeEwl
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_cyzfgzefzAfeEwl

.L_16_blocks_overflow_cyzfgzefzAfeEwl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_cyzfgzefzAfeEwl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_klpxFshqCoDxyiB





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_klpxFshqCoDxyiB
.L_small_initial_partial_block_klpxFshqCoDxyiB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_klpxFshqCoDxyiB:

	orq	%r8,%r8
	je	.L_after_reduction_klpxFshqCoDxyiB
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_klpxFshqCoDxyiB:
	jmp	.L_last_blocks_done_kjvrpwlajuCzDlB
.L_last_num_blocks_is_4_kjvrpwlajuCzDlB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_BvvoavgAxpiAkCj
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_BvvoavgAxpiAkCj

.L_16_blocks_overflow_BvvoavgAxpiAkCj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_BvvoavgAxpiAkCj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CmqokgDCiBkxkzi





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CmqokgDCiBkxkzi
.L_small_initial_partial_block_CmqokgDCiBkxkzi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CmqokgDCiBkxkzi:

	orq	%r8,%r8
	je	.L_after_reduction_CmqokgDCiBkxkzi
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CmqokgDCiBkxkzi:
	jmp	.L_last_blocks_done_kjvrpwlajuCzDlB
.L_last_num_blocks_is_5_kjvrpwlajuCzDlB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_hhxzajoxAzrxjto
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_hhxzajoxAzrxjto

.L_16_blocks_overflow_hhxzajoxAzrxjto:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_hhxzajoxAzrxjto:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_abBxqbqswDAcrfi





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_abBxqbqswDAcrfi
.L_small_initial_partial_block_abBxqbqswDAcrfi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_abBxqbqswDAcrfi:

	orq	%r8,%r8
	je	.L_after_reduction_abBxqbqswDAcrfi
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_abBxqbqswDAcrfi:
	jmp	.L_last_blocks_done_kjvrpwlajuCzDlB
.L_last_num_blocks_is_6_kjvrpwlajuCzDlB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_qdslzkhCqnEpafq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_qdslzkhCqnEpafq

.L_16_blocks_overflow_qdslzkhCqnEpafq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_qdslzkhCqnEpafq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jyqmwiGuolplifB





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jyqmwiGuolplifB
.L_small_initial_partial_block_jyqmwiGuolplifB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jyqmwiGuolplifB:

	orq	%r8,%r8
	je	.L_after_reduction_jyqmwiGuolplifB
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jyqmwiGuolplifB:
	jmp	.L_last_blocks_done_kjvrpwlajuCzDlB
.L_last_num_blocks_is_7_kjvrpwlajuCzDlB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_zbfCgnfnjDjsplG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_zbfCgnfnjDjsplG

.L_16_blocks_overflow_zbfCgnfnjDjsplG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_zbfCgnfnjDjsplG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ogFpuyBndBudlEh





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ogFpuyBndBudlEh
.L_small_initial_partial_block_ogFpuyBndBudlEh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ogFpuyBndBudlEh:

	orq	%r8,%r8
	je	.L_after_reduction_ogFpuyBndBudlEh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ogFpuyBndBudlEh:
	jmp	.L_last_blocks_done_kjvrpwlajuCzDlB
.L_last_num_blocks_is_8_kjvrpwlajuCzDlB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_bsBFtEpdwmussbv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_bsBFtEpdwmussbv

.L_16_blocks_overflow_bsBFtEpdwmussbv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_bsBFtEpdwmussbv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ggthibjupAujcwj





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ggthibjupAujcwj
.L_small_initial_partial_block_ggthibjupAujcwj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ggthibjupAujcwj:

	orq	%r8,%r8
	je	.L_after_reduction_ggthibjupAujcwj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ggthibjupAujcwj:
	jmp	.L_last_blocks_done_kjvrpwlajuCzDlB
.L_last_num_blocks_is_9_kjvrpwlajuCzDlB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_zxlxtqEgcuzncci
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_zxlxtqEgcuzncci

.L_16_blocks_overflow_zxlxtqEgcuzncci:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_zxlxtqEgcuzncci:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cljantdwxxAGdad





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cljantdwxxAGdad
.L_small_initial_partial_block_cljantdwxxAGdad:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cljantdwxxAGdad:

	orq	%r8,%r8
	je	.L_after_reduction_cljantdwxxAGdad
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cljantdwxxAGdad:
	jmp	.L_last_blocks_done_kjvrpwlajuCzDlB
.L_last_num_blocks_is_10_kjvrpwlajuCzDlB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_yjuywpDjwnEExtG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_yjuywpDjwnEExtG

.L_16_blocks_overflow_yjuywpDjwnEExtG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_yjuywpDjwnEExtG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xcpnonijkgcvbff





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xcpnonijkgcvbff
.L_small_initial_partial_block_xcpnonijkgcvbff:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xcpnonijkgcvbff:

	orq	%r8,%r8
	je	.L_after_reduction_xcpnonijkgcvbff
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xcpnonijkgcvbff:
	jmp	.L_last_blocks_done_kjvrpwlajuCzDlB
.L_last_num_blocks_is_11_kjvrpwlajuCzDlB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_jAEBrojdpyGtyBj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_jAEBrojdpyGtyBj

.L_16_blocks_overflow_jAEBrojdpyGtyBj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_jAEBrojdpyGtyBj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jaFAlyjsgFannlc





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jaFAlyjsgFannlc
.L_small_initial_partial_block_jaFAlyjsgFannlc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jaFAlyjsgFannlc:

	orq	%r8,%r8
	je	.L_after_reduction_jaFAlyjsgFannlc
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jaFAlyjsgFannlc:
	jmp	.L_last_blocks_done_kjvrpwlajuCzDlB
.L_last_num_blocks_is_12_kjvrpwlajuCzDlB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_zsqBwwmkvwaiFop
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_zsqBwwmkvwaiFop

.L_16_blocks_overflow_zsqBwwmkvwaiFop:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_zsqBwwmkvwaiFop:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ehnDedquuorvhFC





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ehnDedquuorvhFC
.L_small_initial_partial_block_ehnDedquuorvhFC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ehnDedquuorvhFC:

	orq	%r8,%r8
	je	.L_after_reduction_ehnDedquuorvhFC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ehnDedquuorvhFC:
	jmp	.L_last_blocks_done_kjvrpwlajuCzDlB
.L_last_num_blocks_is_13_kjvrpwlajuCzDlB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_EEfseemdeEjoCjc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_EEfseemdeEjoCjc

.L_16_blocks_overflow_EEfseemdeEjoCjc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_EEfseemdeEjoCjc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_usCnodewbvAcyte





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_usCnodewbvAcyte
.L_small_initial_partial_block_usCnodewbvAcyte:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_usCnodewbvAcyte:

	orq	%r8,%r8
	je	.L_after_reduction_usCnodewbvAcyte
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_usCnodewbvAcyte:
	jmp	.L_last_blocks_done_kjvrpwlajuCzDlB
.L_last_num_blocks_is_14_kjvrpwlajuCzDlB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_kFgbfaDyhkvatsx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_kFgbfaDyhkvatsx

.L_16_blocks_overflow_kFgbfaDyhkvatsx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_kFgbfaDyhkvatsx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DfGGjznkwvkEkED





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DfGGjznkwvkEkED
.L_small_initial_partial_block_DfGGjznkwvkEkED:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DfGGjznkwvkEkED:

	orq	%r8,%r8
	je	.L_after_reduction_DfGGjznkwvkEkED
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DfGGjznkwvkEkED:
	jmp	.L_last_blocks_done_kjvrpwlajuCzDlB
.L_last_num_blocks_is_15_kjvrpwlajuCzDlB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_EkuhvayBbwqnhaB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_EkuhvayBbwqnhaB

.L_16_blocks_overflow_EkuhvayBbwqnhaB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_EkuhvayBbwqnhaB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_azgDDfaBjEvclAu





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_azgDDfaBjEvclAu
.L_small_initial_partial_block_azgDDfaBjEvclAu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_azgDDfaBjEvclAu:

	orq	%r8,%r8
	je	.L_after_reduction_azgDDfaBjEvclAu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_azgDDfaBjEvclAu:
	jmp	.L_last_blocks_done_kjvrpwlajuCzDlB
.L_last_num_blocks_is_16_kjvrpwlajuCzDlB:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_zhpflFcqlcxzniD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_zhpflFcqlcxzniD

.L_16_blocks_overflow_zhpflFcqlcxzniD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_zhpflFcqlcxzniD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_yctbedlwghpdCbd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yctbedlwghpdCbd:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_yctbedlwghpdCbd:
	jmp	.L_last_blocks_done_kjvrpwlajuCzDlB
.L_last_num_blocks_is_0_kjvrpwlajuCzDlB:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_kjvrpwlajuCzDlB:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_qjDDlvkftsqvFxs

.L_message_below_32_blocks_qjDDlvkftsqvFxs:


	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_cuBppAFdxlwoqws
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
.L_skip_hkeys_precomputation_cuBppAFdxlwoqws:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_imrqFEoyfvrrBfe

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_imrqFEoyfvrrBfe
	jb	.L_last_num_blocks_is_7_1_imrqFEoyfvrrBfe


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_imrqFEoyfvrrBfe
	jb	.L_last_num_blocks_is_11_9_imrqFEoyfvrrBfe


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_imrqFEoyfvrrBfe
	ja	.L_last_num_blocks_is_16_imrqFEoyfvrrBfe
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_imrqFEoyfvrrBfe
	jmp	.L_last_num_blocks_is_13_imrqFEoyfvrrBfe

.L_last_num_blocks_is_11_9_imrqFEoyfvrrBfe:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_imrqFEoyfvrrBfe
	ja	.L_last_num_blocks_is_11_imrqFEoyfvrrBfe
	jmp	.L_last_num_blocks_is_9_imrqFEoyfvrrBfe

.L_last_num_blocks_is_7_1_imrqFEoyfvrrBfe:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_imrqFEoyfvrrBfe
	jb	.L_last_num_blocks_is_3_1_imrqFEoyfvrrBfe

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_imrqFEoyfvrrBfe
	je	.L_last_num_blocks_is_6_imrqFEoyfvrrBfe
	jmp	.L_last_num_blocks_is_5_imrqFEoyfvrrBfe

.L_last_num_blocks_is_3_1_imrqFEoyfvrrBfe:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_imrqFEoyfvrrBfe
	je	.L_last_num_blocks_is_2_imrqFEoyfvrrBfe
.L_last_num_blocks_is_1_imrqFEoyfvrrBfe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_xfuajikmuxbbFlG
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_xfuajikmuxbbFlG

.L_16_blocks_overflow_xfuajikmuxbbFlG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_xfuajikmuxbbFlG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AAczGqbEvicjliz





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AAczGqbEvicjliz
.L_small_initial_partial_block_AAczGqbEvicjliz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_AAczGqbEvicjliz
.L_small_initial_compute_done_AAczGqbEvicjliz:
.L_after_reduction_AAczGqbEvicjliz:
	jmp	.L_last_blocks_done_imrqFEoyfvrrBfe
.L_last_num_blocks_is_2_imrqFEoyfvrrBfe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_pvdFjEdcnBmFakt
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_pvdFjEdcnBmFakt

.L_16_blocks_overflow_pvdFjEdcnBmFakt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_pvdFjEdcnBmFakt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_uzsywysfmCpezBw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_uzsywysfmCpezBw
.L_small_initial_partial_block_uzsywysfmCpezBw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_uzsywysfmCpezBw:

	orq	%r8,%r8
	je	.L_after_reduction_uzsywysfmCpezBw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_uzsywysfmCpezBw:
	jmp	.L_last_blocks_done_imrqFEoyfvrrBfe
.L_last_num_blocks_is_3_imrqFEoyfvrrBfe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_gulsszwhkpyGCcG
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_gulsszwhkpyGCcG

.L_16_blocks_overflow_gulsszwhkpyGCcG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_gulsszwhkpyGCcG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ElrdduGxeuxggrC





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ElrdduGxeuxggrC
.L_small_initial_partial_block_ElrdduGxeuxggrC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ElrdduGxeuxggrC:

	orq	%r8,%r8
	je	.L_after_reduction_ElrdduGxeuxggrC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ElrdduGxeuxggrC:
	jmp	.L_last_blocks_done_imrqFEoyfvrrBfe
.L_last_num_blocks_is_4_imrqFEoyfvrrBfe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_ulBlGAcbyApaFkj
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_ulBlGAcbyApaFkj

.L_16_blocks_overflow_ulBlGAcbyApaFkj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_ulBlGAcbyApaFkj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DbFbmcFerlkvxqg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DbFbmcFerlkvxqg
.L_small_initial_partial_block_DbFbmcFerlkvxqg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DbFbmcFerlkvxqg:

	orq	%r8,%r8
	je	.L_after_reduction_DbFbmcFerlkvxqg
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DbFbmcFerlkvxqg:
	jmp	.L_last_blocks_done_imrqFEoyfvrrBfe
.L_last_num_blocks_is_5_imrqFEoyfvrrBfe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_GcADctCwpsmBydA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_GcADctCwpsmBydA

.L_16_blocks_overflow_GcADctCwpsmBydA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_GcADctCwpsmBydA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yFefFxzyAdxlyfD





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yFefFxzyAdxlyfD
.L_small_initial_partial_block_yFefFxzyAdxlyfD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yFefFxzyAdxlyfD:

	orq	%r8,%r8
	je	.L_after_reduction_yFefFxzyAdxlyfD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_yFefFxzyAdxlyfD:
	jmp	.L_last_blocks_done_imrqFEoyfvrrBfe
.L_last_num_blocks_is_6_imrqFEoyfvrrBfe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_nFqBeyfyuGhDhkB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_nFqBeyfyuGhDhkB

.L_16_blocks_overflow_nFqBeyfyuGhDhkB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_nFqBeyfyuGhDhkB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xikGktvdgqFpice





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xikGktvdgqFpice
.L_small_initial_partial_block_xikGktvdgqFpice:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xikGktvdgqFpice:

	orq	%r8,%r8
	je	.L_after_reduction_xikGktvdgqFpice
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xikGktvdgqFpice:
	jmp	.L_last_blocks_done_imrqFEoyfvrrBfe
.L_last_num_blocks_is_7_imrqFEoyfvrrBfe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_ipyCtqGeDtfjjDB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_ipyCtqGeDtfjjDB

.L_16_blocks_overflow_ipyCtqGeDtfjjDB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_ipyCtqGeDtfjjDB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xGbECxrgxCjlwbE





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xGbECxrgxCjlwbE
.L_small_initial_partial_block_xGbECxrgxCjlwbE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xGbECxrgxCjlwbE:

	orq	%r8,%r8
	je	.L_after_reduction_xGbECxrgxCjlwbE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xGbECxrgxCjlwbE:
	jmp	.L_last_blocks_done_imrqFEoyfvrrBfe
.L_last_num_blocks_is_8_imrqFEoyfvrrBfe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_yhuwvoxFcfDEguj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_yhuwvoxFcfDEguj

.L_16_blocks_overflow_yhuwvoxFcfDEguj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_yhuwvoxFcfDEguj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dlqrcfrlyDDGpvo





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dlqrcfrlyDDGpvo
.L_small_initial_partial_block_dlqrcfrlyDDGpvo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dlqrcfrlyDDGpvo:

	orq	%r8,%r8
	je	.L_after_reduction_dlqrcfrlyDDGpvo
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dlqrcfrlyDDGpvo:
	jmp	.L_last_blocks_done_imrqFEoyfvrrBfe
.L_last_num_blocks_is_9_imrqFEoyfvrrBfe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_tCuxDGquprbxiyi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_tCuxDGquprbxiyi

.L_16_blocks_overflow_tCuxDGquprbxiyi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_tCuxDGquprbxiyi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lerCdpGvjGCffDj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lerCdpGvjGCffDj
.L_small_initial_partial_block_lerCdpGvjGCffDj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lerCdpGvjGCffDj:

	orq	%r8,%r8
	je	.L_after_reduction_lerCdpGvjGCffDj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lerCdpGvjGCffDj:
	jmp	.L_last_blocks_done_imrqFEoyfvrrBfe
.L_last_num_blocks_is_10_imrqFEoyfvrrBfe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_smwznzlrpjozvpn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_smwznzlrpjozvpn

.L_16_blocks_overflow_smwznzlrpjozvpn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_smwznzlrpjozvpn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pDuwivBFpmggxzp





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pDuwivBFpmggxzp
.L_small_initial_partial_block_pDuwivBFpmggxzp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pDuwivBFpmggxzp:

	orq	%r8,%r8
	je	.L_after_reduction_pDuwivBFpmggxzp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_pDuwivBFpmggxzp:
	jmp	.L_last_blocks_done_imrqFEoyfvrrBfe
.L_last_num_blocks_is_11_imrqFEoyfvrrBfe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_FGfzvbFyiyDEzej
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_FGfzvbFyiyDEzej

.L_16_blocks_overflow_FGfzvbFyiyDEzej:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_FGfzvbFyiyDEzej:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GdpptEerEoFhFEk





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GdpptEerEoFhFEk
.L_small_initial_partial_block_GdpptEerEoFhFEk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GdpptEerEoFhFEk:

	orq	%r8,%r8
	je	.L_after_reduction_GdpptEerEoFhFEk
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GdpptEerEoFhFEk:
	jmp	.L_last_blocks_done_imrqFEoyfvrrBfe
.L_last_num_blocks_is_12_imrqFEoyfvrrBfe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_eimacszvCoGansl
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_eimacszvCoGansl

.L_16_blocks_overflow_eimacszvCoGansl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_eimacszvCoGansl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rqotafuhFcryzGE





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rqotafuhFcryzGE
.L_small_initial_partial_block_rqotafuhFcryzGE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rqotafuhFcryzGE:

	orq	%r8,%r8
	je	.L_after_reduction_rqotafuhFcryzGE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rqotafuhFcryzGE:
	jmp	.L_last_blocks_done_imrqFEoyfvrrBfe
.L_last_num_blocks_is_13_imrqFEoyfvrrBfe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_yikyvtzCezzslwz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_yikyvtzCezzslwz

.L_16_blocks_overflow_yikyvtzCezzslwz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_yikyvtzCezzslwz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_trhjzmkpdeFxjbb





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_trhjzmkpdeFxjbb
.L_small_initial_partial_block_trhjzmkpdeFxjbb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_trhjzmkpdeFxjbb:

	orq	%r8,%r8
	je	.L_after_reduction_trhjzmkpdeFxjbb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_trhjzmkpdeFxjbb:
	jmp	.L_last_blocks_done_imrqFEoyfvrrBfe
.L_last_num_blocks_is_14_imrqFEoyfvrrBfe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_DrutvGtgvuFngua
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_DrutvGtgvuFngua

.L_16_blocks_overflow_DrutvGtgvuFngua:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_DrutvGtgvuFngua:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DqhAglqtFEjdkEp





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DqhAglqtFEjdkEp
.L_small_initial_partial_block_DqhAglqtFEjdkEp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DqhAglqtFEjdkEp:

	orq	%r8,%r8
	je	.L_after_reduction_DqhAglqtFEjdkEp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DqhAglqtFEjdkEp:
	jmp	.L_last_blocks_done_imrqFEoyfvrrBfe
.L_last_num_blocks_is_15_imrqFEoyfvrrBfe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_vxBeelvoqinipAk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_vxBeelvoqinipAk

.L_16_blocks_overflow_vxBeelvoqinipAk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_vxBeelvoqinipAk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gdCiwAzAvhporCC





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gdCiwAzAvhporCC
.L_small_initial_partial_block_gdCiwAzAvhporCC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gdCiwAzAvhporCC:

	orq	%r8,%r8
	je	.L_after_reduction_gdCiwAzAvhporCC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gdCiwAzAvhporCC:
	jmp	.L_last_blocks_done_imrqFEoyfvrrBfe
.L_last_num_blocks_is_16_imrqFEoyfvrrBfe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_bssiylBbBvhuqlk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_bssiylBbBvhuqlk

.L_16_blocks_overflow_bssiylBbBvhuqlk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_bssiylBbBvhuqlk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_tfuwibDgdfzbkzF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tfuwibDgdfzbkzF:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tfuwibDgdfzbkzF:
	jmp	.L_last_blocks_done_imrqFEoyfvrrBfe
.L_last_num_blocks_is_0_imrqFEoyfvrrBfe:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_imrqFEoyfvrrBfe:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_qjDDlvkftsqvFxs

.L_message_below_equal_16_blocks_qjDDlvkftsqvFxs:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	.L_small_initial_num_blocks_is_8_CsuAenzupvBjlkv
	jl	.L_small_initial_num_blocks_is_7_1_CsuAenzupvBjlkv


	cmpq	$12,%r12
	je	.L_small_initial_num_blocks_is_12_CsuAenzupvBjlkv
	jl	.L_small_initial_num_blocks_is_11_9_CsuAenzupvBjlkv


	cmpq	$16,%r12
	je	.L_small_initial_num_blocks_is_16_CsuAenzupvBjlkv
	cmpq	$15,%r12
	je	.L_small_initial_num_blocks_is_15_CsuAenzupvBjlkv
	cmpq	$14,%r12
	je	.L_small_initial_num_blocks_is_14_CsuAenzupvBjlkv
	jmp	.L_small_initial_num_blocks_is_13_CsuAenzupvBjlkv

.L_small_initial_num_blocks_is_11_9_CsuAenzupvBjlkv:

	cmpq	$11,%r12
	je	.L_small_initial_num_blocks_is_11_CsuAenzupvBjlkv
	cmpq	$10,%r12
	je	.L_small_initial_num_blocks_is_10_CsuAenzupvBjlkv
	jmp	.L_small_initial_num_blocks_is_9_CsuAenzupvBjlkv

.L_small_initial_num_blocks_is_7_1_CsuAenzupvBjlkv:
	cmpq	$4,%r12
	je	.L_small_initial_num_blocks_is_4_CsuAenzupvBjlkv
	jl	.L_small_initial_num_blocks_is_3_1_CsuAenzupvBjlkv

	cmpq	$7,%r12
	je	.L_small_initial_num_blocks_is_7_CsuAenzupvBjlkv
	cmpq	$6,%r12
	je	.L_small_initial_num_blocks_is_6_CsuAenzupvBjlkv
	jmp	.L_small_initial_num_blocks_is_5_CsuAenzupvBjlkv

.L_small_initial_num_blocks_is_3_1_CsuAenzupvBjlkv:

	cmpq	$3,%r12
	je	.L_small_initial_num_blocks_is_3_CsuAenzupvBjlkv
	cmpq	$2,%r12
	je	.L_small_initial_num_blocks_is_2_CsuAenzupvBjlkv





.L_small_initial_num_blocks_is_1_CsuAenzupvBjlkv:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%r11,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rvBtysnEbgvxwfe





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rvBtysnEbgvxwfe
.L_small_initial_partial_block_rvBtysnEbgvxwfe:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	.L_after_reduction_rvBtysnEbgvxwfe
.L_small_initial_compute_done_rvBtysnEbgvxwfe:
.L_after_reduction_rvBtysnEbgvxwfe:
	jmp	.L_small_initial_blocks_encrypted_CsuAenzupvBjlkv
.L_small_initial_num_blocks_is_2_CsuAenzupvBjlkv:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%r11,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_thctFecBzqdimjd





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_thctFecBzqdimjd
.L_small_initial_partial_block_thctFecBzqdimjd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_thctFecBzqdimjd:

	orq	%r8,%r8
	je	.L_after_reduction_thctFecBzqdimjd
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_thctFecBzqdimjd:
	jmp	.L_small_initial_blocks_encrypted_CsuAenzupvBjlkv
.L_small_initial_num_blocks_is_3_CsuAenzupvBjlkv:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bxelGhsbgBxrsfp





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bxelGhsbgBxrsfp
.L_small_initial_partial_block_bxelGhsbgBxrsfp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bxelGhsbgBxrsfp:

	orq	%r8,%r8
	je	.L_after_reduction_bxelGhsbgBxrsfp
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_bxelGhsbgBxrsfp:
	jmp	.L_small_initial_blocks_encrypted_CsuAenzupvBjlkv
.L_small_initial_num_blocks_is_4_CsuAenzupvBjlkv:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_obyubDgvamqfcwn





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_obyubDgvamqfcwn
.L_small_initial_partial_block_obyubDgvamqfcwn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_obyubDgvamqfcwn:

	orq	%r8,%r8
	je	.L_after_reduction_obyubDgvamqfcwn
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_obyubDgvamqfcwn:
	jmp	.L_small_initial_blocks_encrypted_CsuAenzupvBjlkv
.L_small_initial_num_blocks_is_5_CsuAenzupvBjlkv:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%xmm29,%xmm3,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_krFngjqrwFtAzrd





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_krFngjqrwFtAzrd
.L_small_initial_partial_block_krFngjqrwFtAzrd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_krFngjqrwFtAzrd:

	orq	%r8,%r8
	je	.L_after_reduction_krFngjqrwFtAzrd
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_krFngjqrwFtAzrd:
	jmp	.L_small_initial_blocks_encrypted_CsuAenzupvBjlkv
.L_small_initial_num_blocks_is_6_CsuAenzupvBjlkv:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%ymm29,%ymm3,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_siqrawgmcGewdlb





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_siqrawgmcGewdlb
.L_small_initial_partial_block_siqrawgmcGewdlb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_siqrawgmcGewdlb:

	orq	%r8,%r8
	je	.L_after_reduction_siqrawgmcGewdlb
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_siqrawgmcGewdlb:
	jmp	.L_small_initial_blocks_encrypted_CsuAenzupvBjlkv
.L_small_initial_num_blocks_is_7_CsuAenzupvBjlkv:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qfojhihzqftmuly





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qfojhihzqftmuly
.L_small_initial_partial_block_qfojhihzqftmuly:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qfojhihzqftmuly:

	orq	%r8,%r8
	je	.L_after_reduction_qfojhihzqftmuly
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_qfojhihzqftmuly:
	jmp	.L_small_initial_blocks_encrypted_CsuAenzupvBjlkv
.L_small_initial_num_blocks_is_8_CsuAenzupvBjlkv:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kErixkeCuwvGllg





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kErixkeCuwvGllg
.L_small_initial_partial_block_kErixkeCuwvGllg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kErixkeCuwvGllg:

	orq	%r8,%r8
	je	.L_after_reduction_kErixkeCuwvGllg
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_kErixkeCuwvGllg:
	jmp	.L_small_initial_blocks_encrypted_CsuAenzupvBjlkv
.L_small_initial_num_blocks_is_9_CsuAenzupvBjlkv:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%xmm29,%xmm4,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zAavdEEAnyasCgk





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zAavdEEAnyasCgk
.L_small_initial_partial_block_zAavdEEAnyasCgk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zAavdEEAnyasCgk:

	orq	%r8,%r8
	je	.L_after_reduction_zAavdEEAnyasCgk
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_zAavdEEAnyasCgk:
	jmp	.L_small_initial_blocks_encrypted_CsuAenzupvBjlkv
.L_small_initial_num_blocks_is_10_CsuAenzupvBjlkv:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%ymm29,%ymm4,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cecoGmlzprAtFgB





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cecoGmlzprAtFgB
.L_small_initial_partial_block_cecoGmlzprAtFgB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cecoGmlzprAtFgB:

	orq	%r8,%r8
	je	.L_after_reduction_cecoGmlzprAtFgB
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_cecoGmlzprAtFgB:
	jmp	.L_small_initial_blocks_encrypted_CsuAenzupvBjlkv
.L_small_initial_num_blocks_is_11_CsuAenzupvBjlkv:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_eraFyxgooAjhjug





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_eraFyxgooAjhjug
.L_small_initial_partial_block_eraFyxgooAjhjug:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_eraFyxgooAjhjug:

	orq	%r8,%r8
	je	.L_after_reduction_eraFyxgooAjhjug
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_eraFyxgooAjhjug:
	jmp	.L_small_initial_blocks_encrypted_CsuAenzupvBjlkv
.L_small_initial_num_blocks_is_12_CsuAenzupvBjlkv:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zbqhyCDjhjunbAx





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zbqhyCDjhjunbAx
.L_small_initial_partial_block_zbqhyCDjhjunbAx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zbqhyCDjhjunbAx:

	orq	%r8,%r8
	je	.L_after_reduction_zbqhyCDjhjunbAx
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_zbqhyCDjhjunbAx:
	jmp	.L_small_initial_blocks_encrypted_CsuAenzupvBjlkv
.L_small_initial_num_blocks_is_13_CsuAenzupvBjlkv:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%xmm29,%xmm5,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DwDGAyAtpstqhxx





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DwDGAyAtpstqhxx
.L_small_initial_partial_block_DwDGAyAtpstqhxx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DwDGAyAtpstqhxx:

	orq	%r8,%r8
	je	.L_after_reduction_DwDGAyAtpstqhxx
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_DwDGAyAtpstqhxx:
	jmp	.L_small_initial_blocks_encrypted_CsuAenzupvBjlkv
.L_small_initial_num_blocks_is_14_CsuAenzupvBjlkv:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%ymm29,%ymm5,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tlxvqurwwbmjBhz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tlxvqurwwbmjBhz
.L_small_initial_partial_block_tlxvqurwwbmjBhz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tlxvqurwwbmjBhz:

	orq	%r8,%r8
	je	.L_after_reduction_tlxvqurwwbmjBhz
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_tlxvqurwwbmjBhz:
	jmp	.L_small_initial_blocks_encrypted_CsuAenzupvBjlkv
.L_small_initial_num_blocks_is_15_CsuAenzupvBjlkv:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xAygAjrsBhimGgg





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xAygAjrsBhimGgg
.L_small_initial_partial_block_xAygAjrsBhimGgg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xAygAjrsBhimGgg:

	orq	%r8,%r8
	je	.L_after_reduction_xAygAjrsBhimGgg
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_xAygAjrsBhimGgg:
	jmp	.L_small_initial_blocks_encrypted_CsuAenzupvBjlkv
.L_small_initial_num_blocks_is_16_CsuAenzupvBjlkv:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_uEgDEdkpFvrcDdr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_uEgDEdkpFvrcDdr:
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_uEgDEdkpFvrcDdr:
.L_small_initial_blocks_encrypted_CsuAenzupvBjlkv:
.L_ghash_done_qjDDlvkftsqvFxs:
	vmovdqu64	%xmm2,0(%rsi)
	vmovdqu64	%xmm14,64(%rsi)
.L_enc_dec_done_qjDDlvkftsqvFxs:
	jmp	.Lexit_gcm_encrypt
.align	32
.Laes_gcm_encrypt_256_avx512:
	orq	%r8,%r8
	je	.L_enc_dec_done_vniEckCGskxEjje
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14

	movq	(%rdx),%r11
	orq	%r11,%r11
	je	.L_partial_block_done_EfAeEClEulvEpDj
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3
	vmovdqu64	336(%rsi),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%r11,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%r11,1),%r13
	subq	$16,%r13
	jge	.L_no_extra_mask_EfAeEClEulvEpDj
	subq	%r13,%r12
.L_no_extra_mask_EfAeEClEulvEpDj:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	.L_partial_incomplete_EfAeEClEulvEpDj

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movq	$0,(%rdx)

	movq	%r11,%r12
	movq	$16,%r11
	subq	%r12,%r11
	jmp	.L_enc_dec_done_EfAeEClEulvEpDj

.L_partial_incomplete_EfAeEClEulvEpDj:
	addq	%r8,(%rdx)
	movq	%r8,%r11

.L_enc_dec_done_EfAeEClEulvEpDj:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%r11,2),%k1
	vmovdqu64	%xmm14,64(%rsi)

	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
.L_partial_block_done_EfAeEClEulvEpDj:
	vmovdqu64	0(%rsi),%xmm2
	subq	%r11,%r8
	je	.L_enc_dec_done_vniEckCGskxEjje
	cmpq	$256,%r8
	jbe	.L_message_below_equal_16_blocks_vniEckCGskxEjje

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_nnsozBwtEdxdEzw
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_nnsozBwtEdxdEzw
.L_next_16_overflow_nnsozBwtEdxdEzw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_nnsozBwtEdxdEzw:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%r11,1),%zmm0
	vmovdqu8	64(%rcx,%r11,1),%zmm3
	vmovdqu8	128(%rcx,%r11,1),%zmm4
	vmovdqu8	192(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	208(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	224(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%r11,1)
	vmovdqu8	%zmm10,64(%r10,%r11,1)
	vmovdqu8	%zmm11,128(%r10,%r11,1)
	vmovdqu8	%zmm12,192(%r10,%r11,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_CbagcdpflbcAzen

	vmovdqu64	288(%rsi),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	224(%rsi),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	160(%rsi),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	96(%rsi),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
.L_skip_hkeys_precomputation_CbagcdpflbcAzen:
	cmpq	$512,%r8
	jb	.L_message_below_32_blocks_vniEckCGskxEjje



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_DsDwuyeEohflfhA
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_DsDwuyeEohflfhA
.L_next_16_overflow_DsDwuyeEohflfhA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_DsDwuyeEohflfhA:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%r11,1),%zmm0
	vmovdqu8	320(%rcx,%r11,1),%zmm3
	vmovdqu8	384(%rcx,%r11,1),%zmm4
	vmovdqu8	448(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	208(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	224(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%r11,1)
	vmovdqu8	%zmm10,320(%r10,%r11,1)
	vmovdqu8	%zmm11,384(%r10,%r11,1)
	vmovdqu8	%zmm12,448(%r10,%r11,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_msueezvdsduvCtc
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
.L_skip_hkeys_precomputation_msueezvdsduvCtc:
	movq	$1,%r14
	addq	$512,%r11
	subq	$512,%r8

	cmpq	$768,%r8
	jb	.L_no_more_big_nblocks_vniEckCGskxEjje
.L_encrypt_big_nblocks_vniEckCGskxEjje:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_owfebCchxnkxyqD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_owfebCchxnkxyqD
.L_16_blocks_overflow_owfebCchxnkxyqD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_owfebCchxnkxyqD:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_wernzFheEoupzly
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_wernzFheEoupzly
.L_16_blocks_overflow_wernzFheEoupzly:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_wernzFheEoupzly:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_wsrAiqnGazwAgxb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_wsrAiqnGazwAgxb
.L_16_blocks_overflow_wsrAiqnGazwAgxb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_wsrAiqnGazwAgxb:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%r11,1),%zmm17
	vmovdqu8	576(%rcx,%r11,1),%zmm19
	vmovdqu8	640(%rcx,%r11,1),%zmm20
	vmovdqu8	704(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%r11,1)
	vmovdqu8	%zmm3,576(%r10,%r11,1)
	vmovdqu8	%zmm4,640(%r10,%r11,1)
	vmovdqu8	%zmm5,704(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%r11
	subq	$768,%r8
	cmpq	$768,%r8
	jae	.L_encrypt_big_nblocks_vniEckCGskxEjje

.L_no_more_big_nblocks_vniEckCGskxEjje:

	cmpq	$512,%r8
	jae	.L_encrypt_32_blocks_vniEckCGskxEjje

	cmpq	$256,%r8
	jae	.L_encrypt_16_blocks_vniEckCGskxEjje
.L_encrypt_0_blocks_ghash_32_vniEckCGskxEjje:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_xpcEeFbgvtGiGxj

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_xpcEeFbgvtGiGxj
	jb	.L_last_num_blocks_is_7_1_xpcEeFbgvtGiGxj


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_xpcEeFbgvtGiGxj
	jb	.L_last_num_blocks_is_11_9_xpcEeFbgvtGiGxj


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_xpcEeFbgvtGiGxj
	ja	.L_last_num_blocks_is_16_xpcEeFbgvtGiGxj
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_xpcEeFbgvtGiGxj
	jmp	.L_last_num_blocks_is_13_xpcEeFbgvtGiGxj

.L_last_num_blocks_is_11_9_xpcEeFbgvtGiGxj:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_xpcEeFbgvtGiGxj
	ja	.L_last_num_blocks_is_11_xpcEeFbgvtGiGxj
	jmp	.L_last_num_blocks_is_9_xpcEeFbgvtGiGxj

.L_last_num_blocks_is_7_1_xpcEeFbgvtGiGxj:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_xpcEeFbgvtGiGxj
	jb	.L_last_num_blocks_is_3_1_xpcEeFbgvtGiGxj

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_xpcEeFbgvtGiGxj
	je	.L_last_num_blocks_is_6_xpcEeFbgvtGiGxj
	jmp	.L_last_num_blocks_is_5_xpcEeFbgvtGiGxj

.L_last_num_blocks_is_3_1_xpcEeFbgvtGiGxj:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_xpcEeFbgvtGiGxj
	je	.L_last_num_blocks_is_2_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_1_xpcEeFbgvtGiGxj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_fbxdqvDkDazmgBG
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_fbxdqvDkDazmgBG

.L_16_blocks_overflow_fbxdqvDkDazmgBG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_fbxdqvDkDazmgBG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_miejBoxklombjrb





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_miejBoxklombjrb
.L_small_initial_partial_block_miejBoxklombjrb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_miejBoxklombjrb
.L_small_initial_compute_done_miejBoxklombjrb:
.L_after_reduction_miejBoxklombjrb:
	jmp	.L_last_blocks_done_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_2_xpcEeFbgvtGiGxj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_zrGmkjzDAusAwyz
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_zrGmkjzDAusAwyz

.L_16_blocks_overflow_zrGmkjzDAusAwyz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_zrGmkjzDAusAwyz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GDAkDkGhzgqbtmj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GDAkDkGhzgqbtmj
.L_small_initial_partial_block_GDAkDkGhzgqbtmj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GDAkDkGhzgqbtmj:

	orq	%r8,%r8
	je	.L_after_reduction_GDAkDkGhzgqbtmj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GDAkDkGhzgqbtmj:
	jmp	.L_last_blocks_done_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_3_xpcEeFbgvtGiGxj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_wyeCbaehbykotjf
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_wyeCbaehbykotjf

.L_16_blocks_overflow_wyeCbaehbykotjf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_wyeCbaehbykotjf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GvgxcgBchAjrCgE





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GvgxcgBchAjrCgE
.L_small_initial_partial_block_GvgxcgBchAjrCgE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GvgxcgBchAjrCgE:

	orq	%r8,%r8
	je	.L_after_reduction_GvgxcgBchAjrCgE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GvgxcgBchAjrCgE:
	jmp	.L_last_blocks_done_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_4_xpcEeFbgvtGiGxj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_GlhggwqejEFFjko
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_GlhggwqejEFFjko

.L_16_blocks_overflow_GlhggwqejEFFjko:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_GlhggwqejEFFjko:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_eexnhdrCuiqFgdG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_eexnhdrCuiqFgdG
.L_small_initial_partial_block_eexnhdrCuiqFgdG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_eexnhdrCuiqFgdG:

	orq	%r8,%r8
	je	.L_after_reduction_eexnhdrCuiqFgdG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_eexnhdrCuiqFgdG:
	jmp	.L_last_blocks_done_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_5_xpcEeFbgvtGiGxj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_xjqkGtaBfwCnFkr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_xjqkGtaBfwCnFkr

.L_16_blocks_overflow_xjqkGtaBfwCnFkr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_xjqkGtaBfwCnFkr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rnFskpomlgosCzg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rnFskpomlgosCzg
.L_small_initial_partial_block_rnFskpomlgosCzg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rnFskpomlgosCzg:

	orq	%r8,%r8
	je	.L_after_reduction_rnFskpomlgosCzg
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rnFskpomlgosCzg:
	jmp	.L_last_blocks_done_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_6_xpcEeFbgvtGiGxj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_AjFueombxFuhufa
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_AjFueombxFuhufa

.L_16_blocks_overflow_AjFueombxFuhufa:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_AjFueombxFuhufa:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jCGonrzzCEuxFfC





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jCGonrzzCEuxFfC
.L_small_initial_partial_block_jCGonrzzCEuxFfC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jCGonrzzCEuxFfC:

	orq	%r8,%r8
	je	.L_after_reduction_jCGonrzzCEuxFfC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jCGonrzzCEuxFfC:
	jmp	.L_last_blocks_done_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_7_xpcEeFbgvtGiGxj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_qausayxglEvBDiD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_qausayxglEvBDiD

.L_16_blocks_overflow_qausayxglEvBDiD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_qausayxglEvBDiD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DjdozcgqksDdukf





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DjdozcgqksDdukf
.L_small_initial_partial_block_DjdozcgqksDdukf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DjdozcgqksDdukf:

	orq	%r8,%r8
	je	.L_after_reduction_DjdozcgqksDdukf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DjdozcgqksDdukf:
	jmp	.L_last_blocks_done_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_8_xpcEeFbgvtGiGxj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_pkEylsopbcnhtfb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_pkEylsopbcnhtfb

.L_16_blocks_overflow_pkEylsopbcnhtfb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_pkEylsopbcnhtfb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nBfzFxvzdADgkbr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nBfzFxvzdADgkbr
.L_small_initial_partial_block_nBfzFxvzdADgkbr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nBfzFxvzdADgkbr:

	orq	%r8,%r8
	je	.L_after_reduction_nBfzFxvzdADgkbr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_nBfzFxvzdADgkbr:
	jmp	.L_last_blocks_done_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_9_xpcEeFbgvtGiGxj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_CmlowGfsoxatcgz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_CmlowGfsoxatcgz

.L_16_blocks_overflow_CmlowGfsoxatcgz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_CmlowGfsoxatcgz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bjsjlFBDbuysndz





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bjsjlFBDbuysndz
.L_small_initial_partial_block_bjsjlFBDbuysndz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bjsjlFBDbuysndz:

	orq	%r8,%r8
	je	.L_after_reduction_bjsjlFBDbuysndz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bjsjlFBDbuysndz:
	jmp	.L_last_blocks_done_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_10_xpcEeFbgvtGiGxj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_xlwbylElyAzzfhu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_xlwbylElyAzzfhu

.L_16_blocks_overflow_xlwbylElyAzzfhu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_xlwbylElyAzzfhu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GtBrmuyglADbfDw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GtBrmuyglADbfDw
.L_small_initial_partial_block_GtBrmuyglADbfDw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GtBrmuyglADbfDw:

	orq	%r8,%r8
	je	.L_after_reduction_GtBrmuyglADbfDw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GtBrmuyglADbfDw:
	jmp	.L_last_blocks_done_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_11_xpcEeFbgvtGiGxj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_yzDoCwBjiCkcFBC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_yzDoCwBjiCkcFBC

.L_16_blocks_overflow_yzDoCwBjiCkcFBC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_yzDoCwBjiCkcFBC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iiAraFEaBqdhdao





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iiAraFEaBqdhdao
.L_small_initial_partial_block_iiAraFEaBqdhdao:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iiAraFEaBqdhdao:

	orq	%r8,%r8
	je	.L_after_reduction_iiAraFEaBqdhdao
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_iiAraFEaBqdhdao:
	jmp	.L_last_blocks_done_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_12_xpcEeFbgvtGiGxj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_kouctfsAmwwmiFf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_kouctfsAmwwmiFf

.L_16_blocks_overflow_kouctfsAmwwmiFf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_kouctfsAmwwmiFf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_mCGnFuaGzoxzjDD





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_mCGnFuaGzoxzjDD
.L_small_initial_partial_block_mCGnFuaGzoxzjDD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mCGnFuaGzoxzjDD:

	orq	%r8,%r8
	je	.L_after_reduction_mCGnFuaGzoxzjDD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_mCGnFuaGzoxzjDD:
	jmp	.L_last_blocks_done_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_13_xpcEeFbgvtGiGxj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_kesuumfCgzCiBmu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_kesuumfCgzCiBmu

.L_16_blocks_overflow_kesuumfCgzCiBmu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_kesuumfCgzCiBmu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qnawtxhCsCaptmq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qnawtxhCsCaptmq
.L_small_initial_partial_block_qnawtxhCsCaptmq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qnawtxhCsCaptmq:

	orq	%r8,%r8
	je	.L_after_reduction_qnawtxhCsCaptmq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_qnawtxhCsCaptmq:
	jmp	.L_last_blocks_done_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_14_xpcEeFbgvtGiGxj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_zxnrwcczDrpAtav
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_zxnrwcczDrpAtav

.L_16_blocks_overflow_zxnrwcczDrpAtav:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_zxnrwcczDrpAtav:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lhpvhinBnwpeest





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lhpvhinBnwpeest
.L_small_initial_partial_block_lhpvhinBnwpeest:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lhpvhinBnwpeest:

	orq	%r8,%r8
	je	.L_after_reduction_lhpvhinBnwpeest
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lhpvhinBnwpeest:
	jmp	.L_last_blocks_done_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_15_xpcEeFbgvtGiGxj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_mdFFfCipjkpgjBx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_mdFFfCipjkpgjBx

.L_16_blocks_overflow_mdFFfCipjkpgjBx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_mdFFfCipjkpgjBx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bAwyzEioAgfFfqf





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bAwyzEioAgfFfqf
.L_small_initial_partial_block_bAwyzEioAgfFfqf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bAwyzEioAgfFfqf:

	orq	%r8,%r8
	je	.L_after_reduction_bAwyzEioAgfFfqf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bAwyzEioAgfFfqf:
	jmp	.L_last_blocks_done_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_16_xpcEeFbgvtGiGxj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_cubAFjgnjlugypx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_cubAFjgnjlugypx

.L_16_blocks_overflow_cubAFjgnjlugypx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_cubAFjgnjlugypx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_mgoxkGvmiwtpybD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mgoxkGvmiwtpybD:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_mgoxkGvmiwtpybD:
	jmp	.L_last_blocks_done_xpcEeFbgvtGiGxj
.L_last_num_blocks_is_0_xpcEeFbgvtGiGxj:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_xpcEeFbgvtGiGxj:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_vniEckCGskxEjje
.L_encrypt_32_blocks_vniEckCGskxEjje:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_mlfncksGgnaCnml
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_mlfncksGgnaCnml
.L_16_blocks_overflow_mlfncksGgnaCnml:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_mlfncksGgnaCnml:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_vqvxAuxyiwnvFCk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_vqvxAuxyiwnvFCk
.L_16_blocks_overflow_vqvxAuxyiwnvFCk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_vqvxAuxyiwnvFCk:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%r11
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_EijxbxAzjtCAljd

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_EijxbxAzjtCAljd
	jb	.L_last_num_blocks_is_7_1_EijxbxAzjtCAljd


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_EijxbxAzjtCAljd
	jb	.L_last_num_blocks_is_11_9_EijxbxAzjtCAljd


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_EijxbxAzjtCAljd
	ja	.L_last_num_blocks_is_16_EijxbxAzjtCAljd
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_EijxbxAzjtCAljd
	jmp	.L_last_num_blocks_is_13_EijxbxAzjtCAljd

.L_last_num_blocks_is_11_9_EijxbxAzjtCAljd:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_EijxbxAzjtCAljd
	ja	.L_last_num_blocks_is_11_EijxbxAzjtCAljd
	jmp	.L_last_num_blocks_is_9_EijxbxAzjtCAljd

.L_last_num_blocks_is_7_1_EijxbxAzjtCAljd:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_EijxbxAzjtCAljd
	jb	.L_last_num_blocks_is_3_1_EijxbxAzjtCAljd

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_EijxbxAzjtCAljd
	je	.L_last_num_blocks_is_6_EijxbxAzjtCAljd
	jmp	.L_last_num_blocks_is_5_EijxbxAzjtCAljd

.L_last_num_blocks_is_3_1_EijxbxAzjtCAljd:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_EijxbxAzjtCAljd
	je	.L_last_num_blocks_is_2_EijxbxAzjtCAljd
.L_last_num_blocks_is_1_EijxbxAzjtCAljd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_tByajnGEssxDkfG
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_tByajnGEssxDkfG

.L_16_blocks_overflow_tByajnGEssxDkfG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_tByajnGEssxDkfG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_petwtfbzrdDtxdg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_petwtfbzrdDtxdg
.L_small_initial_partial_block_petwtfbzrdDtxdg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_petwtfbzrdDtxdg
.L_small_initial_compute_done_petwtfbzrdDtxdg:
.L_after_reduction_petwtfbzrdDtxdg:
	jmp	.L_last_blocks_done_EijxbxAzjtCAljd
.L_last_num_blocks_is_2_EijxbxAzjtCAljd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_ilAnppotsqaCqEo
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_ilAnppotsqaCqEo

.L_16_blocks_overflow_ilAnppotsqaCqEo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_ilAnppotsqaCqEo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vtGdaegzqysDaxp





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vtGdaegzqysDaxp
.L_small_initial_partial_block_vtGdaegzqysDaxp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vtGdaegzqysDaxp:

	orq	%r8,%r8
	je	.L_after_reduction_vtGdaegzqysDaxp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vtGdaegzqysDaxp:
	jmp	.L_last_blocks_done_EijxbxAzjtCAljd
.L_last_num_blocks_is_3_EijxbxAzjtCAljd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_zuckdkFttskwqhs
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_zuckdkFttskwqhs

.L_16_blocks_overflow_zuckdkFttskwqhs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_zuckdkFttskwqhs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wulcDpnysdbooqu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wulcDpnysdbooqu
.L_small_initial_partial_block_wulcDpnysdbooqu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wulcDpnysdbooqu:

	orq	%r8,%r8
	je	.L_after_reduction_wulcDpnysdbooqu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_wulcDpnysdbooqu:
	jmp	.L_last_blocks_done_EijxbxAzjtCAljd
.L_last_num_blocks_is_4_EijxbxAzjtCAljd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_GuEAgBeBkztEpGE
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_GuEAgBeBkztEpGE

.L_16_blocks_overflow_GuEAgBeBkztEpGE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_GuEAgBeBkztEpGE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gviDCjCyyzygCfE





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gviDCjCyyzygCfE
.L_small_initial_partial_block_gviDCjCyyzygCfE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gviDCjCyyzygCfE:

	orq	%r8,%r8
	je	.L_after_reduction_gviDCjCyyzygCfE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gviDCjCyyzygCfE:
	jmp	.L_last_blocks_done_EijxbxAzjtCAljd
.L_last_num_blocks_is_5_EijxbxAzjtCAljd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_ssoemvkdBBglrma
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_ssoemvkdBBglrma

.L_16_blocks_overflow_ssoemvkdBBglrma:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_ssoemvkdBBglrma:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kAAnifpeluBqroj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kAAnifpeluBqroj
.L_small_initial_partial_block_kAAnifpeluBqroj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kAAnifpeluBqroj:

	orq	%r8,%r8
	je	.L_after_reduction_kAAnifpeluBqroj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kAAnifpeluBqroj:
	jmp	.L_last_blocks_done_EijxbxAzjtCAljd
.L_last_num_blocks_is_6_EijxbxAzjtCAljd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_yesgtvdqqyucawB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_yesgtvdqqyucawB

.L_16_blocks_overflow_yesgtvdqqyucawB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_yesgtvdqqyucawB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FyguEuapvAGqqha





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FyguEuapvAGqqha
.L_small_initial_partial_block_FyguEuapvAGqqha:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FyguEuapvAGqqha:

	orq	%r8,%r8
	je	.L_after_reduction_FyguEuapvAGqqha
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FyguEuapvAGqqha:
	jmp	.L_last_blocks_done_EijxbxAzjtCAljd
.L_last_num_blocks_is_7_EijxbxAzjtCAljd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_BowsjvwAijsDgDF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_BowsjvwAijsDgDF

.L_16_blocks_overflow_BowsjvwAijsDgDF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_BowsjvwAijsDgDF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_hGotqFnfawvmwzp





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_hGotqFnfawvmwzp
.L_small_initial_partial_block_hGotqFnfawvmwzp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_hGotqFnfawvmwzp:

	orq	%r8,%r8
	je	.L_after_reduction_hGotqFnfawvmwzp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_hGotqFnfawvmwzp:
	jmp	.L_last_blocks_done_EijxbxAzjtCAljd
.L_last_num_blocks_is_8_EijxbxAzjtCAljd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_nDthsAAsszdzbjt
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_nDthsAAsszdzbjt

.L_16_blocks_overflow_nDthsAAsszdzbjt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_nDthsAAsszdzbjt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BkkxviaEwhdcdfu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BkkxviaEwhdcdfu
.L_small_initial_partial_block_BkkxviaEwhdcdfu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BkkxviaEwhdcdfu:

	orq	%r8,%r8
	je	.L_after_reduction_BkkxviaEwhdcdfu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BkkxviaEwhdcdfu:
	jmp	.L_last_blocks_done_EijxbxAzjtCAljd
.L_last_num_blocks_is_9_EijxbxAzjtCAljd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_pgGhCimAqlkmCoc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_pgGhCimAqlkmCoc

.L_16_blocks_overflow_pgGhCimAqlkmCoc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_pgGhCimAqlkmCoc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rynFcAwmowqnCqu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rynFcAwmowqnCqu
.L_small_initial_partial_block_rynFcAwmowqnCqu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rynFcAwmowqnCqu:

	orq	%r8,%r8
	je	.L_after_reduction_rynFcAwmowqnCqu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rynFcAwmowqnCqu:
	jmp	.L_last_blocks_done_EijxbxAzjtCAljd
.L_last_num_blocks_is_10_EijxbxAzjtCAljd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_jegtriyhnpDBxFs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_jegtriyhnpDBxFs

.L_16_blocks_overflow_jegtriyhnpDBxFs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_jegtriyhnpDBxFs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iynnfjGfFeuDcuG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iynnfjGfFeuDcuG
.L_small_initial_partial_block_iynnfjGfFeuDcuG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iynnfjGfFeuDcuG:

	orq	%r8,%r8
	je	.L_after_reduction_iynnfjGfFeuDcuG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_iynnfjGfFeuDcuG:
	jmp	.L_last_blocks_done_EijxbxAzjtCAljd
.L_last_num_blocks_is_11_EijxbxAzjtCAljd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_FoflepAhvgBtgse
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_FoflepAhvgBtgse

.L_16_blocks_overflow_FoflepAhvgBtgse:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_FoflepAhvgBtgse:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_aqdAizstrxndBef





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_aqdAizstrxndBef
.L_small_initial_partial_block_aqdAizstrxndBef:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_aqdAizstrxndBef:

	orq	%r8,%r8
	je	.L_after_reduction_aqdAizstrxndBef
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_aqdAizstrxndBef:
	jmp	.L_last_blocks_done_EijxbxAzjtCAljd
.L_last_num_blocks_is_12_EijxbxAzjtCAljd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_ivclgwhDhwvhCuz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_ivclgwhDhwvhCuz

.L_16_blocks_overflow_ivclgwhDhwvhCuz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_ivclgwhDhwvhCuz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_muDAkrogfswolym





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_muDAkrogfswolym
.L_small_initial_partial_block_muDAkrogfswolym:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_muDAkrogfswolym:

	orq	%r8,%r8
	je	.L_after_reduction_muDAkrogfswolym
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_muDAkrogfswolym:
	jmp	.L_last_blocks_done_EijxbxAzjtCAljd
.L_last_num_blocks_is_13_EijxbxAzjtCAljd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_DwaDbiffuxtpthy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_DwaDbiffuxtpthy

.L_16_blocks_overflow_DwaDbiffuxtpthy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_DwaDbiffuxtpthy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_phxevAsCEvDnikq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_phxevAsCEvDnikq
.L_small_initial_partial_block_phxevAsCEvDnikq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_phxevAsCEvDnikq:

	orq	%r8,%r8
	je	.L_after_reduction_phxevAsCEvDnikq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_phxevAsCEvDnikq:
	jmp	.L_last_blocks_done_EijxbxAzjtCAljd
.L_last_num_blocks_is_14_EijxbxAzjtCAljd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_bmCnadlhetljtkx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_bmCnadlhetljtkx

.L_16_blocks_overflow_bmCnadlhetljtkx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_bmCnadlhetljtkx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yeGxecolaEElmbc





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yeGxecolaEElmbc
.L_small_initial_partial_block_yeGxecolaEElmbc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yeGxecolaEElmbc:

	orq	%r8,%r8
	je	.L_after_reduction_yeGxecolaEElmbc
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_yeGxecolaEElmbc:
	jmp	.L_last_blocks_done_EijxbxAzjtCAljd
.L_last_num_blocks_is_15_EijxbxAzjtCAljd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_kwBzaaunBjEsGBz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_kwBzaaunBjEsGBz

.L_16_blocks_overflow_kwBzaaunBjEsGBz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_kwBzaaunBjEsGBz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gnftErubCwFAsfr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gnftErubCwFAsfr
.L_small_initial_partial_block_gnftErubCwFAsfr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gnftErubCwFAsfr:

	orq	%r8,%r8
	je	.L_after_reduction_gnftErubCwFAsfr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gnftErubCwFAsfr:
	jmp	.L_last_blocks_done_EijxbxAzjtCAljd
.L_last_num_blocks_is_16_EijxbxAzjtCAljd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_FllxkpjelbxmGhz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_FllxkpjelbxmGhz

.L_16_blocks_overflow_FllxkpjelbxmGhz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_FllxkpjelbxmGhz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_bockabkGdxDBjtA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bockabkGdxDBjtA:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bockabkGdxDBjtA:
	jmp	.L_last_blocks_done_EijxbxAzjtCAljd
.L_last_num_blocks_is_0_EijxbxAzjtCAljd:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_EijxbxAzjtCAljd:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_vniEckCGskxEjje
.L_encrypt_16_blocks_vniEckCGskxEjje:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_GaFqlbdzpaCbbmF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_GaFqlbdzpaCbbmF
.L_16_blocks_overflow_GaFqlbdzpaCbbmF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_GaFqlbdzpaCbbmF:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_axEuytnwfywcCeF

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_axEuytnwfywcCeF
	jb	.L_last_num_blocks_is_7_1_axEuytnwfywcCeF


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_axEuytnwfywcCeF
	jb	.L_last_num_blocks_is_11_9_axEuytnwfywcCeF


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_axEuytnwfywcCeF
	ja	.L_last_num_blocks_is_16_axEuytnwfywcCeF
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_axEuytnwfywcCeF
	jmp	.L_last_num_blocks_is_13_axEuytnwfywcCeF

.L_last_num_blocks_is_11_9_axEuytnwfywcCeF:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_axEuytnwfywcCeF
	ja	.L_last_num_blocks_is_11_axEuytnwfywcCeF
	jmp	.L_last_num_blocks_is_9_axEuytnwfywcCeF

.L_last_num_blocks_is_7_1_axEuytnwfywcCeF:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_axEuytnwfywcCeF
	jb	.L_last_num_blocks_is_3_1_axEuytnwfywcCeF

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_axEuytnwfywcCeF
	je	.L_last_num_blocks_is_6_axEuytnwfywcCeF
	jmp	.L_last_num_blocks_is_5_axEuytnwfywcCeF

.L_last_num_blocks_is_3_1_axEuytnwfywcCeF:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_axEuytnwfywcCeF
	je	.L_last_num_blocks_is_2_axEuytnwfywcCeF
.L_last_num_blocks_is_1_axEuytnwfywcCeF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_ufdaBggnrakGtsD
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_ufdaBggnrakGtsD

.L_16_blocks_overflow_ufdaBggnrakGtsD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_ufdaBggnrakGtsD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fweaigypwEClhAy





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fweaigypwEClhAy
.L_small_initial_partial_block_fweaigypwEClhAy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_fweaigypwEClhAy
.L_small_initial_compute_done_fweaigypwEClhAy:
.L_after_reduction_fweaigypwEClhAy:
	jmp	.L_last_blocks_done_axEuytnwfywcCeF
.L_last_num_blocks_is_2_axEuytnwfywcCeF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_hciBtGAixjwlpuB
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_hciBtGAixjwlpuB

.L_16_blocks_overflow_hciBtGAixjwlpuB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_hciBtGAixjwlpuB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pbwCyuhelbemzen





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pbwCyuhelbemzen
.L_small_initial_partial_block_pbwCyuhelbemzen:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pbwCyuhelbemzen:

	orq	%r8,%r8
	je	.L_after_reduction_pbwCyuhelbemzen
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_pbwCyuhelbemzen:
	jmp	.L_last_blocks_done_axEuytnwfywcCeF
.L_last_num_blocks_is_3_axEuytnwfywcCeF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_FCsCtthADjwpoyo
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_FCsCtthADjwpoyo

.L_16_blocks_overflow_FCsCtthADjwpoyo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_FCsCtthADjwpoyo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ujBpcalEoxcrgiu





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ujBpcalEoxcrgiu
.L_small_initial_partial_block_ujBpcalEoxcrgiu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ujBpcalEoxcrgiu:

	orq	%r8,%r8
	je	.L_after_reduction_ujBpcalEoxcrgiu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ujBpcalEoxcrgiu:
	jmp	.L_last_blocks_done_axEuytnwfywcCeF
.L_last_num_blocks_is_4_axEuytnwfywcCeF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_jmusGkDddqakDlh
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_jmusGkDddqakDlh

.L_16_blocks_overflow_jmusGkDddqakDlh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_jmusGkDddqakDlh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iesasgGhsCpFCgp





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iesasgGhsCpFCgp
.L_small_initial_partial_block_iesasgGhsCpFCgp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iesasgGhsCpFCgp:

	orq	%r8,%r8
	je	.L_after_reduction_iesasgGhsCpFCgp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_iesasgGhsCpFCgp:
	jmp	.L_last_blocks_done_axEuytnwfywcCeF
.L_last_num_blocks_is_5_axEuytnwfywcCeF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_vEsEgsfAAGkwFqc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_vEsEgsfAAGkwFqc

.L_16_blocks_overflow_vEsEgsfAAGkwFqc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_vEsEgsfAAGkwFqc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_mdpwgcChxicBpgv





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_mdpwgcChxicBpgv
.L_small_initial_partial_block_mdpwgcChxicBpgv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mdpwgcChxicBpgv:

	orq	%r8,%r8
	je	.L_after_reduction_mdpwgcChxicBpgv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_mdpwgcChxicBpgv:
	jmp	.L_last_blocks_done_axEuytnwfywcCeF
.L_last_num_blocks_is_6_axEuytnwfywcCeF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_negiGzrgCpgeivn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_negiGzrgCpgeivn

.L_16_blocks_overflow_negiGzrgCpgeivn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_negiGzrgCpgeivn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gGgddauhFqsmfkn





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gGgddauhFqsmfkn
.L_small_initial_partial_block_gGgddauhFqsmfkn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gGgddauhFqsmfkn:

	orq	%r8,%r8
	je	.L_after_reduction_gGgddauhFqsmfkn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gGgddauhFqsmfkn:
	jmp	.L_last_blocks_done_axEuytnwfywcCeF
.L_last_num_blocks_is_7_axEuytnwfywcCeF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_scblyxbasdEiler
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_scblyxbasdEiler

.L_16_blocks_overflow_scblyxbasdEiler:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_scblyxbasdEiler:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CotnAhEcexvcgnl





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CotnAhEcexvcgnl
.L_small_initial_partial_block_CotnAhEcexvcgnl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CotnAhEcexvcgnl:

	orq	%r8,%r8
	je	.L_after_reduction_CotnAhEcexvcgnl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CotnAhEcexvcgnl:
	jmp	.L_last_blocks_done_axEuytnwfywcCeF
.L_last_num_blocks_is_8_axEuytnwfywcCeF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_ctzkCekGblEhuEd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_ctzkCekGblEhuEd

.L_16_blocks_overflow_ctzkCekGblEhuEd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_ctzkCekGblEhuEd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xEmiAdxxwmCzEqa





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xEmiAdxxwmCzEqa
.L_small_initial_partial_block_xEmiAdxxwmCzEqa:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xEmiAdxxwmCzEqa:

	orq	%r8,%r8
	je	.L_after_reduction_xEmiAdxxwmCzEqa
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xEmiAdxxwmCzEqa:
	jmp	.L_last_blocks_done_axEuytnwfywcCeF
.L_last_num_blocks_is_9_axEuytnwfywcCeF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_zqdqDGwdgzckFGk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_zqdqDGwdgzckFGk

.L_16_blocks_overflow_zqdqDGwdgzckFGk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_zqdqDGwdgzckFGk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zqkguqDFGwrndot





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zqkguqDFGwrndot
.L_small_initial_partial_block_zqkguqDFGwrndot:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zqkguqDFGwrndot:

	orq	%r8,%r8
	je	.L_after_reduction_zqkguqDFGwrndot
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zqkguqDFGwrndot:
	jmp	.L_last_blocks_done_axEuytnwfywcCeF
.L_last_num_blocks_is_10_axEuytnwfywcCeF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_dwrgqbcnctaBGwC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_dwrgqbcnctaBGwC

.L_16_blocks_overflow_dwrgqbcnctaBGwC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_dwrgqbcnctaBGwC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wlhyuaCpaEejmhk





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wlhyuaCpaEejmhk
.L_small_initial_partial_block_wlhyuaCpaEejmhk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wlhyuaCpaEejmhk:

	orq	%r8,%r8
	je	.L_after_reduction_wlhyuaCpaEejmhk
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_wlhyuaCpaEejmhk:
	jmp	.L_last_blocks_done_axEuytnwfywcCeF
.L_last_num_blocks_is_11_axEuytnwfywcCeF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_GdurorqdCtbygfA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_GdurorqdCtbygfA

.L_16_blocks_overflow_GdurorqdCtbygfA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_GdurorqdCtbygfA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_uinwbllshjhbGjw





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_uinwbllshjhbGjw
.L_small_initial_partial_block_uinwbllshjhbGjw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_uinwbllshjhbGjw:

	orq	%r8,%r8
	je	.L_after_reduction_uinwbllshjhbGjw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_uinwbllshjhbGjw:
	jmp	.L_last_blocks_done_axEuytnwfywcCeF
.L_last_num_blocks_is_12_axEuytnwfywcCeF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_cftcBfgbxrFGoft
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_cftcBfgbxrFGoft

.L_16_blocks_overflow_cftcBfgbxrFGoft:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_cftcBfgbxrFGoft:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cobpCtwvGmwDhuq





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cobpCtwvGmwDhuq
.L_small_initial_partial_block_cobpCtwvGmwDhuq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cobpCtwvGmwDhuq:

	orq	%r8,%r8
	je	.L_after_reduction_cobpCtwvGmwDhuq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cobpCtwvGmwDhuq:
	jmp	.L_last_blocks_done_axEuytnwfywcCeF
.L_last_num_blocks_is_13_axEuytnwfywcCeF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_BeeBCcngtgApuwC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_BeeBCcngtgApuwC

.L_16_blocks_overflow_BeeBCcngtgApuwC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_BeeBCcngtgApuwC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_hFlBxyBAcoyEFtg





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_hFlBxyBAcoyEFtg
.L_small_initial_partial_block_hFlBxyBAcoyEFtg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_hFlBxyBAcoyEFtg:

	orq	%r8,%r8
	je	.L_after_reduction_hFlBxyBAcoyEFtg
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_hFlBxyBAcoyEFtg:
	jmp	.L_last_blocks_done_axEuytnwfywcCeF
.L_last_num_blocks_is_14_axEuytnwfywcCeF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_arEbgpkkAvtcFdy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_arEbgpkkAvtcFdy

.L_16_blocks_overflow_arEbgpkkAvtcFdy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_arEbgpkkAvtcFdy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lzGccgubueoelfz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lzGccgubueoelfz
.L_small_initial_partial_block_lzGccgubueoelfz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lzGccgubueoelfz:

	orq	%r8,%r8
	je	.L_after_reduction_lzGccgubueoelfz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lzGccgubueoelfz:
	jmp	.L_last_blocks_done_axEuytnwfywcCeF
.L_last_num_blocks_is_15_axEuytnwfywcCeF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_cdboglvfarlDxqD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_cdboglvfarlDxqD

.L_16_blocks_overflow_cdboglvfarlDxqD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_cdboglvfarlDxqD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tqFCbqyzesDpzaj





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tqFCbqyzesDpzaj
.L_small_initial_partial_block_tqFCbqyzesDpzaj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tqFCbqyzesDpzaj:

	orq	%r8,%r8
	je	.L_after_reduction_tqFCbqyzesDpzaj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tqFCbqyzesDpzaj:
	jmp	.L_last_blocks_done_axEuytnwfywcCeF
.L_last_num_blocks_is_16_axEuytnwfywcCeF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_dkqsuvDmfzArnjj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_dkqsuvDmfzArnjj

.L_16_blocks_overflow_dkqsuvDmfzArnjj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_dkqsuvDmfzArnjj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_kAqsBtwadzhFADl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kAqsBtwadzhFADl:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kAqsBtwadzhFADl:
	jmp	.L_last_blocks_done_axEuytnwfywcCeF
.L_last_num_blocks_is_0_axEuytnwfywcCeF:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_axEuytnwfywcCeF:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_vniEckCGskxEjje

.L_message_below_32_blocks_vniEckCGskxEjje:


	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_EyikgzehvjClGix
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
.L_skip_hkeys_precomputation_EyikgzehvjClGix:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_byewvAufdxezrnj

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_byewvAufdxezrnj
	jb	.L_last_num_blocks_is_7_1_byewvAufdxezrnj


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_byewvAufdxezrnj
	jb	.L_last_num_blocks_is_11_9_byewvAufdxezrnj


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_byewvAufdxezrnj
	ja	.L_last_num_blocks_is_16_byewvAufdxezrnj
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_byewvAufdxezrnj
	jmp	.L_last_num_blocks_is_13_byewvAufdxezrnj

.L_last_num_blocks_is_11_9_byewvAufdxezrnj:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_byewvAufdxezrnj
	ja	.L_last_num_blocks_is_11_byewvAufdxezrnj
	jmp	.L_last_num_blocks_is_9_byewvAufdxezrnj

.L_last_num_blocks_is_7_1_byewvAufdxezrnj:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_byewvAufdxezrnj
	jb	.L_last_num_blocks_is_3_1_byewvAufdxezrnj

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_byewvAufdxezrnj
	je	.L_last_num_blocks_is_6_byewvAufdxezrnj
	jmp	.L_last_num_blocks_is_5_byewvAufdxezrnj

.L_last_num_blocks_is_3_1_byewvAufdxezrnj:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_byewvAufdxezrnj
	je	.L_last_num_blocks_is_2_byewvAufdxezrnj
.L_last_num_blocks_is_1_byewvAufdxezrnj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_tvaBaCrdmhkkfsq
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_tvaBaCrdmhkkfsq

.L_16_blocks_overflow_tvaBaCrdmhkkfsq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_tvaBaCrdmhkkfsq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jCcEpxiEAuGwxfB





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jCcEpxiEAuGwxfB
.L_small_initial_partial_block_jCcEpxiEAuGwxfB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_jCcEpxiEAuGwxfB
.L_small_initial_compute_done_jCcEpxiEAuGwxfB:
.L_after_reduction_jCcEpxiEAuGwxfB:
	jmp	.L_last_blocks_done_byewvAufdxezrnj
.L_last_num_blocks_is_2_byewvAufdxezrnj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_lBBsdGbBfbscqsd
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_lBBsdGbBfbscqsd

.L_16_blocks_overflow_lBBsdGbBfbscqsd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_lBBsdGbBfbscqsd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nrbAErbElhBeDqv





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nrbAErbElhBeDqv
.L_small_initial_partial_block_nrbAErbElhBeDqv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nrbAErbElhBeDqv:

	orq	%r8,%r8
	je	.L_after_reduction_nrbAErbElhBeDqv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_nrbAErbElhBeDqv:
	jmp	.L_last_blocks_done_byewvAufdxezrnj
.L_last_num_blocks_is_3_byewvAufdxezrnj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_lEyhoCGhrBCnpEe
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_lEyhoCGhrBCnpEe

.L_16_blocks_overflow_lEyhoCGhrBCnpEe:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_lEyhoCGhrBCnpEe:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_thixgtwsoavsFxl





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_thixgtwsoavsFxl
.L_small_initial_partial_block_thixgtwsoavsFxl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_thixgtwsoavsFxl:

	orq	%r8,%r8
	je	.L_after_reduction_thixgtwsoavsFxl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_thixgtwsoavsFxl:
	jmp	.L_last_blocks_done_byewvAufdxezrnj
.L_last_num_blocks_is_4_byewvAufdxezrnj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_jgyFozsaisoBfeq
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_jgyFozsaisoBfeq

.L_16_blocks_overflow_jgyFozsaisoBfeq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_jgyFozsaisoBfeq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_waCbapEjctllDex





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_waCbapEjctllDex
.L_small_initial_partial_block_waCbapEjctllDex:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_waCbapEjctllDex:

	orq	%r8,%r8
	je	.L_after_reduction_waCbapEjctllDex
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_waCbapEjctllDex:
	jmp	.L_last_blocks_done_byewvAufdxezrnj
.L_last_num_blocks_is_5_byewvAufdxezrnj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_fvnDrpBCbmluvlG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_fvnDrpBCbmluvlG

.L_16_blocks_overflow_fvnDrpBCbmluvlG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_fvnDrpBCbmluvlG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iylcqhvoszoszel





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iylcqhvoszoszel
.L_small_initial_partial_block_iylcqhvoszoszel:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iylcqhvoszoszel:

	orq	%r8,%r8
	je	.L_after_reduction_iylcqhvoszoszel
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_iylcqhvoszoszel:
	jmp	.L_last_blocks_done_byewvAufdxezrnj
.L_last_num_blocks_is_6_byewvAufdxezrnj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_vDGbBhqwxpBoChf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_vDGbBhqwxpBoChf

.L_16_blocks_overflow_vDGbBhqwxpBoChf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_vDGbBhqwxpBoChf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lepbEwgFBsxaFGz





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lepbEwgFBsxaFGz
.L_small_initial_partial_block_lepbEwgFBsxaFGz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lepbEwgFBsxaFGz:

	orq	%r8,%r8
	je	.L_after_reduction_lepbEwgFBsxaFGz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lepbEwgFBsxaFGz:
	jmp	.L_last_blocks_done_byewvAufdxezrnj
.L_last_num_blocks_is_7_byewvAufdxezrnj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_Dyrpbdigimmiduc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_Dyrpbdigimmiduc

.L_16_blocks_overflow_Dyrpbdigimmiduc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_Dyrpbdigimmiduc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bFvoEaqFpoAqCdx





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bFvoEaqFpoAqCdx
.L_small_initial_partial_block_bFvoEaqFpoAqCdx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bFvoEaqFpoAqCdx:

	orq	%r8,%r8
	je	.L_after_reduction_bFvoEaqFpoAqCdx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bFvoEaqFpoAqCdx:
	jmp	.L_last_blocks_done_byewvAufdxezrnj
.L_last_num_blocks_is_8_byewvAufdxezrnj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_ArGwaAaizEvkABo
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_ArGwaAaizEvkABo

.L_16_blocks_overflow_ArGwaAaizEvkABo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_ArGwaAaizEvkABo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kFmgaFvjbtFjwxw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kFmgaFvjbtFjwxw
.L_small_initial_partial_block_kFmgaFvjbtFjwxw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kFmgaFvjbtFjwxw:

	orq	%r8,%r8
	je	.L_after_reduction_kFmgaFvjbtFjwxw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kFmgaFvjbtFjwxw:
	jmp	.L_last_blocks_done_byewvAufdxezrnj
.L_last_num_blocks_is_9_byewvAufdxezrnj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_BFvynhyAnlhhAck
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_BFvynhyAnlhhAck

.L_16_blocks_overflow_BFvynhyAnlhhAck:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_BFvynhyAnlhhAck:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jyeiabzCbpmibgA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jyeiabzCbpmibgA
.L_small_initial_partial_block_jyeiabzCbpmibgA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jyeiabzCbpmibgA:

	orq	%r8,%r8
	je	.L_after_reduction_jyeiabzCbpmibgA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jyeiabzCbpmibgA:
	jmp	.L_last_blocks_done_byewvAufdxezrnj
.L_last_num_blocks_is_10_byewvAufdxezrnj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_zyEnDlyaEssvice
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_zyEnDlyaEssvice

.L_16_blocks_overflow_zyEnDlyaEssvice:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_zyEnDlyaEssvice:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jhBiqchuhmajBeu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jhBiqchuhmajBeu
.L_small_initial_partial_block_jhBiqchuhmajBeu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jhBiqchuhmajBeu:

	orq	%r8,%r8
	je	.L_after_reduction_jhBiqchuhmajBeu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jhBiqchuhmajBeu:
	jmp	.L_last_blocks_done_byewvAufdxezrnj
.L_last_num_blocks_is_11_byewvAufdxezrnj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_olpvCsBpkvdujzb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_olpvCsBpkvdujzb

.L_16_blocks_overflow_olpvCsBpkvdujzb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_olpvCsBpkvdujzb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_aCvzcbtmxwliptF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_aCvzcbtmxwliptF
.L_small_initial_partial_block_aCvzcbtmxwliptF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_aCvzcbtmxwliptF:

	orq	%r8,%r8
	je	.L_after_reduction_aCvzcbtmxwliptF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_aCvzcbtmxwliptF:
	jmp	.L_last_blocks_done_byewvAufdxezrnj
.L_last_num_blocks_is_12_byewvAufdxezrnj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_bdwgDFnECBvoyGj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_bdwgDFnECBvoyGj

.L_16_blocks_overflow_bdwgDFnECBvoyGj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_bdwgDFnECBvoyGj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bqtzapfBskwrrrb





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bqtzapfBskwrrrb
.L_small_initial_partial_block_bqtzapfBskwrrrb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bqtzapfBskwrrrb:

	orq	%r8,%r8
	je	.L_after_reduction_bqtzapfBskwrrrb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bqtzapfBskwrrrb:
	jmp	.L_last_blocks_done_byewvAufdxezrnj
.L_last_num_blocks_is_13_byewvAufdxezrnj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_veFryhwfzyztBrD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_veFryhwfzyztBrD

.L_16_blocks_overflow_veFryhwfzyztBrD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_veFryhwfzyztBrD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zjuqApplqjbldvt





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zjuqApplqjbldvt
.L_small_initial_partial_block_zjuqApplqjbldvt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zjuqApplqjbldvt:

	orq	%r8,%r8
	je	.L_after_reduction_zjuqApplqjbldvt
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zjuqApplqjbldvt:
	jmp	.L_last_blocks_done_byewvAufdxezrnj
.L_last_num_blocks_is_14_byewvAufdxezrnj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_uqchBejnFFjcEcn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_uqchBejnFFjcEcn

.L_16_blocks_overflow_uqchBejnFFjcEcn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_uqchBejnFFjcEcn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rfmGmmiqltqwEEa





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rfmGmmiqltqwEEa
.L_small_initial_partial_block_rfmGmmiqltqwEEa:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rfmGmmiqltqwEEa:

	orq	%r8,%r8
	je	.L_after_reduction_rfmGmmiqltqwEEa
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rfmGmmiqltqwEEa:
	jmp	.L_last_blocks_done_byewvAufdxezrnj
.L_last_num_blocks_is_15_byewvAufdxezrnj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_mrwjGzbADuqbxww
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_mrwjGzbADuqbxww

.L_16_blocks_overflow_mrwjGzbADuqbxww:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_mrwjGzbADuqbxww:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qgftnwnkuohtEBp





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qgftnwnkuohtEBp
.L_small_initial_partial_block_qgftnwnkuohtEBp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qgftnwnkuohtEBp:

	orq	%r8,%r8
	je	.L_after_reduction_qgftnwnkuohtEBp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_qgftnwnkuohtEBp:
	jmp	.L_last_blocks_done_byewvAufdxezrnj
.L_last_num_blocks_is_16_byewvAufdxezrnj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_iumCupfybaBovjo
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_iumCupfybaBovjo

.L_16_blocks_overflow_iumCupfybaBovjo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_iumCupfybaBovjo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_ntmpvvizbkxwbxa:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ntmpvvizbkxwbxa:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ntmpvvizbkxwbxa:
	jmp	.L_last_blocks_done_byewvAufdxezrnj
.L_last_num_blocks_is_0_byewvAufdxezrnj:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_byewvAufdxezrnj:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_vniEckCGskxEjje

.L_message_below_equal_16_blocks_vniEckCGskxEjje:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	.L_small_initial_num_blocks_is_8_oBycmvbndpozjcq
	jl	.L_small_initial_num_blocks_is_7_1_oBycmvbndpozjcq


	cmpq	$12,%r12
	je	.L_small_initial_num_blocks_is_12_oBycmvbndpozjcq
	jl	.L_small_initial_num_blocks_is_11_9_oBycmvbndpozjcq


	cmpq	$16,%r12
	je	.L_small_initial_num_blocks_is_16_oBycmvbndpozjcq
	cmpq	$15,%r12
	je	.L_small_initial_num_blocks_is_15_oBycmvbndpozjcq
	cmpq	$14,%r12
	je	.L_small_initial_num_blocks_is_14_oBycmvbndpozjcq
	jmp	.L_small_initial_num_blocks_is_13_oBycmvbndpozjcq

.L_small_initial_num_blocks_is_11_9_oBycmvbndpozjcq:

	cmpq	$11,%r12
	je	.L_small_initial_num_blocks_is_11_oBycmvbndpozjcq
	cmpq	$10,%r12
	je	.L_small_initial_num_blocks_is_10_oBycmvbndpozjcq
	jmp	.L_small_initial_num_blocks_is_9_oBycmvbndpozjcq

.L_small_initial_num_blocks_is_7_1_oBycmvbndpozjcq:
	cmpq	$4,%r12
	je	.L_small_initial_num_blocks_is_4_oBycmvbndpozjcq
	jl	.L_small_initial_num_blocks_is_3_1_oBycmvbndpozjcq

	cmpq	$7,%r12
	je	.L_small_initial_num_blocks_is_7_oBycmvbndpozjcq
	cmpq	$6,%r12
	je	.L_small_initial_num_blocks_is_6_oBycmvbndpozjcq
	jmp	.L_small_initial_num_blocks_is_5_oBycmvbndpozjcq

.L_small_initial_num_blocks_is_3_1_oBycmvbndpozjcq:

	cmpq	$3,%r12
	je	.L_small_initial_num_blocks_is_3_oBycmvbndpozjcq
	cmpq	$2,%r12
	je	.L_small_initial_num_blocks_is_2_oBycmvbndpozjcq





.L_small_initial_num_blocks_is_1_oBycmvbndpozjcq:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%r11,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GFBaAiiniBCEAGG





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GFBaAiiniBCEAGG
.L_small_initial_partial_block_GFBaAiiniBCEAGG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	.L_after_reduction_GFBaAiiniBCEAGG
.L_small_initial_compute_done_GFBaAiiniBCEAGG:
.L_after_reduction_GFBaAiiniBCEAGG:
	jmp	.L_small_initial_blocks_encrypted_oBycmvbndpozjcq
.L_small_initial_num_blocks_is_2_oBycmvbndpozjcq:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%r11,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CnbsbxnGBkehsEj





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CnbsbxnGBkehsEj
.L_small_initial_partial_block_CnbsbxnGBkehsEj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CnbsbxnGBkehsEj:

	orq	%r8,%r8
	je	.L_after_reduction_CnbsbxnGBkehsEj
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_CnbsbxnGBkehsEj:
	jmp	.L_small_initial_blocks_encrypted_oBycmvbndpozjcq
.L_small_initial_num_blocks_is_3_oBycmvbndpozjcq:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EvygEeszizqxfow





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EvygEeszizqxfow
.L_small_initial_partial_block_EvygEeszizqxfow:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EvygEeszizqxfow:

	orq	%r8,%r8
	je	.L_after_reduction_EvygEeszizqxfow
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_EvygEeszizqxfow:
	jmp	.L_small_initial_blocks_encrypted_oBycmvbndpozjcq
.L_small_initial_num_blocks_is_4_oBycmvbndpozjcq:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GuCEBAgAnlGuwes





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GuCEBAgAnlGuwes
.L_small_initial_partial_block_GuCEBAgAnlGuwes:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GuCEBAgAnlGuwes:

	orq	%r8,%r8
	je	.L_after_reduction_GuCEBAgAnlGuwes
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_GuCEBAgAnlGuwes:
	jmp	.L_small_initial_blocks_encrypted_oBycmvbndpozjcq
.L_small_initial_num_blocks_is_5_oBycmvbndpozjcq:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%xmm29,%xmm3,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qaDjBkCDbjfDkac





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qaDjBkCDbjfDkac
.L_small_initial_partial_block_qaDjBkCDbjfDkac:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qaDjBkCDbjfDkac:

	orq	%r8,%r8
	je	.L_after_reduction_qaDjBkCDbjfDkac
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_qaDjBkCDbjfDkac:
	jmp	.L_small_initial_blocks_encrypted_oBycmvbndpozjcq
.L_small_initial_num_blocks_is_6_oBycmvbndpozjcq:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%ymm29,%ymm3,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EApwjEtrobnhBre





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EApwjEtrobnhBre
.L_small_initial_partial_block_EApwjEtrobnhBre:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EApwjEtrobnhBre:

	orq	%r8,%r8
	je	.L_after_reduction_EApwjEtrobnhBre
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_EApwjEtrobnhBre:
	jmp	.L_small_initial_blocks_encrypted_oBycmvbndpozjcq
.L_small_initial_num_blocks_is_7_oBycmvbndpozjcq:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pqpCkbemxGdysoo





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pqpCkbemxGdysoo
.L_small_initial_partial_block_pqpCkbemxGdysoo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pqpCkbemxGdysoo:

	orq	%r8,%r8
	je	.L_after_reduction_pqpCkbemxGdysoo
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_pqpCkbemxGdysoo:
	jmp	.L_small_initial_blocks_encrypted_oBycmvbndpozjcq
.L_small_initial_num_blocks_is_8_oBycmvbndpozjcq:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yqBnlljudhtuEyq





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yqBnlljudhtuEyq
.L_small_initial_partial_block_yqBnlljudhtuEyq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yqBnlljudhtuEyq:

	orq	%r8,%r8
	je	.L_after_reduction_yqBnlljudhtuEyq
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_yqBnlljudhtuEyq:
	jmp	.L_small_initial_blocks_encrypted_oBycmvbndpozjcq
.L_small_initial_num_blocks_is_9_oBycmvbndpozjcq:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%xmm29,%xmm4,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_djpnrAbasnlBsdG





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_djpnrAbasnlBsdG
.L_small_initial_partial_block_djpnrAbasnlBsdG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_djpnrAbasnlBsdG:

	orq	%r8,%r8
	je	.L_after_reduction_djpnrAbasnlBsdG
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_djpnrAbasnlBsdG:
	jmp	.L_small_initial_blocks_encrypted_oBycmvbndpozjcq
.L_small_initial_num_blocks_is_10_oBycmvbndpozjcq:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%ymm29,%ymm4,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fxkwnnuAaFDvqve





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fxkwnnuAaFDvqve
.L_small_initial_partial_block_fxkwnnuAaFDvqve:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_fxkwnnuAaFDvqve:

	orq	%r8,%r8
	je	.L_after_reduction_fxkwnnuAaFDvqve
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_fxkwnnuAaFDvqve:
	jmp	.L_small_initial_blocks_encrypted_oBycmvbndpozjcq
.L_small_initial_num_blocks_is_11_oBycmvbndpozjcq:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_oCseqifBqlaefwy





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_oCseqifBqlaefwy
.L_small_initial_partial_block_oCseqifBqlaefwy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_oCseqifBqlaefwy:

	orq	%r8,%r8
	je	.L_after_reduction_oCseqifBqlaefwy
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_oCseqifBqlaefwy:
	jmp	.L_small_initial_blocks_encrypted_oBycmvbndpozjcq
.L_small_initial_num_blocks_is_12_oBycmvbndpozjcq:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nouswknpduyusih





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nouswknpduyusih
.L_small_initial_partial_block_nouswknpduyusih:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nouswknpduyusih:

	orq	%r8,%r8
	je	.L_after_reduction_nouswknpduyusih
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_nouswknpduyusih:
	jmp	.L_small_initial_blocks_encrypted_oBycmvbndpozjcq
.L_small_initial_num_blocks_is_13_oBycmvbndpozjcq:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%xmm29,%xmm5,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rffzclxwEjmoDpj





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rffzclxwEjmoDpj
.L_small_initial_partial_block_rffzclxwEjmoDpj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rffzclxwEjmoDpj:

	orq	%r8,%r8
	je	.L_after_reduction_rffzclxwEjmoDpj
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_rffzclxwEjmoDpj:
	jmp	.L_small_initial_blocks_encrypted_oBycmvbndpozjcq
.L_small_initial_num_blocks_is_14_oBycmvbndpozjcq:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%ymm29,%ymm5,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wxmdBFCuEFBjyiF





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wxmdBFCuEFBjyiF
.L_small_initial_partial_block_wxmdBFCuEFBjyiF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wxmdBFCuEFBjyiF:

	orq	%r8,%r8
	je	.L_after_reduction_wxmdBFCuEFBjyiF
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_wxmdBFCuEFBjyiF:
	jmp	.L_small_initial_blocks_encrypted_oBycmvbndpozjcq
.L_small_initial_num_blocks_is_15_oBycmvbndpozjcq:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_sEragtcfaCGkjoG





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_sEragtcfaCGkjoG
.L_small_initial_partial_block_sEragtcfaCGkjoG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_sEragtcfaCGkjoG:

	orq	%r8,%r8
	je	.L_after_reduction_sEragtcfaCGkjoG
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_sEragtcfaCGkjoG:
	jmp	.L_small_initial_blocks_encrypted_oBycmvbndpozjcq
.L_small_initial_num_blocks_is_16_oBycmvbndpozjcq:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_qganDrbfswqxtpb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qganDrbfswqxtpb:
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_qganDrbfswqxtpb:
.L_small_initial_blocks_encrypted_oBycmvbndpozjcq:
.L_ghash_done_vniEckCGskxEjje:
	vmovdqu64	%xmm2,0(%rsi)
	vmovdqu64	%xmm14,64(%rsi)
.L_enc_dec_done_vniEckCGskxEjje:
	jmp	.Lexit_gcm_encrypt
.Lexit_gcm_encrypt:
	cmpq	$256,%r8
	jbe	.Lskip_hkeys_cleanup_erGnfBggwyuevtA
	vpxor	%xmm0,%xmm0,%xmm0
	vmovdqa64	%zmm0,0(%rsp)
	vmovdqa64	%zmm0,64(%rsp)
	vmovdqa64	%zmm0,128(%rsp)
	vmovdqa64	%zmm0,192(%rsp)
	vmovdqa64	%zmm0,256(%rsp)
	vmovdqa64	%zmm0,320(%rsp)
	vmovdqa64	%zmm0,384(%rsp)
	vmovdqa64	%zmm0,448(%rsp)
	vmovdqa64	%zmm0,512(%rsp)
	vmovdqa64	%zmm0,576(%rsp)
	vmovdqa64	%zmm0,640(%rsp)
	vmovdqa64	%zmm0,704(%rsp)
.Lskip_hkeys_cleanup_erGnfBggwyuevtA:
	vzeroupper
	leaq	(%rbp),%rsp
.cfi_def_cfa_register	%rsp
	popq	%r15
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r15
	popq	%r14
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r14
	popq	%r13
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r13
	popq	%r12
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r12
	popq	%rbp
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbp
	popq	%rbx
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbx
	.byte	0xf3,0xc3
.Lencrypt_seh_end:
.cfi_endproc	
.size	ossl_aes_gcm_encrypt_avx512, .-ossl_aes_gcm_encrypt_avx512
.globl	ossl_aes_gcm_decrypt_avx512
.type	ossl_aes_gcm_decrypt_avx512,@function
.align	32
ossl_aes_gcm_decrypt_avx512:
.cfi_startproc	
.Ldecrypt_seh_begin:
.byte	243,15,30,250
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
.Ldecrypt_seh_push_rbx:
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
.Ldecrypt_seh_push_rbp:
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
.Ldecrypt_seh_push_r12:
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
.Ldecrypt_seh_push_r13:
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
.Ldecrypt_seh_push_r14:
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
.Ldecrypt_seh_push_r15:










	leaq	0(%rsp),%rbp
.cfi_def_cfa_register	%rbp
.Ldecrypt_seh_setfp:

.Ldecrypt_seh_prolog_end:
	subq	$1588,%rsp
	andq	$(-64),%rsp


	movl	240(%rdi),%eax
	cmpl	$9,%eax
	je	.Laes_gcm_decrypt_128_avx512
	cmpl	$11,%eax
	je	.Laes_gcm_decrypt_192_avx512
	cmpl	$13,%eax
	je	.Laes_gcm_decrypt_256_avx512
	xorl	%eax,%eax
	jmp	.Lexit_gcm_decrypt
.align	32
.Laes_gcm_decrypt_128_avx512:
	orq	%r8,%r8
	je	.L_enc_dec_done_gbfeFpdczgiEidA
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14

	movq	(%rdx),%r11
	orq	%r11,%r11
	je	.L_partial_block_done_wsFgbCiwsgzbFso
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3
	vmovdqu64	336(%rsi),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%r11,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3

	vmovdqa64	%xmm0,%xmm6
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%r11,1),%r13
	subq	$16,%r13
	jge	.L_no_extra_mask_wsFgbCiwsgzbFso
	subq	%r13,%r12
.L_no_extra_mask_wsFgbCiwsgzbFso:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpand	%xmm0,%xmm6,%xmm6
	vpshufb	SHUF_MASK(%rip),%xmm6,%xmm6
	vpshufb	%xmm5,%xmm6,%xmm6
	vpxorq	%xmm6,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	.L_partial_incomplete_wsFgbCiwsgzbFso

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movq	$0,(%rdx)

	movq	%r11,%r12
	movq	$16,%r11
	subq	%r12,%r11
	jmp	.L_enc_dec_done_wsFgbCiwsgzbFso

.L_partial_incomplete_wsFgbCiwsgzbFso:
	addq	%r8,(%rdx)
	movq	%r8,%r11

.L_enc_dec_done_wsFgbCiwsgzbFso:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%r11,2),%k1
	vmovdqu64	%xmm14,64(%rsi)
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
.L_partial_block_done_wsFgbCiwsgzbFso:
	vmovdqu64	0(%rsi),%xmm2
	subq	%r11,%r8
	je	.L_enc_dec_done_gbfeFpdczgiEidA
	cmpq	$256,%r8
	jbe	.L_message_below_equal_16_blocks_gbfeFpdczgiEidA

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_xvEDtCifeytshCg
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_xvEDtCifeytshCg
.L_next_16_overflow_xvEDtCifeytshCg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_xvEDtCifeytshCg:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%r11,1),%zmm0
	vmovdqu8	64(%rcx,%r11,1),%zmm3
	vmovdqu8	128(%rcx,%r11,1),%zmm4
	vmovdqu8	192(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%r11,1)
	vmovdqu8	%zmm10,64(%r10,%r11,1)
	vmovdqu8	%zmm11,128(%r10,%r11,1)
	vmovdqu8	%zmm12,192(%r10,%r11,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_rzlhBnxGgxzrnjE

	vmovdqu64	288(%rsi),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	224(%rsi),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	160(%rsi),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	96(%rsi),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
.L_skip_hkeys_precomputation_rzlhBnxGgxzrnjE:
	cmpq	$512,%r8
	jb	.L_message_below_32_blocks_gbfeFpdczgiEidA



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_nacdbyDafCtqucj
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_nacdbyDafCtqucj
.L_next_16_overflow_nacdbyDafCtqucj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_nacdbyDafCtqucj:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%r11,1),%zmm0
	vmovdqu8	320(%rcx,%r11,1),%zmm3
	vmovdqu8	384(%rcx,%r11,1),%zmm4
	vmovdqu8	448(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%r11,1)
	vmovdqu8	%zmm10,320(%r10,%r11,1)
	vmovdqu8	%zmm11,384(%r10,%r11,1)
	vmovdqu8	%zmm12,448(%r10,%r11,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_AAqjuzCBdchAAsn
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
.L_skip_hkeys_precomputation_AAqjuzCBdchAAsn:
	movq	$1,%r14
	addq	$512,%r11
	subq	$512,%r8

	cmpq	$768,%r8
	jb	.L_no_more_big_nblocks_gbfeFpdczgiEidA
.L_encrypt_big_nblocks_gbfeFpdczgiEidA:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_DttutBzrCdCtzAi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_DttutBzrCdCtzAi
.L_16_blocks_overflow_DttutBzrCdCtzAi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_DttutBzrCdCtzAi:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_chctgjhksdvCFwz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_chctgjhksdvCFwz
.L_16_blocks_overflow_chctgjhksdvCFwz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_chctgjhksdvCFwz:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_jxzuCrpwAlcaufn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_jxzuCrpwAlcaufn
.L_16_blocks_overflow_jxzuCrpwAlcaufn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_jxzuCrpwAlcaufn:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%r11,1),%zmm17
	vmovdqu8	576(%rcx,%r11,1),%zmm19
	vmovdqu8	640(%rcx,%r11,1),%zmm20
	vmovdqu8	704(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%r11,1)
	vmovdqu8	%zmm3,576(%r10,%r11,1)
	vmovdqu8	%zmm4,640(%r10,%r11,1)
	vmovdqu8	%zmm5,704(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%r11
	subq	$768,%r8
	cmpq	$768,%r8
	jae	.L_encrypt_big_nblocks_gbfeFpdczgiEidA

.L_no_more_big_nblocks_gbfeFpdczgiEidA:

	cmpq	$512,%r8
	jae	.L_encrypt_32_blocks_gbfeFpdczgiEidA

	cmpq	$256,%r8
	jae	.L_encrypt_16_blocks_gbfeFpdczgiEidA
.L_encrypt_0_blocks_ghash_32_gbfeFpdczgiEidA:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_oEmGGbejpnGkgdx

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_oEmGGbejpnGkgdx
	jb	.L_last_num_blocks_is_7_1_oEmGGbejpnGkgdx


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_oEmGGbejpnGkgdx
	jb	.L_last_num_blocks_is_11_9_oEmGGbejpnGkgdx


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_oEmGGbejpnGkgdx
	ja	.L_last_num_blocks_is_16_oEmGGbejpnGkgdx
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_oEmGGbejpnGkgdx
	jmp	.L_last_num_blocks_is_13_oEmGGbejpnGkgdx

.L_last_num_blocks_is_11_9_oEmGGbejpnGkgdx:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_oEmGGbejpnGkgdx
	ja	.L_last_num_blocks_is_11_oEmGGbejpnGkgdx
	jmp	.L_last_num_blocks_is_9_oEmGGbejpnGkgdx

.L_last_num_blocks_is_7_1_oEmGGbejpnGkgdx:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_oEmGGbejpnGkgdx
	jb	.L_last_num_blocks_is_3_1_oEmGGbejpnGkgdx

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_oEmGGbejpnGkgdx
	je	.L_last_num_blocks_is_6_oEmGGbejpnGkgdx
	jmp	.L_last_num_blocks_is_5_oEmGGbejpnGkgdx

.L_last_num_blocks_is_3_1_oEmGGbejpnGkgdx:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_oEmGGbejpnGkgdx
	je	.L_last_num_blocks_is_2_oEmGGbejpnGkgdx
.L_last_num_blocks_is_1_oEmGGbejpnGkgdx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_ltbBlrhxcDbsxGh
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_ltbBlrhxcDbsxGh

.L_16_blocks_overflow_ltbBlrhxcDbsxGh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_ltbBlrhxcDbsxGh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nmcgBzpqAvDewsB





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nmcgBzpqAvDewsB
.L_small_initial_partial_block_nmcgBzpqAvDewsB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_nmcgBzpqAvDewsB
.L_small_initial_compute_done_nmcgBzpqAvDewsB:
.L_after_reduction_nmcgBzpqAvDewsB:
	jmp	.L_last_blocks_done_oEmGGbejpnGkgdx
.L_last_num_blocks_is_2_oEmGGbejpnGkgdx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_cjbxpBFGACaufqF
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_cjbxpBFGACaufqF

.L_16_blocks_overflow_cjbxpBFGACaufqF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_cjbxpBFGACaufqF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ekFxzmownlnpitm





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ekFxzmownlnpitm
.L_small_initial_partial_block_ekFxzmownlnpitm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ekFxzmownlnpitm:

	orq	%r8,%r8
	je	.L_after_reduction_ekFxzmownlnpitm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ekFxzmownlnpitm:
	jmp	.L_last_blocks_done_oEmGGbejpnGkgdx
.L_last_num_blocks_is_3_oEmGGbejpnGkgdx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_cwzpAgGuyedrbmd
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_cwzpAgGuyedrbmd

.L_16_blocks_overflow_cwzpAgGuyedrbmd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_cwzpAgGuyedrbmd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_sgzBfCGemiojewn





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_sgzBfCGemiojewn
.L_small_initial_partial_block_sgzBfCGemiojewn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_sgzBfCGemiojewn:

	orq	%r8,%r8
	je	.L_after_reduction_sgzBfCGemiojewn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_sgzBfCGemiojewn:
	jmp	.L_last_blocks_done_oEmGGbejpnGkgdx
.L_last_num_blocks_is_4_oEmGGbejpnGkgdx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_rqwfyBasmBcqCjg
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_rqwfyBasmBcqCjg

.L_16_blocks_overflow_rqwfyBasmBcqCjg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_rqwfyBasmBcqCjg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gDCqEltEdemuDBF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gDCqEltEdemuDBF
.L_small_initial_partial_block_gDCqEltEdemuDBF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gDCqEltEdemuDBF:

	orq	%r8,%r8
	je	.L_after_reduction_gDCqEltEdemuDBF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gDCqEltEdemuDBF:
	jmp	.L_last_blocks_done_oEmGGbejpnGkgdx
.L_last_num_blocks_is_5_oEmGGbejpnGkgdx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_sDjistcqGeFxuiD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_sDjistcqGeFxuiD

.L_16_blocks_overflow_sDjistcqGeFxuiD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_sDjistcqGeFxuiD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tDnBnfwuqidFCvC





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tDnBnfwuqidFCvC
.L_small_initial_partial_block_tDnBnfwuqidFCvC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tDnBnfwuqidFCvC:

	orq	%r8,%r8
	je	.L_after_reduction_tDnBnfwuqidFCvC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tDnBnfwuqidFCvC:
	jmp	.L_last_blocks_done_oEmGGbejpnGkgdx
.L_last_num_blocks_is_6_oEmGGbejpnGkgdx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_FBlbuCpDzfodDGs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_FBlbuCpDzfodDGs

.L_16_blocks_overflow_FBlbuCpDzfodDGs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_FBlbuCpDzfodDGs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dBejmmEqDdalxzz





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dBejmmEqDdalxzz
.L_small_initial_partial_block_dBejmmEqDdalxzz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dBejmmEqDdalxzz:

	orq	%r8,%r8
	je	.L_after_reduction_dBejmmEqDdalxzz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dBejmmEqDdalxzz:
	jmp	.L_last_blocks_done_oEmGGbejpnGkgdx
.L_last_num_blocks_is_7_oEmGGbejpnGkgdx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_rnznnrnemzgaBcb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_rnznnrnemzgaBcb

.L_16_blocks_overflow_rnznnrnemzgaBcb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_rnznnrnemzgaBcb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ovfEfnBiDsvpktF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ovfEfnBiDsvpktF
.L_small_initial_partial_block_ovfEfnBiDsvpktF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ovfEfnBiDsvpktF:

	orq	%r8,%r8
	je	.L_after_reduction_ovfEfnBiDsvpktF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ovfEfnBiDsvpktF:
	jmp	.L_last_blocks_done_oEmGGbejpnGkgdx
.L_last_num_blocks_is_8_oEmGGbejpnGkgdx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_scpctbujxhledyC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_scpctbujxhledyC

.L_16_blocks_overflow_scpctbujxhledyC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_scpctbujxhledyC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_mEpxBopvAhpfFgq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_mEpxBopvAhpfFgq
.L_small_initial_partial_block_mEpxBopvAhpfFgq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mEpxBopvAhpfFgq:

	orq	%r8,%r8
	je	.L_after_reduction_mEpxBopvAhpfFgq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_mEpxBopvAhpfFgq:
	jmp	.L_last_blocks_done_oEmGGbejpnGkgdx
.L_last_num_blocks_is_9_oEmGGbejpnGkgdx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_rCcwynarahauEDw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_rCcwynarahauEDw

.L_16_blocks_overflow_rCcwynarahauEDw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_rCcwynarahauEDw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yrAjuptlAaykvEm





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yrAjuptlAaykvEm
.L_small_initial_partial_block_yrAjuptlAaykvEm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yrAjuptlAaykvEm:

	orq	%r8,%r8
	je	.L_after_reduction_yrAjuptlAaykvEm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_yrAjuptlAaykvEm:
	jmp	.L_last_blocks_done_oEmGGbejpnGkgdx
.L_last_num_blocks_is_10_oEmGGbejpnGkgdx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_gndDbhCybazbzwA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_gndDbhCybazbzwA

.L_16_blocks_overflow_gndDbhCybazbzwA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_gndDbhCybazbzwA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BcpwiDEbrtnByla





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BcpwiDEbrtnByla
.L_small_initial_partial_block_BcpwiDEbrtnByla:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BcpwiDEbrtnByla:

	orq	%r8,%r8
	je	.L_after_reduction_BcpwiDEbrtnByla
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BcpwiDEbrtnByla:
	jmp	.L_last_blocks_done_oEmGGbejpnGkgdx
.L_last_num_blocks_is_11_oEmGGbejpnGkgdx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_tEmcjjothiyszFD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_tEmcjjothiyszFD

.L_16_blocks_overflow_tEmcjjothiyszFD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_tEmcjjothiyszFD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_aGecqCttfagrEdg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_aGecqCttfagrEdg
.L_small_initial_partial_block_aGecqCttfagrEdg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_aGecqCttfagrEdg:

	orq	%r8,%r8
	je	.L_after_reduction_aGecqCttfagrEdg
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_aGecqCttfagrEdg:
	jmp	.L_last_blocks_done_oEmGGbejpnGkgdx
.L_last_num_blocks_is_12_oEmGGbejpnGkgdx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_dgivqjwiDuAaueD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_dgivqjwiDuAaueD

.L_16_blocks_overflow_dgivqjwiDuAaueD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_dgivqjwiDuAaueD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nbrhCeezsCDwDEg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nbrhCeezsCDwDEg
.L_small_initial_partial_block_nbrhCeezsCDwDEg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nbrhCeezsCDwDEg:

	orq	%r8,%r8
	je	.L_after_reduction_nbrhCeezsCDwDEg
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_nbrhCeezsCDwDEg:
	jmp	.L_last_blocks_done_oEmGGbejpnGkgdx
.L_last_num_blocks_is_13_oEmGGbejpnGkgdx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_hsGqDwwzqahbfjp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_hsGqDwwzqahbfjp

.L_16_blocks_overflow_hsGqDwwzqahbfjp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_hsGqDwwzqahbfjp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BizjpDGwyaBwwfj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BizjpDGwyaBwwfj
.L_small_initial_partial_block_BizjpDGwyaBwwfj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BizjpDGwyaBwwfj:

	orq	%r8,%r8
	je	.L_after_reduction_BizjpDGwyaBwwfj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BizjpDGwyaBwwfj:
	jmp	.L_last_blocks_done_oEmGGbejpnGkgdx
.L_last_num_blocks_is_14_oEmGGbejpnGkgdx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_nxvkamaEnjtxAuB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_nxvkamaEnjtxAuB

.L_16_blocks_overflow_nxvkamaEnjtxAuB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_nxvkamaEnjtxAuB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ckdohGCdoxriguD





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ckdohGCdoxriguD
.L_small_initial_partial_block_ckdohGCdoxriguD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ckdohGCdoxriguD:

	orq	%r8,%r8
	je	.L_after_reduction_ckdohGCdoxriguD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ckdohGCdoxriguD:
	jmp	.L_last_blocks_done_oEmGGbejpnGkgdx
.L_last_num_blocks_is_15_oEmGGbejpnGkgdx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_AzjzylljjhaqAjr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_AzjzylljjhaqAjr

.L_16_blocks_overflow_AzjzylljjhaqAjr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_AzjzylljjhaqAjr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_exzluafCjsGuDgB





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_exzluafCjsGuDgB
.L_small_initial_partial_block_exzluafCjsGuDgB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_exzluafCjsGuDgB:

	orq	%r8,%r8
	je	.L_after_reduction_exzluafCjsGuDgB
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_exzluafCjsGuDgB:
	jmp	.L_last_blocks_done_oEmGGbejpnGkgdx
.L_last_num_blocks_is_16_oEmGGbejpnGkgdx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_qsjxsllmnGpfCBl
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_qsjxsllmnGpfCBl

.L_16_blocks_overflow_qsjxsllmnGpfCBl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_qsjxsllmnGpfCBl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_FmGEFjqoilujwml:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FmGEFjqoilujwml:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FmGEFjqoilujwml:
	jmp	.L_last_blocks_done_oEmGGbejpnGkgdx
.L_last_num_blocks_is_0_oEmGGbejpnGkgdx:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_oEmGGbejpnGkgdx:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_gbfeFpdczgiEidA
.L_encrypt_32_blocks_gbfeFpdczgiEidA:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_muedCxBvGxgpphB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_muedCxBvGxgpphB
.L_16_blocks_overflow_muedCxBvGxgpphB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_muedCxBvGxgpphB:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_CwEjpBmthrzcpfi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_CwEjpBmthrzcpfi
.L_16_blocks_overflow_CwEjpBmthrzcpfi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_CwEjpBmthrzcpfi:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%r11
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_GiaBwbqDborkorj

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_GiaBwbqDborkorj
	jb	.L_last_num_blocks_is_7_1_GiaBwbqDborkorj


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_GiaBwbqDborkorj
	jb	.L_last_num_blocks_is_11_9_GiaBwbqDborkorj


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_GiaBwbqDborkorj
	ja	.L_last_num_blocks_is_16_GiaBwbqDborkorj
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_GiaBwbqDborkorj
	jmp	.L_last_num_blocks_is_13_GiaBwbqDborkorj

.L_last_num_blocks_is_11_9_GiaBwbqDborkorj:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_GiaBwbqDborkorj
	ja	.L_last_num_blocks_is_11_GiaBwbqDborkorj
	jmp	.L_last_num_blocks_is_9_GiaBwbqDborkorj

.L_last_num_blocks_is_7_1_GiaBwbqDborkorj:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_GiaBwbqDborkorj
	jb	.L_last_num_blocks_is_3_1_GiaBwbqDborkorj

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_GiaBwbqDborkorj
	je	.L_last_num_blocks_is_6_GiaBwbqDborkorj
	jmp	.L_last_num_blocks_is_5_GiaBwbqDborkorj

.L_last_num_blocks_is_3_1_GiaBwbqDborkorj:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_GiaBwbqDborkorj
	je	.L_last_num_blocks_is_2_GiaBwbqDborkorj
.L_last_num_blocks_is_1_GiaBwbqDborkorj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_Brabsiqliirurvt
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_Brabsiqliirurvt

.L_16_blocks_overflow_Brabsiqliirurvt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_Brabsiqliirurvt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vqhACrejnzfFoFA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vqhACrejnzfFoFA
.L_small_initial_partial_block_vqhACrejnzfFoFA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_vqhACrejnzfFoFA
.L_small_initial_compute_done_vqhACrejnzfFoFA:
.L_after_reduction_vqhACrejnzfFoFA:
	jmp	.L_last_blocks_done_GiaBwbqDborkorj
.L_last_num_blocks_is_2_GiaBwbqDborkorj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_mpsGpBAkkrtinzk
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_mpsGpBAkkrtinzk

.L_16_blocks_overflow_mpsGpBAkkrtinzk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_mpsGpBAkkrtinzk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_utBEtBypkABoAuw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_utBEtBypkABoAuw
.L_small_initial_partial_block_utBEtBypkABoAuw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_utBEtBypkABoAuw:

	orq	%r8,%r8
	je	.L_after_reduction_utBEtBypkABoAuw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_utBEtBypkABoAuw:
	jmp	.L_last_blocks_done_GiaBwbqDborkorj
.L_last_num_blocks_is_3_GiaBwbqDborkorj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_ucpijCqufvrhwuB
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_ucpijCqufvrhwuB

.L_16_blocks_overflow_ucpijCqufvrhwuB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_ucpijCqufvrhwuB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zAwbGBtamCexxoy





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zAwbGBtamCexxoy
.L_small_initial_partial_block_zAwbGBtamCexxoy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zAwbGBtamCexxoy:

	orq	%r8,%r8
	je	.L_after_reduction_zAwbGBtamCexxoy
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zAwbGBtamCexxoy:
	jmp	.L_last_blocks_done_GiaBwbqDborkorj
.L_last_num_blocks_is_4_GiaBwbqDborkorj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_tbmhgkoghywEGmz
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_tbmhgkoghywEGmz

.L_16_blocks_overflow_tbmhgkoghywEGmz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_tbmhgkoghywEGmz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iBxBgyrzpDEkgtw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iBxBgyrzpDEkgtw
.L_small_initial_partial_block_iBxBgyrzpDEkgtw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iBxBgyrzpDEkgtw:

	orq	%r8,%r8
	je	.L_after_reduction_iBxBgyrzpDEkgtw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_iBxBgyrzpDEkgtw:
	jmp	.L_last_blocks_done_GiaBwbqDborkorj
.L_last_num_blocks_is_5_GiaBwbqDborkorj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_vktfestkjtqyBvh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_vktfestkjtqyBvh

.L_16_blocks_overflow_vktfestkjtqyBvh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_vktfestkjtqyBvh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cgmFxmtieEEhnFv





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cgmFxmtieEEhnFv
.L_small_initial_partial_block_cgmFxmtieEEhnFv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cgmFxmtieEEhnFv:

	orq	%r8,%r8
	je	.L_after_reduction_cgmFxmtieEEhnFv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cgmFxmtieEEhnFv:
	jmp	.L_last_blocks_done_GiaBwbqDborkorj
.L_last_num_blocks_is_6_GiaBwbqDborkorj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_tmbvFGFxesnhjnf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_tmbvFGFxesnhjnf

.L_16_blocks_overflow_tmbvFGFxesnhjnf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_tmbvFGFxesnhjnf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_usieuffEsDqlidl





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_usieuffEsDqlidl
.L_small_initial_partial_block_usieuffEsDqlidl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_usieuffEsDqlidl:

	orq	%r8,%r8
	je	.L_after_reduction_usieuffEsDqlidl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_usieuffEsDqlidl:
	jmp	.L_last_blocks_done_GiaBwbqDborkorj
.L_last_num_blocks_is_7_GiaBwbqDborkorj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_tAkxpgbanBraDdg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_tAkxpgbanBraDdg

.L_16_blocks_overflow_tAkxpgbanBraDdg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_tAkxpgbanBraDdg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tzmArbvtbgBtiFE





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tzmArbvtbgBtiFE
.L_small_initial_partial_block_tzmArbvtbgBtiFE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tzmArbvtbgBtiFE:

	orq	%r8,%r8
	je	.L_after_reduction_tzmArbvtbgBtiFE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tzmArbvtbgBtiFE:
	jmp	.L_last_blocks_done_GiaBwbqDborkorj
.L_last_num_blocks_is_8_GiaBwbqDborkorj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_rcaoxehptzgorla
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_rcaoxehptzgorla

.L_16_blocks_overflow_rcaoxehptzgorla:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_rcaoxehptzgorla:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ExsEcwgbDGDvzqr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ExsEcwgbDGDvzqr
.L_small_initial_partial_block_ExsEcwgbDGDvzqr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ExsEcwgbDGDvzqr:

	orq	%r8,%r8
	je	.L_after_reduction_ExsEcwgbDGDvzqr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ExsEcwgbDGDvzqr:
	jmp	.L_last_blocks_done_GiaBwbqDborkorj
.L_last_num_blocks_is_9_GiaBwbqDborkorj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_hdweodbBxpnhepx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_hdweodbBxpnhepx

.L_16_blocks_overflow_hdweodbBxpnhepx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_hdweodbBxpnhepx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FpphgdEEsqhqsfw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FpphgdEEsqhqsfw
.L_small_initial_partial_block_FpphgdEEsqhqsfw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FpphgdEEsqhqsfw:

	orq	%r8,%r8
	je	.L_after_reduction_FpphgdEEsqhqsfw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FpphgdEEsqhqsfw:
	jmp	.L_last_blocks_done_GiaBwbqDborkorj
.L_last_num_blocks_is_10_GiaBwbqDborkorj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_qsFFwsEjhcAnlog
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_qsFFwsEjhcAnlog

.L_16_blocks_overflow_qsFFwsEjhcAnlog:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_qsFFwsEjhcAnlog:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lomiBEAlmAuzcfF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lomiBEAlmAuzcfF
.L_small_initial_partial_block_lomiBEAlmAuzcfF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lomiBEAlmAuzcfF:

	orq	%r8,%r8
	je	.L_after_reduction_lomiBEAlmAuzcfF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lomiBEAlmAuzcfF:
	jmp	.L_last_blocks_done_GiaBwbqDborkorj
.L_last_num_blocks_is_11_GiaBwbqDborkorj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_myhEorGdejxonzj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_myhEorGdejxonzj

.L_16_blocks_overflow_myhEorGdejxonzj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_myhEorGdejxonzj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cEyqtFqryczuzhq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cEyqtFqryczuzhq
.L_small_initial_partial_block_cEyqtFqryczuzhq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cEyqtFqryczuzhq:

	orq	%r8,%r8
	je	.L_after_reduction_cEyqtFqryczuzhq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cEyqtFqryczuzhq:
	jmp	.L_last_blocks_done_GiaBwbqDborkorj
.L_last_num_blocks_is_12_GiaBwbqDborkorj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_nrrBedEykqaaxdv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_nrrBedEykqaaxdv

.L_16_blocks_overflow_nrrBedEykqaaxdv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_nrrBedEykqaaxdv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FsfbskvwyEngsrE





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FsfbskvwyEngsrE
.L_small_initial_partial_block_FsfbskvwyEngsrE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FsfbskvwyEngsrE:

	orq	%r8,%r8
	je	.L_after_reduction_FsfbskvwyEngsrE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FsfbskvwyEngsrE:
	jmp	.L_last_blocks_done_GiaBwbqDborkorj
.L_last_num_blocks_is_13_GiaBwbqDborkorj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_iptnBEdllGjowyw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_iptnBEdllGjowyw

.L_16_blocks_overflow_iptnBEdllGjowyw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_iptnBEdllGjowyw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_anGnAiCqksFqnsm





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_anGnAiCqksFqnsm
.L_small_initial_partial_block_anGnAiCqksFqnsm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_anGnAiCqksFqnsm:

	orq	%r8,%r8
	je	.L_after_reduction_anGnAiCqksFqnsm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_anGnAiCqksFqnsm:
	jmp	.L_last_blocks_done_GiaBwbqDborkorj
.L_last_num_blocks_is_14_GiaBwbqDborkorj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_iEwpjvGjGEjklBn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_iEwpjvGjGEjklBn

.L_16_blocks_overflow_iEwpjvGjGEjklBn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_iEwpjvGjGEjklBn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_profiwaovdBuFiD





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_profiwaovdBuFiD
.L_small_initial_partial_block_profiwaovdBuFiD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_profiwaovdBuFiD:

	orq	%r8,%r8
	je	.L_after_reduction_profiwaovdBuFiD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_profiwaovdBuFiD:
	jmp	.L_last_blocks_done_GiaBwbqDborkorj
.L_last_num_blocks_is_15_GiaBwbqDborkorj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_yijbeGDaxmzbpFx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_yijbeGDaxmzbpFx

.L_16_blocks_overflow_yijbeGDaxmzbpFx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_yijbeGDaxmzbpFx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_sxnyBpFDsAxiBpl





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_sxnyBpFDsAxiBpl
.L_small_initial_partial_block_sxnyBpFDsAxiBpl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_sxnyBpFDsAxiBpl:

	orq	%r8,%r8
	je	.L_after_reduction_sxnyBpFDsAxiBpl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_sxnyBpFDsAxiBpl:
	jmp	.L_last_blocks_done_GiaBwbqDborkorj
.L_last_num_blocks_is_16_GiaBwbqDborkorj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_ffpaBDBkyAmryqk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_ffpaBDBkyAmryqk

.L_16_blocks_overflow_ffpaBDBkyAmryqk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_ffpaBDBkyAmryqk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_Goypdpuwmcatolp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_Goypdpuwmcatolp:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_Goypdpuwmcatolp:
	jmp	.L_last_blocks_done_GiaBwbqDborkorj
.L_last_num_blocks_is_0_GiaBwbqDborkorj:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_GiaBwbqDborkorj:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_gbfeFpdczgiEidA
.L_encrypt_16_blocks_gbfeFpdczgiEidA:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_nbxkExjqhutlidb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_nbxkExjqhutlidb
.L_16_blocks_overflow_nbxkExjqhutlidb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_nbxkExjqhutlidb:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_pEjrionnahtrtzr

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_pEjrionnahtrtzr
	jb	.L_last_num_blocks_is_7_1_pEjrionnahtrtzr


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_pEjrionnahtrtzr
	jb	.L_last_num_blocks_is_11_9_pEjrionnahtrtzr


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_pEjrionnahtrtzr
	ja	.L_last_num_blocks_is_16_pEjrionnahtrtzr
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_pEjrionnahtrtzr
	jmp	.L_last_num_blocks_is_13_pEjrionnahtrtzr

.L_last_num_blocks_is_11_9_pEjrionnahtrtzr:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_pEjrionnahtrtzr
	ja	.L_last_num_blocks_is_11_pEjrionnahtrtzr
	jmp	.L_last_num_blocks_is_9_pEjrionnahtrtzr

.L_last_num_blocks_is_7_1_pEjrionnahtrtzr:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_pEjrionnahtrtzr
	jb	.L_last_num_blocks_is_3_1_pEjrionnahtrtzr

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_pEjrionnahtrtzr
	je	.L_last_num_blocks_is_6_pEjrionnahtrtzr
	jmp	.L_last_num_blocks_is_5_pEjrionnahtrtzr

.L_last_num_blocks_is_3_1_pEjrionnahtrtzr:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_pEjrionnahtrtzr
	je	.L_last_num_blocks_is_2_pEjrionnahtrtzr
.L_last_num_blocks_is_1_pEjrionnahtrtzr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_GravEfDaqvgCAoj
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_GravEfDaqvgCAoj

.L_16_blocks_overflow_GravEfDaqvgCAoj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_GravEfDaqvgCAoj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BqehFeEprsvjiao





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BqehFeEprsvjiao
.L_small_initial_partial_block_BqehFeEprsvjiao:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_BqehFeEprsvjiao
.L_small_initial_compute_done_BqehFeEprsvjiao:
.L_after_reduction_BqehFeEprsvjiao:
	jmp	.L_last_blocks_done_pEjrionnahtrtzr
.L_last_num_blocks_is_2_pEjrionnahtrtzr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_hCCBlEwnDbssbhr
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_hCCBlEwnDbssbhr

.L_16_blocks_overflow_hCCBlEwnDbssbhr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_hCCBlEwnDbssbhr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lhmwFzwybfufyyh





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lhmwFzwybfufyyh
.L_small_initial_partial_block_lhmwFzwybfufyyh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lhmwFzwybfufyyh:

	orq	%r8,%r8
	je	.L_after_reduction_lhmwFzwybfufyyh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lhmwFzwybfufyyh:
	jmp	.L_last_blocks_done_pEjrionnahtrtzr
.L_last_num_blocks_is_3_pEjrionnahtrtzr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_vtAbDqCniEaFkva
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_vtAbDqCniEaFkva

.L_16_blocks_overflow_vtAbDqCniEaFkva:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_vtAbDqCniEaFkva:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qEaobidiBzgqqdm





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qEaobidiBzgqqdm
.L_small_initial_partial_block_qEaobidiBzgqqdm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qEaobidiBzgqqdm:

	orq	%r8,%r8
	je	.L_after_reduction_qEaobidiBzgqqdm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_qEaobidiBzgqqdm:
	jmp	.L_last_blocks_done_pEjrionnahtrtzr
.L_last_num_blocks_is_4_pEjrionnahtrtzr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_DiEkbjjypDqjAyp
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_DiEkbjjypDqjAyp

.L_16_blocks_overflow_DiEkbjjypDqjAyp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_DiEkbjjypDqjAyp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yvslvaztuEozCwF





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yvslvaztuEozCwF
.L_small_initial_partial_block_yvslvaztuEozCwF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yvslvaztuEozCwF:

	orq	%r8,%r8
	je	.L_after_reduction_yvslvaztuEozCwF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_yvslvaztuEozCwF:
	jmp	.L_last_blocks_done_pEjrionnahtrtzr
.L_last_num_blocks_is_5_pEjrionnahtrtzr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_qDdgAlBCnnveCky
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_qDdgAlBCnnveCky

.L_16_blocks_overflow_qDdgAlBCnnveCky:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_qDdgAlBCnnveCky:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_klmykivqlttEwcd





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_klmykivqlttEwcd
.L_small_initial_partial_block_klmykivqlttEwcd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_klmykivqlttEwcd:

	orq	%r8,%r8
	je	.L_after_reduction_klmykivqlttEwcd
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_klmykivqlttEwcd:
	jmp	.L_last_blocks_done_pEjrionnahtrtzr
.L_last_num_blocks_is_6_pEjrionnahtrtzr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_BdddpzchlEgDxji
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_BdddpzchlEgDxji

.L_16_blocks_overflow_BdddpzchlEgDxji:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_BdddpzchlEgDxji:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gEkkqBnovdDFBbt





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gEkkqBnovdDFBbt
.L_small_initial_partial_block_gEkkqBnovdDFBbt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gEkkqBnovdDFBbt:

	orq	%r8,%r8
	je	.L_after_reduction_gEkkqBnovdDFBbt
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gEkkqBnovdDFBbt:
	jmp	.L_last_blocks_done_pEjrionnahtrtzr
.L_last_num_blocks_is_7_pEjrionnahtrtzr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_nEiruylrcymGCFC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_nEiruylrcymGCFC

.L_16_blocks_overflow_nEiruylrcymGCFC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_nEiruylrcymGCFC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BEjgnBiypEgiord





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BEjgnBiypEgiord
.L_small_initial_partial_block_BEjgnBiypEgiord:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BEjgnBiypEgiord:

	orq	%r8,%r8
	je	.L_after_reduction_BEjgnBiypEgiord
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BEjgnBiypEgiord:
	jmp	.L_last_blocks_done_pEjrionnahtrtzr
.L_last_num_blocks_is_8_pEjrionnahtrtzr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_AauFigAtvjinAqm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_AauFigAtvjinAqm

.L_16_blocks_overflow_AauFigAtvjinAqm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_AauFigAtvjinAqm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ctaxuwmhhlhapBD





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ctaxuwmhhlhapBD
.L_small_initial_partial_block_ctaxuwmhhlhapBD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ctaxuwmhhlhapBD:

	orq	%r8,%r8
	je	.L_after_reduction_ctaxuwmhhlhapBD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ctaxuwmhhlhapBD:
	jmp	.L_last_blocks_done_pEjrionnahtrtzr
.L_last_num_blocks_is_9_pEjrionnahtrtzr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_FpBbjzuixGsdqEd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_FpBbjzuixGsdqEd

.L_16_blocks_overflow_FpBbjzuixGsdqEd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_FpBbjzuixGsdqEd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_skkBDyetqCvkDbp





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_skkBDyetqCvkDbp
.L_small_initial_partial_block_skkBDyetqCvkDbp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_skkBDyetqCvkDbp:

	orq	%r8,%r8
	je	.L_after_reduction_skkBDyetqCvkDbp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_skkBDyetqCvkDbp:
	jmp	.L_last_blocks_done_pEjrionnahtrtzr
.L_last_num_blocks_is_10_pEjrionnahtrtzr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_yGyemlAizErcopE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_yGyemlAizErcopE

.L_16_blocks_overflow_yGyemlAizErcopE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_yGyemlAizErcopE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FuhibsynsnCklqz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FuhibsynsnCklqz
.L_small_initial_partial_block_FuhibsynsnCklqz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FuhibsynsnCklqz:

	orq	%r8,%r8
	je	.L_after_reduction_FuhibsynsnCklqz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FuhibsynsnCklqz:
	jmp	.L_last_blocks_done_pEjrionnahtrtzr
.L_last_num_blocks_is_11_pEjrionnahtrtzr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_pnezoypheClvrco
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_pnezoypheClvrco

.L_16_blocks_overflow_pnezoypheClvrco:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_pnezoypheClvrco:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GwuneybGhsGqGFj





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GwuneybGhsGqGFj
.L_small_initial_partial_block_GwuneybGhsGqGFj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GwuneybGhsGqGFj:

	orq	%r8,%r8
	je	.L_after_reduction_GwuneybGhsGqGFj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GwuneybGhsGqGFj:
	jmp	.L_last_blocks_done_pEjrionnahtrtzr
.L_last_num_blocks_is_12_pEjrionnahtrtzr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_CGptEdcFbxFGvCA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_CGptEdcFbxFGvCA

.L_16_blocks_overflow_CGptEdcFbxFGvCA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_CGptEdcFbxFGvCA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zjtknFGvmhEieGn





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zjtknFGvmhEieGn
.L_small_initial_partial_block_zjtknFGvmhEieGn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zjtknFGvmhEieGn:

	orq	%r8,%r8
	je	.L_after_reduction_zjtknFGvmhEieGn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zjtknFGvmhEieGn:
	jmp	.L_last_blocks_done_pEjrionnahtrtzr
.L_last_num_blocks_is_13_pEjrionnahtrtzr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_fGfgGpEnckrDxFn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_fGfgGpEnckrDxFn

.L_16_blocks_overflow_fGfgGpEnckrDxFn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_fGfgGpEnckrDxFn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BGwzDyBsCjGtpsn





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BGwzDyBsCjGtpsn
.L_small_initial_partial_block_BGwzDyBsCjGtpsn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BGwzDyBsCjGtpsn:

	orq	%r8,%r8
	je	.L_after_reduction_BGwzDyBsCjGtpsn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BGwzDyBsCjGtpsn:
	jmp	.L_last_blocks_done_pEjrionnahtrtzr
.L_last_num_blocks_is_14_pEjrionnahtrtzr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_fikznDfEnrdzwxA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_fikznDfEnrdzwxA

.L_16_blocks_overflow_fikznDfEnrdzwxA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_fikznDfEnrdzwxA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_orbwqmjgqqCzunk





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_orbwqmjgqqCzunk
.L_small_initial_partial_block_orbwqmjgqqCzunk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_orbwqmjgqqCzunk:

	orq	%r8,%r8
	je	.L_after_reduction_orbwqmjgqqCzunk
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_orbwqmjgqqCzunk:
	jmp	.L_last_blocks_done_pEjrionnahtrtzr
.L_last_num_blocks_is_15_pEjrionnahtrtzr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_coFhmqyjmikhukB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_coFhmqyjmikhukB

.L_16_blocks_overflow_coFhmqyjmikhukB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_coFhmqyjmikhukB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GAgbCenreqCCfdy





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GAgbCenreqCCfdy
.L_small_initial_partial_block_GAgbCenreqCCfdy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GAgbCenreqCCfdy:

	orq	%r8,%r8
	je	.L_after_reduction_GAgbCenreqCCfdy
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GAgbCenreqCCfdy:
	jmp	.L_last_blocks_done_pEjrionnahtrtzr
.L_last_num_blocks_is_16_pEjrionnahtrtzr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_oykqelFojDuCtnf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_oykqelFojDuCtnf

.L_16_blocks_overflow_oykqelFojDuCtnf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_oykqelFojDuCtnf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_dftFlGpkqfrrkhy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dftFlGpkqfrrkhy:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dftFlGpkqfrrkhy:
	jmp	.L_last_blocks_done_pEjrionnahtrtzr
.L_last_num_blocks_is_0_pEjrionnahtrtzr:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_pEjrionnahtrtzr:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_gbfeFpdczgiEidA

.L_message_below_32_blocks_gbfeFpdczgiEidA:


	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_szvAzrucppqDibp
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
.L_skip_hkeys_precomputation_szvAzrucppqDibp:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_BqaydoezjoaBrBk

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_BqaydoezjoaBrBk
	jb	.L_last_num_blocks_is_7_1_BqaydoezjoaBrBk


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_BqaydoezjoaBrBk
	jb	.L_last_num_blocks_is_11_9_BqaydoezjoaBrBk


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_BqaydoezjoaBrBk
	ja	.L_last_num_blocks_is_16_BqaydoezjoaBrBk
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_BqaydoezjoaBrBk
	jmp	.L_last_num_blocks_is_13_BqaydoezjoaBrBk

.L_last_num_blocks_is_11_9_BqaydoezjoaBrBk:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_BqaydoezjoaBrBk
	ja	.L_last_num_blocks_is_11_BqaydoezjoaBrBk
	jmp	.L_last_num_blocks_is_9_BqaydoezjoaBrBk

.L_last_num_blocks_is_7_1_BqaydoezjoaBrBk:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_BqaydoezjoaBrBk
	jb	.L_last_num_blocks_is_3_1_BqaydoezjoaBrBk

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_BqaydoezjoaBrBk
	je	.L_last_num_blocks_is_6_BqaydoezjoaBrBk
	jmp	.L_last_num_blocks_is_5_BqaydoezjoaBrBk

.L_last_num_blocks_is_3_1_BqaydoezjoaBrBk:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_BqaydoezjoaBrBk
	je	.L_last_num_blocks_is_2_BqaydoezjoaBrBk
.L_last_num_blocks_is_1_BqaydoezjoaBrBk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_coGfDmgEigipeDt
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_coGfDmgEigipeDt

.L_16_blocks_overflow_coGfDmgEigipeDt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_coGfDmgEigipeDt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fCGpoiuGGfvCzev





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fCGpoiuGGfvCzev
.L_small_initial_partial_block_fCGpoiuGGfvCzev:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_fCGpoiuGGfvCzev
.L_small_initial_compute_done_fCGpoiuGGfvCzev:
.L_after_reduction_fCGpoiuGGfvCzev:
	jmp	.L_last_blocks_done_BqaydoezjoaBrBk
.L_last_num_blocks_is_2_BqaydoezjoaBrBk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_soxmbFzhjBAldGh
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_soxmbFzhjBAldGh

.L_16_blocks_overflow_soxmbFzhjBAldGh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_soxmbFzhjBAldGh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xkfpitrxEmeADhp





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xkfpitrxEmeADhp
.L_small_initial_partial_block_xkfpitrxEmeADhp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xkfpitrxEmeADhp:

	orq	%r8,%r8
	je	.L_after_reduction_xkfpitrxEmeADhp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xkfpitrxEmeADhp:
	jmp	.L_last_blocks_done_BqaydoezjoaBrBk
.L_last_num_blocks_is_3_BqaydoezjoaBrBk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_sCoukhtdkcnovEy
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_sCoukhtdkcnovEy

.L_16_blocks_overflow_sCoukhtdkcnovEy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_sCoukhtdkcnovEy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gemcnhdtFzClwww





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gemcnhdtFzClwww
.L_small_initial_partial_block_gemcnhdtFzClwww:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gemcnhdtFzClwww:

	orq	%r8,%r8
	je	.L_after_reduction_gemcnhdtFzClwww
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gemcnhdtFzClwww:
	jmp	.L_last_blocks_done_BqaydoezjoaBrBk
.L_last_num_blocks_is_4_BqaydoezjoaBrBk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_nhDFeqlpdjaxgvn
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_nhDFeqlpdjaxgvn

.L_16_blocks_overflow_nhDFeqlpdjaxgvn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_nhDFeqlpdjaxgvn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zqcclEuoAmyGcGb





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zqcclEuoAmyGcGb
.L_small_initial_partial_block_zqcclEuoAmyGcGb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zqcclEuoAmyGcGb:

	orq	%r8,%r8
	je	.L_after_reduction_zqcclEuoAmyGcGb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zqcclEuoAmyGcGb:
	jmp	.L_last_blocks_done_BqaydoezjoaBrBk
.L_last_num_blocks_is_5_BqaydoezjoaBrBk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_uBlvClwwFsDrthD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_uBlvClwwFsDrthD

.L_16_blocks_overflow_uBlvClwwFsDrthD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_uBlvClwwFsDrthD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ukogzpvuwdceACF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ukogzpvuwdceACF
.L_small_initial_partial_block_ukogzpvuwdceACF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ukogzpvuwdceACF:

	orq	%r8,%r8
	je	.L_after_reduction_ukogzpvuwdceACF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ukogzpvuwdceACF:
	jmp	.L_last_blocks_done_BqaydoezjoaBrBk
.L_last_num_blocks_is_6_BqaydoezjoaBrBk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_FzaktrnkfeGhugd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_FzaktrnkfeGhugd

.L_16_blocks_overflow_FzaktrnkfeGhugd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_FzaktrnkfeGhugd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_coGtFGmwqdDbfbu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_coGtFGmwqdDbfbu
.L_small_initial_partial_block_coGtFGmwqdDbfbu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_coGtFGmwqdDbfbu:

	orq	%r8,%r8
	je	.L_after_reduction_coGtFGmwqdDbfbu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_coGtFGmwqdDbfbu:
	jmp	.L_last_blocks_done_BqaydoezjoaBrBk
.L_last_num_blocks_is_7_BqaydoezjoaBrBk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_uaebxCfGeqyyBrv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_uaebxCfGeqyyBrv

.L_16_blocks_overflow_uaebxCfGeqyyBrv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_uaebxCfGeqyyBrv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CDpxxtlyBGEwtjF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CDpxxtlyBGEwtjF
.L_small_initial_partial_block_CDpxxtlyBGEwtjF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CDpxxtlyBGEwtjF:

	orq	%r8,%r8
	je	.L_after_reduction_CDpxxtlyBGEwtjF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CDpxxtlyBGEwtjF:
	jmp	.L_last_blocks_done_BqaydoezjoaBrBk
.L_last_num_blocks_is_8_BqaydoezjoaBrBk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_wnBlylzqDGqdrBC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_wnBlylzqDGqdrBC

.L_16_blocks_overflow_wnBlylzqDGqdrBC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_wnBlylzqDGqdrBC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_arwxrdvDrafoqpb





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_arwxrdvDrafoqpb
.L_small_initial_partial_block_arwxrdvDrafoqpb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_arwxrdvDrafoqpb:

	orq	%r8,%r8
	je	.L_after_reduction_arwxrdvDrafoqpb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_arwxrdvDrafoqpb:
	jmp	.L_last_blocks_done_BqaydoezjoaBrBk
.L_last_num_blocks_is_9_BqaydoezjoaBrBk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_erkwzqpzyECteed
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_erkwzqpzyECteed

.L_16_blocks_overflow_erkwzqpzyECteed:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_erkwzqpzyECteed:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vBFnozutEfavzij





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vBFnozutEfavzij
.L_small_initial_partial_block_vBFnozutEfavzij:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vBFnozutEfavzij:

	orq	%r8,%r8
	je	.L_after_reduction_vBFnozutEfavzij
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vBFnozutEfavzij:
	jmp	.L_last_blocks_done_BqaydoezjoaBrBk
.L_last_num_blocks_is_10_BqaydoezjoaBrBk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_ornyEcAvswrBxqA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_ornyEcAvswrBxqA

.L_16_blocks_overflow_ornyEcAvswrBxqA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_ornyEcAvswrBxqA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_oGivkqeexBuEggp





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_oGivkqeexBuEggp
.L_small_initial_partial_block_oGivkqeexBuEggp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_oGivkqeexBuEggp:

	orq	%r8,%r8
	je	.L_after_reduction_oGivkqeexBuEggp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_oGivkqeexBuEggp:
	jmp	.L_last_blocks_done_BqaydoezjoaBrBk
.L_last_num_blocks_is_11_BqaydoezjoaBrBk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_eGzdvgGxgGczECz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_eGzdvgGxgGczECz

.L_16_blocks_overflow_eGzdvgGxgGczECz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_eGzdvgGxgGczECz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_mrzidpArmtyCAfa





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_mrzidpArmtyCAfa
.L_small_initial_partial_block_mrzidpArmtyCAfa:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mrzidpArmtyCAfa:

	orq	%r8,%r8
	je	.L_after_reduction_mrzidpArmtyCAfa
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_mrzidpArmtyCAfa:
	jmp	.L_last_blocks_done_BqaydoezjoaBrBk
.L_last_num_blocks_is_12_BqaydoezjoaBrBk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_AGAjzdkheCbmiwy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_AGAjzdkheCbmiwy

.L_16_blocks_overflow_AGAjzdkheCbmiwy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_AGAjzdkheCbmiwy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xiisutEBFogjEdq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xiisutEBFogjEdq
.L_small_initial_partial_block_xiisutEBFogjEdq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xiisutEBFogjEdq:

	orq	%r8,%r8
	je	.L_after_reduction_xiisutEBFogjEdq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xiisutEBFogjEdq:
	jmp	.L_last_blocks_done_BqaydoezjoaBrBk
.L_last_num_blocks_is_13_BqaydoezjoaBrBk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_FoBvBbivCliCDpG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_FoBvBbivCliCDpG

.L_16_blocks_overflow_FoBvBbivCliCDpG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_FoBvBbivCliCDpG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GpwfCkdorcdvvCm





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GpwfCkdorcdvvCm
.L_small_initial_partial_block_GpwfCkdorcdvvCm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GpwfCkdorcdvvCm:

	orq	%r8,%r8
	je	.L_after_reduction_GpwfCkdorcdvvCm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GpwfCkdorcdvvCm:
	jmp	.L_last_blocks_done_BqaydoezjoaBrBk
.L_last_num_blocks_is_14_BqaydoezjoaBrBk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_oatsaoqhnCGyDwe
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_oatsaoqhnCGyDwe

.L_16_blocks_overflow_oatsaoqhnCGyDwe:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_oatsaoqhnCGyDwe:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AfvCodfdxgCjpzF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AfvCodfdxgCjpzF
.L_small_initial_partial_block_AfvCodfdxgCjpzF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AfvCodfdxgCjpzF:

	orq	%r8,%r8
	je	.L_after_reduction_AfvCodfdxgCjpzF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_AfvCodfdxgCjpzF:
	jmp	.L_last_blocks_done_BqaydoezjoaBrBk
.L_last_num_blocks_is_15_BqaydoezjoaBrBk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_EpanbcqaElBxxaB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_EpanbcqaElBxxaB

.L_16_blocks_overflow_EpanbcqaElBxxaB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_EpanbcqaElBxxaB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xiqnCtrfuGDaCEE





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xiqnCtrfuGDaCEE
.L_small_initial_partial_block_xiqnCtrfuGDaCEE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xiqnCtrfuGDaCEE:

	orq	%r8,%r8
	je	.L_after_reduction_xiqnCtrfuGDaCEE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xiqnCtrfuGDaCEE:
	jmp	.L_last_blocks_done_BqaydoezjoaBrBk
.L_last_num_blocks_is_16_BqaydoezjoaBrBk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_fkklbfEtermsrht
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_fkklbfEtermsrht

.L_16_blocks_overflow_fkklbfEtermsrht:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_fkklbfEtermsrht:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_jFACbwlasszlvhv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jFACbwlasszlvhv:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jFACbwlasszlvhv:
	jmp	.L_last_blocks_done_BqaydoezjoaBrBk
.L_last_num_blocks_is_0_BqaydoezjoaBrBk:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_BqaydoezjoaBrBk:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_gbfeFpdczgiEidA

.L_message_below_equal_16_blocks_gbfeFpdczgiEidA:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	.L_small_initial_num_blocks_is_8_iarmDqgbmlDDsuD
	jl	.L_small_initial_num_blocks_is_7_1_iarmDqgbmlDDsuD


	cmpq	$12,%r12
	je	.L_small_initial_num_blocks_is_12_iarmDqgbmlDDsuD
	jl	.L_small_initial_num_blocks_is_11_9_iarmDqgbmlDDsuD


	cmpq	$16,%r12
	je	.L_small_initial_num_blocks_is_16_iarmDqgbmlDDsuD
	cmpq	$15,%r12
	je	.L_small_initial_num_blocks_is_15_iarmDqgbmlDDsuD
	cmpq	$14,%r12
	je	.L_small_initial_num_blocks_is_14_iarmDqgbmlDDsuD
	jmp	.L_small_initial_num_blocks_is_13_iarmDqgbmlDDsuD

.L_small_initial_num_blocks_is_11_9_iarmDqgbmlDDsuD:

	cmpq	$11,%r12
	je	.L_small_initial_num_blocks_is_11_iarmDqgbmlDDsuD
	cmpq	$10,%r12
	je	.L_small_initial_num_blocks_is_10_iarmDqgbmlDDsuD
	jmp	.L_small_initial_num_blocks_is_9_iarmDqgbmlDDsuD

.L_small_initial_num_blocks_is_7_1_iarmDqgbmlDDsuD:
	cmpq	$4,%r12
	je	.L_small_initial_num_blocks_is_4_iarmDqgbmlDDsuD
	jl	.L_small_initial_num_blocks_is_3_1_iarmDqgbmlDDsuD

	cmpq	$7,%r12
	je	.L_small_initial_num_blocks_is_7_iarmDqgbmlDDsuD
	cmpq	$6,%r12
	je	.L_small_initial_num_blocks_is_6_iarmDqgbmlDDsuD
	jmp	.L_small_initial_num_blocks_is_5_iarmDqgbmlDDsuD

.L_small_initial_num_blocks_is_3_1_iarmDqgbmlDDsuD:

	cmpq	$3,%r12
	je	.L_small_initial_num_blocks_is_3_iarmDqgbmlDDsuD
	cmpq	$2,%r12
	je	.L_small_initial_num_blocks_is_2_iarmDqgbmlDDsuD





.L_small_initial_num_blocks_is_1_iarmDqgbmlDDsuD:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%r11,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm6,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zulqFloEhlqdqrG





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zulqFloEhlqdqrG
.L_small_initial_partial_block_zulqFloEhlqdqrG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	.L_after_reduction_zulqFloEhlqdqrG
.L_small_initial_compute_done_zulqFloEhlqdqrG:
.L_after_reduction_zulqFloEhlqdqrG:
	jmp	.L_small_initial_blocks_encrypted_iarmDqgbmlDDsuD
.L_small_initial_num_blocks_is_2_iarmDqgbmlDDsuD:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%r11,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm6,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xvwClfymbdaxsiq





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xvwClfymbdaxsiq
.L_small_initial_partial_block_xvwClfymbdaxsiq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xvwClfymbdaxsiq:

	orq	%r8,%r8
	je	.L_after_reduction_xvwClfymbdaxsiq
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_xvwClfymbdaxsiq:
	jmp	.L_small_initial_blocks_encrypted_iarmDqgbmlDDsuD
.L_small_initial_num_blocks_is_3_iarmDqgbmlDDsuD:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_koCvcohAudepuxs





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_koCvcohAudepuxs
.L_small_initial_partial_block_koCvcohAudepuxs:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_koCvcohAudepuxs:

	orq	%r8,%r8
	je	.L_after_reduction_koCvcohAudepuxs
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_koCvcohAudepuxs:
	jmp	.L_small_initial_blocks_encrypted_iarmDqgbmlDDsuD
.L_small_initial_num_blocks_is_4_iarmDqgbmlDDsuD:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_oiyzmczFbFdasdf





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_oiyzmczFbFdasdf
.L_small_initial_partial_block_oiyzmczFbFdasdf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_oiyzmczFbFdasdf:

	orq	%r8,%r8
	je	.L_after_reduction_oiyzmczFbFdasdf
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_oiyzmczFbFdasdf:
	jmp	.L_small_initial_blocks_encrypted_iarmDqgbmlDDsuD
.L_small_initial_num_blocks_is_5_iarmDqgbmlDDsuD:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%xmm29,%xmm7,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xcsBhifdafcFDjl





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xcsBhifdafcFDjl
.L_small_initial_partial_block_xcsBhifdafcFDjl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xcsBhifdafcFDjl:

	orq	%r8,%r8
	je	.L_after_reduction_xcsBhifdafcFDjl
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_xcsBhifdafcFDjl:
	jmp	.L_small_initial_blocks_encrypted_iarmDqgbmlDDsuD
.L_small_initial_num_blocks_is_6_iarmDqgbmlDDsuD:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%ymm29,%ymm7,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zDCtqwineppnblk





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zDCtqwineppnblk
.L_small_initial_partial_block_zDCtqwineppnblk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zDCtqwineppnblk:

	orq	%r8,%r8
	je	.L_after_reduction_zDCtqwineppnblk
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_zDCtqwineppnblk:
	jmp	.L_small_initial_blocks_encrypted_iarmDqgbmlDDsuD
.L_small_initial_num_blocks_is_7_iarmDqgbmlDDsuD:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vemihrbBakzwGFu





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vemihrbBakzwGFu
.L_small_initial_partial_block_vemihrbBakzwGFu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vemihrbBakzwGFu:

	orq	%r8,%r8
	je	.L_after_reduction_vemihrbBakzwGFu
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_vemihrbBakzwGFu:
	jmp	.L_small_initial_blocks_encrypted_iarmDqgbmlDDsuD
.L_small_initial_num_blocks_is_8_iarmDqgbmlDDsuD:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AclfGCwebAszczk





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AclfGCwebAszczk
.L_small_initial_partial_block_AclfGCwebAszczk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AclfGCwebAszczk:

	orq	%r8,%r8
	je	.L_after_reduction_AclfGCwebAszczk
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_AclfGCwebAszczk:
	jmp	.L_small_initial_blocks_encrypted_iarmDqgbmlDDsuD
.L_small_initial_num_blocks_is_9_iarmDqgbmlDDsuD:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%xmm29,%xmm10,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_mvjdrawuCzyocxk





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_mvjdrawuCzyocxk
.L_small_initial_partial_block_mvjdrawuCzyocxk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mvjdrawuCzyocxk:

	orq	%r8,%r8
	je	.L_after_reduction_mvjdrawuCzyocxk
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_mvjdrawuCzyocxk:
	jmp	.L_small_initial_blocks_encrypted_iarmDqgbmlDDsuD
.L_small_initial_num_blocks_is_10_iarmDqgbmlDDsuD:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%ymm29,%ymm10,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_sgyslesFioxazaD





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_sgyslesFioxazaD
.L_small_initial_partial_block_sgyslesFioxazaD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_sgyslesFioxazaD:

	orq	%r8,%r8
	je	.L_after_reduction_sgyslesFioxazaD
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_sgyslesFioxazaD:
	jmp	.L_small_initial_blocks_encrypted_iarmDqgbmlDDsuD
.L_small_initial_num_blocks_is_11_iarmDqgbmlDDsuD:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zcyfydfCyrlpwbb





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zcyfydfCyrlpwbb
.L_small_initial_partial_block_zcyfydfCyrlpwbb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zcyfydfCyrlpwbb:

	orq	%r8,%r8
	je	.L_after_reduction_zcyfydfCyrlpwbb
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_zcyfydfCyrlpwbb:
	jmp	.L_small_initial_blocks_encrypted_iarmDqgbmlDDsuD
.L_small_initial_num_blocks_is_12_iarmDqgbmlDDsuD:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wrfjCpofhzqCcqy





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wrfjCpofhzqCcqy
.L_small_initial_partial_block_wrfjCpofhzqCcqy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wrfjCpofhzqCcqy:

	orq	%r8,%r8
	je	.L_after_reduction_wrfjCpofhzqCcqy
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_wrfjCpofhzqCcqy:
	jmp	.L_small_initial_blocks_encrypted_iarmDqgbmlDDsuD
.L_small_initial_num_blocks_is_13_iarmDqgbmlDDsuD:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%xmm29,%xmm11,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ozlmrGxEyguvCkz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ozlmrGxEyguvCkz
.L_small_initial_partial_block_ozlmrGxEyguvCkz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ozlmrGxEyguvCkz:

	orq	%r8,%r8
	je	.L_after_reduction_ozlmrGxEyguvCkz
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_ozlmrGxEyguvCkz:
	jmp	.L_small_initial_blocks_encrypted_iarmDqgbmlDDsuD
.L_small_initial_num_blocks_is_14_iarmDqgbmlDDsuD:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%ymm29,%ymm11,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ukxioAArepwdfer





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ukxioAArepwdfer
.L_small_initial_partial_block_ukxioAArepwdfer:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ukxioAArepwdfer:

	orq	%r8,%r8
	je	.L_after_reduction_ukxioAArepwdfer
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_ukxioAArepwdfer:
	jmp	.L_small_initial_blocks_encrypted_iarmDqgbmlDDsuD
.L_small_initial_num_blocks_is_15_iarmDqgbmlDDsuD:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EmpgACnranfbjGz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EmpgACnranfbjGz
.L_small_initial_partial_block_EmpgACnranfbjGz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EmpgACnranfbjGz:

	orq	%r8,%r8
	je	.L_after_reduction_EmpgACnranfbjGz
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_EmpgACnranfbjGz:
	jmp	.L_small_initial_blocks_encrypted_iarmDqgbmlDDsuD
.L_small_initial_num_blocks_is_16_iarmDqgbmlDDsuD:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_CnsFbuyurzsCvoE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CnsFbuyurzsCvoE:
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_CnsFbuyurzsCvoE:
.L_small_initial_blocks_encrypted_iarmDqgbmlDDsuD:
.L_ghash_done_gbfeFpdczgiEidA:
	vmovdqu64	%xmm2,0(%rsi)
	vmovdqu64	%xmm14,64(%rsi)
.L_enc_dec_done_gbfeFpdczgiEidA:
	jmp	.Lexit_gcm_decrypt
.align	32
.Laes_gcm_decrypt_192_avx512:
	orq	%r8,%r8
	je	.L_enc_dec_done_ioflBkhvfsafcEu
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14

	movq	(%rdx),%r11
	orq	%r11,%r11
	je	.L_partial_block_done_AAusuoeoiGpjcDa
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3
	vmovdqu64	336(%rsi),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%r11,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3

	vmovdqa64	%xmm0,%xmm6
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%r11,1),%r13
	subq	$16,%r13
	jge	.L_no_extra_mask_AAusuoeoiGpjcDa
	subq	%r13,%r12
.L_no_extra_mask_AAusuoeoiGpjcDa:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpand	%xmm0,%xmm6,%xmm6
	vpshufb	SHUF_MASK(%rip),%xmm6,%xmm6
	vpshufb	%xmm5,%xmm6,%xmm6
	vpxorq	%xmm6,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	.L_partial_incomplete_AAusuoeoiGpjcDa

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movq	$0,(%rdx)

	movq	%r11,%r12
	movq	$16,%r11
	subq	%r12,%r11
	jmp	.L_enc_dec_done_AAusuoeoiGpjcDa

.L_partial_incomplete_AAusuoeoiGpjcDa:
	addq	%r8,(%rdx)
	movq	%r8,%r11

.L_enc_dec_done_AAusuoeoiGpjcDa:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%r11,2),%k1
	vmovdqu64	%xmm14,64(%rsi)
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
.L_partial_block_done_AAusuoeoiGpjcDa:
	vmovdqu64	0(%rsi),%xmm2
	subq	%r11,%r8
	je	.L_enc_dec_done_ioflBkhvfsafcEu
	cmpq	$256,%r8
	jbe	.L_message_below_equal_16_blocks_ioflBkhvfsafcEu

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_CGCezxEuzpbzagk
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_CGCezxEuzpbzagk
.L_next_16_overflow_CGCezxEuzpbzagk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_CGCezxEuzpbzagk:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%r11,1),%zmm0
	vmovdqu8	64(%rcx,%r11,1),%zmm3
	vmovdqu8	128(%rcx,%r11,1),%zmm4
	vmovdqu8	192(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%r11,1)
	vmovdqu8	%zmm10,64(%r10,%r11,1)
	vmovdqu8	%zmm11,128(%r10,%r11,1)
	vmovdqu8	%zmm12,192(%r10,%r11,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_FdbCmzlgFeAighF

	vmovdqu64	288(%rsi),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	224(%rsi),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	160(%rsi),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	96(%rsi),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
.L_skip_hkeys_precomputation_FdbCmzlgFeAighF:
	cmpq	$512,%r8
	jb	.L_message_below_32_blocks_ioflBkhvfsafcEu



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_svqEivwEofgryqC
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_svqEivwEofgryqC
.L_next_16_overflow_svqEivwEofgryqC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_svqEivwEofgryqC:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%r11,1),%zmm0
	vmovdqu8	320(%rcx,%r11,1),%zmm3
	vmovdqu8	384(%rcx,%r11,1),%zmm4
	vmovdqu8	448(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%r11,1)
	vmovdqu8	%zmm10,320(%r10,%r11,1)
	vmovdqu8	%zmm11,384(%r10,%r11,1)
	vmovdqu8	%zmm12,448(%r10,%r11,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_AyBkEaEAdmqpcGz
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
.L_skip_hkeys_precomputation_AyBkEaEAdmqpcGz:
	movq	$1,%r14
	addq	$512,%r11
	subq	$512,%r8

	cmpq	$768,%r8
	jb	.L_no_more_big_nblocks_ioflBkhvfsafcEu
.L_encrypt_big_nblocks_ioflBkhvfsafcEu:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_gErmizFqBnwqGcg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_gErmizFqBnwqGcg
.L_16_blocks_overflow_gErmizFqBnwqGcg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_gErmizFqBnwqGcg:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_FougpGkegeagxas
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_FougpGkegeagxas
.L_16_blocks_overflow_FougpGkegeagxas:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_FougpGkegeagxas:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_uycoBwgeAzAqtCk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_uycoBwgeAzAqtCk
.L_16_blocks_overflow_uycoBwgeAzAqtCk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_uycoBwgeAzAqtCk:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%r11,1),%zmm17
	vmovdqu8	576(%rcx,%r11,1),%zmm19
	vmovdqu8	640(%rcx,%r11,1),%zmm20
	vmovdqu8	704(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%r11,1)
	vmovdqu8	%zmm3,576(%r10,%r11,1)
	vmovdqu8	%zmm4,640(%r10,%r11,1)
	vmovdqu8	%zmm5,704(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%r11
	subq	$768,%r8
	cmpq	$768,%r8
	jae	.L_encrypt_big_nblocks_ioflBkhvfsafcEu

.L_no_more_big_nblocks_ioflBkhvfsafcEu:

	cmpq	$512,%r8
	jae	.L_encrypt_32_blocks_ioflBkhvfsafcEu

	cmpq	$256,%r8
	jae	.L_encrypt_16_blocks_ioflBkhvfsafcEu
.L_encrypt_0_blocks_ghash_32_ioflBkhvfsafcEu:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_GDAswCozihfcrjw

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_GDAswCozihfcrjw
	jb	.L_last_num_blocks_is_7_1_GDAswCozihfcrjw


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_GDAswCozihfcrjw
	jb	.L_last_num_blocks_is_11_9_GDAswCozihfcrjw


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_GDAswCozihfcrjw
	ja	.L_last_num_blocks_is_16_GDAswCozihfcrjw
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_GDAswCozihfcrjw
	jmp	.L_last_num_blocks_is_13_GDAswCozihfcrjw

.L_last_num_blocks_is_11_9_GDAswCozihfcrjw:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_GDAswCozihfcrjw
	ja	.L_last_num_blocks_is_11_GDAswCozihfcrjw
	jmp	.L_last_num_blocks_is_9_GDAswCozihfcrjw

.L_last_num_blocks_is_7_1_GDAswCozihfcrjw:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_GDAswCozihfcrjw
	jb	.L_last_num_blocks_is_3_1_GDAswCozihfcrjw

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_GDAswCozihfcrjw
	je	.L_last_num_blocks_is_6_GDAswCozihfcrjw
	jmp	.L_last_num_blocks_is_5_GDAswCozihfcrjw

.L_last_num_blocks_is_3_1_GDAswCozihfcrjw:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_GDAswCozihfcrjw
	je	.L_last_num_blocks_is_2_GDAswCozihfcrjw
.L_last_num_blocks_is_1_GDAswCozihfcrjw:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_vdrtadeoaBwkjik
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_vdrtadeoaBwkjik

.L_16_blocks_overflow_vdrtadeoaBwkjik:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_vdrtadeoaBwkjik:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_eGbbdsCyxqgFEtD





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_eGbbdsCyxqgFEtD
.L_small_initial_partial_block_eGbbdsCyxqgFEtD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_eGbbdsCyxqgFEtD
.L_small_initial_compute_done_eGbbdsCyxqgFEtD:
.L_after_reduction_eGbbdsCyxqgFEtD:
	jmp	.L_last_blocks_done_GDAswCozihfcrjw
.L_last_num_blocks_is_2_GDAswCozihfcrjw:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_ebjzrEDzjrgkxlx
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_ebjzrEDzjrgkxlx

.L_16_blocks_overflow_ebjzrEDzjrgkxlx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_ebjzrEDzjrgkxlx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lojsdlqsemqspoo





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lojsdlqsemqspoo
.L_small_initial_partial_block_lojsdlqsemqspoo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lojsdlqsemqspoo:

	orq	%r8,%r8
	je	.L_after_reduction_lojsdlqsemqspoo
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lojsdlqsemqspoo:
	jmp	.L_last_blocks_done_GDAswCozihfcrjw
.L_last_num_blocks_is_3_GDAswCozihfcrjw:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_FkCFezBjfwBpoAg
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_FkCFezBjfwBpoAg

.L_16_blocks_overflow_FkCFezBjfwBpoAg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_FkCFezBjfwBpoAg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dyEFnkhoFflathm





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dyEFnkhoFflathm
.L_small_initial_partial_block_dyEFnkhoFflathm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dyEFnkhoFflathm:

	orq	%r8,%r8
	je	.L_after_reduction_dyEFnkhoFflathm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dyEFnkhoFflathm:
	jmp	.L_last_blocks_done_GDAswCozihfcrjw
.L_last_num_blocks_is_4_GDAswCozihfcrjw:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_nzaGpCcnlsEmmzp
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_nzaGpCcnlsEmmzp

.L_16_blocks_overflow_nzaGpCcnlsEmmzp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_nzaGpCcnlsEmmzp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dzbyordoibxyDAi





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dzbyordoibxyDAi
.L_small_initial_partial_block_dzbyordoibxyDAi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dzbyordoibxyDAi:

	orq	%r8,%r8
	je	.L_after_reduction_dzbyordoibxyDAi
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dzbyordoibxyDAi:
	jmp	.L_last_blocks_done_GDAswCozihfcrjw
.L_last_num_blocks_is_5_GDAswCozihfcrjw:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_pwCwktvhhbvEmtg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_pwCwktvhhbvEmtg

.L_16_blocks_overflow_pwCwktvhhbvEmtg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_pwCwktvhhbvEmtg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_smevAhyCBgvFzup





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_smevAhyCBgvFzup
.L_small_initial_partial_block_smevAhyCBgvFzup:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_smevAhyCBgvFzup:

	orq	%r8,%r8
	je	.L_after_reduction_smevAhyCBgvFzup
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_smevAhyCBgvFzup:
	jmp	.L_last_blocks_done_GDAswCozihfcrjw
.L_last_num_blocks_is_6_GDAswCozihfcrjw:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_jarptAwepFeneqy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_jarptAwepFeneqy

.L_16_blocks_overflow_jarptAwepFeneqy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_jarptAwepFeneqy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GuvmgDEjBjmaalm





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GuvmgDEjBjmaalm
.L_small_initial_partial_block_GuvmgDEjBjmaalm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GuvmgDEjBjmaalm:

	orq	%r8,%r8
	je	.L_after_reduction_GuvmgDEjBjmaalm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GuvmgDEjBjmaalm:
	jmp	.L_last_blocks_done_GDAswCozihfcrjw
.L_last_num_blocks_is_7_GDAswCozihfcrjw:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_mioadyqpBBasAyz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_mioadyqpBBasAyz

.L_16_blocks_overflow_mioadyqpBBasAyz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_mioadyqpBBasAyz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_smFqcwBvnaDjcAh





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_smFqcwBvnaDjcAh
.L_small_initial_partial_block_smFqcwBvnaDjcAh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_smFqcwBvnaDjcAh:

	orq	%r8,%r8
	je	.L_after_reduction_smFqcwBvnaDjcAh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_smFqcwBvnaDjcAh:
	jmp	.L_last_blocks_done_GDAswCozihfcrjw
.L_last_num_blocks_is_8_GDAswCozihfcrjw:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_kqidmFlcFrtqajo
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_kqidmFlcFrtqajo

.L_16_blocks_overflow_kqidmFlcFrtqajo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_kqidmFlcFrtqajo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bgniEDcisyxFliu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bgniEDcisyxFliu
.L_small_initial_partial_block_bgniEDcisyxFliu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bgniEDcisyxFliu:

	orq	%r8,%r8
	je	.L_after_reduction_bgniEDcisyxFliu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bgniEDcisyxFliu:
	jmp	.L_last_blocks_done_GDAswCozihfcrjw
.L_last_num_blocks_is_9_GDAswCozihfcrjw:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_eBuDytaFbatmxba
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_eBuDytaFbatmxba

.L_16_blocks_overflow_eBuDytaFbatmxba:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_eBuDytaFbatmxba:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CeqqAGbiktfDGiy





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CeqqAGbiktfDGiy
.L_small_initial_partial_block_CeqqAGbiktfDGiy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CeqqAGbiktfDGiy:

	orq	%r8,%r8
	je	.L_after_reduction_CeqqAGbiktfDGiy
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CeqqAGbiktfDGiy:
	jmp	.L_last_blocks_done_GDAswCozihfcrjw
.L_last_num_blocks_is_10_GDAswCozihfcrjw:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_nkqCkhbAjDjFjwu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_nkqCkhbAjDjFjwu

.L_16_blocks_overflow_nkqCkhbAjDjFjwu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_nkqCkhbAjDjFjwu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iBidbEbBjiCcDqn





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iBidbEbBjiCcDqn
.L_small_initial_partial_block_iBidbEbBjiCcDqn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iBidbEbBjiCcDqn:

	orq	%r8,%r8
	je	.L_after_reduction_iBidbEbBjiCcDqn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_iBidbEbBjiCcDqn:
	jmp	.L_last_blocks_done_GDAswCozihfcrjw
.L_last_num_blocks_is_11_GDAswCozihfcrjw:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_gzDDzxeeEximCds
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_gzDDzxeeEximCds

.L_16_blocks_overflow_gzDDzxeeEximCds:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_gzDDzxeeEximCds:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jxGtGFxwvnGFjxb





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jxGtGFxwvnGFjxb
.L_small_initial_partial_block_jxGtGFxwvnGFjxb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jxGtGFxwvnGFjxb:

	orq	%r8,%r8
	je	.L_after_reduction_jxGtGFxwvnGFjxb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jxGtGFxwvnGFjxb:
	jmp	.L_last_blocks_done_GDAswCozihfcrjw
.L_last_num_blocks_is_12_GDAswCozihfcrjw:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_cFjozdnAqzlcFvf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_cFjozdnAqzlcFvf

.L_16_blocks_overflow_cFjozdnAqzlcFvf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_cFjozdnAqzlcFvf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rmgsoBpspvwuvvm





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rmgsoBpspvwuvvm
.L_small_initial_partial_block_rmgsoBpspvwuvvm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rmgsoBpspvwuvvm:

	orq	%r8,%r8
	je	.L_after_reduction_rmgsoBpspvwuvvm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rmgsoBpspvwuvvm:
	jmp	.L_last_blocks_done_GDAswCozihfcrjw
.L_last_num_blocks_is_13_GDAswCozihfcrjw:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_juagAEmlFbeAudC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_juagAEmlFbeAudC

.L_16_blocks_overflow_juagAEmlFbeAudC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_juagAEmlFbeAudC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EDGaeqBhEzyzuAh





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EDGaeqBhEzyzuAh
.L_small_initial_partial_block_EDGaeqBhEzyzuAh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EDGaeqBhEzyzuAh:

	orq	%r8,%r8
	je	.L_after_reduction_EDGaeqBhEzyzuAh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_EDGaeqBhEzyzuAh:
	jmp	.L_last_blocks_done_GDAswCozihfcrjw
.L_last_num_blocks_is_14_GDAswCozihfcrjw:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_cdElesvBxrlfsuc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_cdElesvBxrlfsuc

.L_16_blocks_overflow_cdElesvBxrlfsuc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_cdElesvBxrlfsuc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BmnnxpvqgunFnFD





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BmnnxpvqgunFnFD
.L_small_initial_partial_block_BmnnxpvqgunFnFD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BmnnxpvqgunFnFD:

	orq	%r8,%r8
	je	.L_after_reduction_BmnnxpvqgunFnFD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BmnnxpvqgunFnFD:
	jmp	.L_last_blocks_done_GDAswCozihfcrjw
.L_last_num_blocks_is_15_GDAswCozihfcrjw:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_oBgidyBtGhGBrDb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_oBgidyBtGhGBrDb

.L_16_blocks_overflow_oBgidyBtGhGBrDb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_oBgidyBtGhGBrDb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ErDygymfybAqjjq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ErDygymfybAqjjq
.L_small_initial_partial_block_ErDygymfybAqjjq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ErDygymfybAqjjq:

	orq	%r8,%r8
	je	.L_after_reduction_ErDygymfybAqjjq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ErDygymfybAqjjq:
	jmp	.L_last_blocks_done_GDAswCozihfcrjw
.L_last_num_blocks_is_16_GDAswCozihfcrjw:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_hbbzqfCsrubuipB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_hbbzqfCsrubuipB

.L_16_blocks_overflow_hbbzqfCsrubuipB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_hbbzqfCsrubuipB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_BtFchawexyypisz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BtFchawexyypisz:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BtFchawexyypisz:
	jmp	.L_last_blocks_done_GDAswCozihfcrjw
.L_last_num_blocks_is_0_GDAswCozihfcrjw:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_GDAswCozihfcrjw:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_ioflBkhvfsafcEu
.L_encrypt_32_blocks_ioflBkhvfsafcEu:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_ziknratlDiGwsni
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_ziknratlDiGwsni
.L_16_blocks_overflow_ziknratlDiGwsni:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_ziknratlDiGwsni:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_Bsnesyhqwplpcnp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_Bsnesyhqwplpcnp
.L_16_blocks_overflow_Bsnesyhqwplpcnp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_Bsnesyhqwplpcnp:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%r11
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_geifcuAbzFlAapy

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_geifcuAbzFlAapy
	jb	.L_last_num_blocks_is_7_1_geifcuAbzFlAapy


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_geifcuAbzFlAapy
	jb	.L_last_num_blocks_is_11_9_geifcuAbzFlAapy


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_geifcuAbzFlAapy
	ja	.L_last_num_blocks_is_16_geifcuAbzFlAapy
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_geifcuAbzFlAapy
	jmp	.L_last_num_blocks_is_13_geifcuAbzFlAapy

.L_last_num_blocks_is_11_9_geifcuAbzFlAapy:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_geifcuAbzFlAapy
	ja	.L_last_num_blocks_is_11_geifcuAbzFlAapy
	jmp	.L_last_num_blocks_is_9_geifcuAbzFlAapy

.L_last_num_blocks_is_7_1_geifcuAbzFlAapy:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_geifcuAbzFlAapy
	jb	.L_last_num_blocks_is_3_1_geifcuAbzFlAapy

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_geifcuAbzFlAapy
	je	.L_last_num_blocks_is_6_geifcuAbzFlAapy
	jmp	.L_last_num_blocks_is_5_geifcuAbzFlAapy

.L_last_num_blocks_is_3_1_geifcuAbzFlAapy:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_geifcuAbzFlAapy
	je	.L_last_num_blocks_is_2_geifcuAbzFlAapy
.L_last_num_blocks_is_1_geifcuAbzFlAapy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_vferoFhvvjdenBc
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_vferoFhvvjdenBc

.L_16_blocks_overflow_vferoFhvvjdenBc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_vferoFhvvjdenBc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nqxzBcvnzdthDuy





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nqxzBcvnzdthDuy
.L_small_initial_partial_block_nqxzBcvnzdthDuy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_nqxzBcvnzdthDuy
.L_small_initial_compute_done_nqxzBcvnzdthDuy:
.L_after_reduction_nqxzBcvnzdthDuy:
	jmp	.L_last_blocks_done_geifcuAbzFlAapy
.L_last_num_blocks_is_2_geifcuAbzFlAapy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_CntFxwulDtcdxyf
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_CntFxwulDtcdxyf

.L_16_blocks_overflow_CntFxwulDtcdxyf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_CntFxwulDtcdxyf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_mwCCsFqjEkABgEj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_mwCCsFqjEkABgEj
.L_small_initial_partial_block_mwCCsFqjEkABgEj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mwCCsFqjEkABgEj:

	orq	%r8,%r8
	je	.L_after_reduction_mwCCsFqjEkABgEj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_mwCCsFqjEkABgEj:
	jmp	.L_last_blocks_done_geifcuAbzFlAapy
.L_last_num_blocks_is_3_geifcuAbzFlAapy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_BridshDxAhwsboE
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_BridshDxAhwsboE

.L_16_blocks_overflow_BridshDxAhwsboE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_BridshDxAhwsboE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kxahfGlwjdqDgjd





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kxahfGlwjdqDgjd
.L_small_initial_partial_block_kxahfGlwjdqDgjd:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kxahfGlwjdqDgjd:

	orq	%r8,%r8
	je	.L_after_reduction_kxahfGlwjdqDgjd
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kxahfGlwjdqDgjd:
	jmp	.L_last_blocks_done_geifcuAbzFlAapy
.L_last_num_blocks_is_4_geifcuAbzFlAapy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_rftpudoygAhlprG
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_rftpudoygAhlprG

.L_16_blocks_overflow_rftpudoygAhlprG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_rftpudoygAhlprG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CElnriadyCiDaon





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CElnriadyCiDaon
.L_small_initial_partial_block_CElnriadyCiDaon:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CElnriadyCiDaon:

	orq	%r8,%r8
	je	.L_after_reduction_CElnriadyCiDaon
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CElnriadyCiDaon:
	jmp	.L_last_blocks_done_geifcuAbzFlAapy
.L_last_num_blocks_is_5_geifcuAbzFlAapy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_ottuvkmpwyninhj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_ottuvkmpwyninhj

.L_16_blocks_overflow_ottuvkmpwyninhj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_ottuvkmpwyninhj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bouaknndaenBiAf





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bouaknndaenBiAf
.L_small_initial_partial_block_bouaknndaenBiAf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bouaknndaenBiAf:

	orq	%r8,%r8
	je	.L_after_reduction_bouaknndaenBiAf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bouaknndaenBiAf:
	jmp	.L_last_blocks_done_geifcuAbzFlAapy
.L_last_num_blocks_is_6_geifcuAbzFlAapy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_qoEjAxifyGxvEgE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_qoEjAxifyGxvEgE

.L_16_blocks_overflow_qoEjAxifyGxvEgE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_qoEjAxifyGxvEgE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zwkktCGjbyogqgt





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zwkktCGjbyogqgt
.L_small_initial_partial_block_zwkktCGjbyogqgt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zwkktCGjbyogqgt:

	orq	%r8,%r8
	je	.L_after_reduction_zwkktCGjbyogqgt
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zwkktCGjbyogqgt:
	jmp	.L_last_blocks_done_geifcuAbzFlAapy
.L_last_num_blocks_is_7_geifcuAbzFlAapy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_evCoixcAcohfeqB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_evCoixcAcohfeqB

.L_16_blocks_overflow_evCoixcAcohfeqB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_evCoixcAcohfeqB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_shuCdtijlejnhdq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_shuCdtijlejnhdq
.L_small_initial_partial_block_shuCdtijlejnhdq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_shuCdtijlejnhdq:

	orq	%r8,%r8
	je	.L_after_reduction_shuCdtijlejnhdq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_shuCdtijlejnhdq:
	jmp	.L_last_blocks_done_geifcuAbzFlAapy
.L_last_num_blocks_is_8_geifcuAbzFlAapy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_ziGjlErCtFlAria
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_ziGjlErCtFlAria

.L_16_blocks_overflow_ziGjlErCtFlAria:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_ziGjlErCtFlAria:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cpvCGwyvgAhkfkl





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cpvCGwyvgAhkfkl
.L_small_initial_partial_block_cpvCGwyvgAhkfkl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cpvCGwyvgAhkfkl:

	orq	%r8,%r8
	je	.L_after_reduction_cpvCGwyvgAhkfkl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cpvCGwyvgAhkfkl:
	jmp	.L_last_blocks_done_geifcuAbzFlAapy
.L_last_num_blocks_is_9_geifcuAbzFlAapy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_gkfhGyAelxwgskj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_gkfhGyAelxwgskj

.L_16_blocks_overflow_gkfhGyAelxwgskj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_gkfhGyAelxwgskj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EgtrdErskitCoqa





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EgtrdErskitCoqa
.L_small_initial_partial_block_EgtrdErskitCoqa:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EgtrdErskitCoqa:

	orq	%r8,%r8
	je	.L_after_reduction_EgtrdErskitCoqa
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_EgtrdErskitCoqa:
	jmp	.L_last_blocks_done_geifcuAbzFlAapy
.L_last_num_blocks_is_10_geifcuAbzFlAapy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_obrFohcskCiBvrz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_obrFohcskCiBvrz

.L_16_blocks_overflow_obrFohcskCiBvrz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_obrFohcskCiBvrz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jtpxavBgvhbliyA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jtpxavBgvhbliyA
.L_small_initial_partial_block_jtpxavBgvhbliyA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jtpxavBgvhbliyA:

	orq	%r8,%r8
	je	.L_after_reduction_jtpxavBgvhbliyA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jtpxavBgvhbliyA:
	jmp	.L_last_blocks_done_geifcuAbzFlAapy
.L_last_num_blocks_is_11_geifcuAbzFlAapy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_ebGykqaukApsrFx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_ebGykqaukApsrFx

.L_16_blocks_overflow_ebGykqaukApsrFx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_ebGykqaukApsrFx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bvygmFayCaslqht





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bvygmFayCaslqht
.L_small_initial_partial_block_bvygmFayCaslqht:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bvygmFayCaslqht:

	orq	%r8,%r8
	je	.L_after_reduction_bvygmFayCaslqht
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bvygmFayCaslqht:
	jmp	.L_last_blocks_done_geifcuAbzFlAapy
.L_last_num_blocks_is_12_geifcuAbzFlAapy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_atvEixpjawCCsym
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_atvEixpjawCCsym

.L_16_blocks_overflow_atvEixpjawCCsym:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_atvEixpjawCCsym:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_orFmlsljpxigbaf





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_orFmlsljpxigbaf
.L_small_initial_partial_block_orFmlsljpxigbaf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_orFmlsljpxigbaf:

	orq	%r8,%r8
	je	.L_after_reduction_orFmlsljpxigbaf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_orFmlsljpxigbaf:
	jmp	.L_last_blocks_done_geifcuAbzFlAapy
.L_last_num_blocks_is_13_geifcuAbzFlAapy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_ucEGuGeqvBbgaai
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_ucEGuGeqvBbgaai

.L_16_blocks_overflow_ucEGuGeqvBbgaai:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_ucEGuGeqvBbgaai:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_aqaDnbamEgEzgsF





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_aqaDnbamEgEzgsF
.L_small_initial_partial_block_aqaDnbamEgEzgsF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_aqaDnbamEgEzgsF:

	orq	%r8,%r8
	je	.L_after_reduction_aqaDnbamEgEzgsF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_aqaDnbamEgEzgsF:
	jmp	.L_last_blocks_done_geifcuAbzFlAapy
.L_last_num_blocks_is_14_geifcuAbzFlAapy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_bFcDdwGrCEpwyel
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_bFcDdwGrCEpwyel

.L_16_blocks_overflow_bFcDdwGrCEpwyel:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_bFcDdwGrCEpwyel:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iaFfgjfGDiqAdiG





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iaFfgjfGDiqAdiG
.L_small_initial_partial_block_iaFfgjfGDiqAdiG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iaFfgjfGDiqAdiG:

	orq	%r8,%r8
	je	.L_after_reduction_iaFfgjfGDiqAdiG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_iaFfgjfGDiqAdiG:
	jmp	.L_last_blocks_done_geifcuAbzFlAapy
.L_last_num_blocks_is_15_geifcuAbzFlAapy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_nhiCpBmtguraEdl
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_nhiCpBmtguraEdl

.L_16_blocks_overflow_nhiCpBmtguraEdl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_nhiCpBmtguraEdl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DyuaGoFpqrjxBmv





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DyuaGoFpqrjxBmv
.L_small_initial_partial_block_DyuaGoFpqrjxBmv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DyuaGoFpqrjxBmv:

	orq	%r8,%r8
	je	.L_after_reduction_DyuaGoFpqrjxBmv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DyuaGoFpqrjxBmv:
	jmp	.L_last_blocks_done_geifcuAbzFlAapy
.L_last_num_blocks_is_16_geifcuAbzFlAapy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_gbDFnyxlzcgbBqf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_gbDFnyxlzcgbBqf

.L_16_blocks_overflow_gbDFnyxlzcgbBqf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_gbDFnyxlzcgbBqf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_CFvCBuBbkwuAteC:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CFvCBuBbkwuAteC:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CFvCBuBbkwuAteC:
	jmp	.L_last_blocks_done_geifcuAbzFlAapy
.L_last_num_blocks_is_0_geifcuAbzFlAapy:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_geifcuAbzFlAapy:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_ioflBkhvfsafcEu
.L_encrypt_16_blocks_ioflBkhvfsafcEu:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_uecbbulgkdwxrvq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_uecbbulgkdwxrvq
.L_16_blocks_overflow_uecbbulgkdwxrvq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_uecbbulgkdwxrvq:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_GsmpGmtvdqCfkBm

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_GsmpGmtvdqCfkBm
	jb	.L_last_num_blocks_is_7_1_GsmpGmtvdqCfkBm


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_GsmpGmtvdqCfkBm
	jb	.L_last_num_blocks_is_11_9_GsmpGmtvdqCfkBm


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_GsmpGmtvdqCfkBm
	ja	.L_last_num_blocks_is_16_GsmpGmtvdqCfkBm
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_GsmpGmtvdqCfkBm
	jmp	.L_last_num_blocks_is_13_GsmpGmtvdqCfkBm

.L_last_num_blocks_is_11_9_GsmpGmtvdqCfkBm:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_GsmpGmtvdqCfkBm
	ja	.L_last_num_blocks_is_11_GsmpGmtvdqCfkBm
	jmp	.L_last_num_blocks_is_9_GsmpGmtvdqCfkBm

.L_last_num_blocks_is_7_1_GsmpGmtvdqCfkBm:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_GsmpGmtvdqCfkBm
	jb	.L_last_num_blocks_is_3_1_GsmpGmtvdqCfkBm

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_GsmpGmtvdqCfkBm
	je	.L_last_num_blocks_is_6_GsmpGmtvdqCfkBm
	jmp	.L_last_num_blocks_is_5_GsmpGmtvdqCfkBm

.L_last_num_blocks_is_3_1_GsmpGmtvdqCfkBm:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_GsmpGmtvdqCfkBm
	je	.L_last_num_blocks_is_2_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_1_GsmpGmtvdqCfkBm:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_rpiyrdAvsltolEl
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_rpiyrdAvsltolEl

.L_16_blocks_overflow_rpiyrdAvsltolEl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_rpiyrdAvsltolEl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cbfCmtEqfiuzoji





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cbfCmtEqfiuzoji
.L_small_initial_partial_block_cbfCmtEqfiuzoji:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_cbfCmtEqfiuzoji
.L_small_initial_compute_done_cbfCmtEqfiuzoji:
.L_after_reduction_cbfCmtEqfiuzoji:
	jmp	.L_last_blocks_done_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_2_GsmpGmtvdqCfkBm:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_AszanukAyFbhppg
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_AszanukAyFbhppg

.L_16_blocks_overflow_AszanukAyFbhppg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_AszanukAyFbhppg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dlvijFGmblobpau





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dlvijFGmblobpau
.L_small_initial_partial_block_dlvijFGmblobpau:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dlvijFGmblobpau:

	orq	%r8,%r8
	je	.L_after_reduction_dlvijFGmblobpau
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dlvijFGmblobpau:
	jmp	.L_last_blocks_done_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_3_GsmpGmtvdqCfkBm:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_BnwsCbicGsABuqE
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_BnwsCbicGsABuqE

.L_16_blocks_overflow_BnwsCbicGsABuqE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_BnwsCbicGsABuqE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dxqswACqByhyqoe





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dxqswACqByhyqoe
.L_small_initial_partial_block_dxqswACqByhyqoe:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dxqswACqByhyqoe:

	orq	%r8,%r8
	je	.L_after_reduction_dxqswACqByhyqoe
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dxqswACqByhyqoe:
	jmp	.L_last_blocks_done_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_4_GsmpGmtvdqCfkBm:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_gpciwryDmBawrer
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_gpciwryDmBawrer

.L_16_blocks_overflow_gpciwryDmBawrer:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_gpciwryDmBawrer:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_detixpdyofdxhwp





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_detixpdyofdxhwp
.L_small_initial_partial_block_detixpdyofdxhwp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_detixpdyofdxhwp:

	orq	%r8,%r8
	je	.L_after_reduction_detixpdyofdxhwp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_detixpdyofdxhwp:
	jmp	.L_last_blocks_done_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_5_GsmpGmtvdqCfkBm:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_agsFBFmzFiqvtCv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_agsFBFmzFiqvtCv

.L_16_blocks_overflow_agsFBFmzFiqvtCv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_agsFBFmzFiqvtCv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_irkgagArocFBpra





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_irkgagArocFBpra
.L_small_initial_partial_block_irkgagArocFBpra:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_irkgagArocFBpra:

	orq	%r8,%r8
	je	.L_after_reduction_irkgagArocFBpra
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_irkgagArocFBpra:
	jmp	.L_last_blocks_done_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_6_GsmpGmtvdqCfkBm:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_kGDhGxkkxaCBCEy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_kGDhGxkkxaCBCEy

.L_16_blocks_overflow_kGDhGxkkxaCBCEy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_kGDhGxkkxaCBCEy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_hEzcjrnGiiDmEqu





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_hEzcjrnGiiDmEqu
.L_small_initial_partial_block_hEzcjrnGiiDmEqu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_hEzcjrnGiiDmEqu:

	orq	%r8,%r8
	je	.L_after_reduction_hEzcjrnGiiDmEqu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_hEzcjrnGiiDmEqu:
	jmp	.L_last_blocks_done_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_7_GsmpGmtvdqCfkBm:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_nDtaufoshGmaFtD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_nDtaufoshGmaFtD

.L_16_blocks_overflow_nDtaufoshGmaFtD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_nDtaufoshGmaFtD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_knCkEkuEyznxcec





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_knCkEkuEyznxcec
.L_small_initial_partial_block_knCkEkuEyznxcec:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_knCkEkuEyznxcec:

	orq	%r8,%r8
	je	.L_after_reduction_knCkEkuEyznxcec
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_knCkEkuEyznxcec:
	jmp	.L_last_blocks_done_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_8_GsmpGmtvdqCfkBm:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_CfkoldsCBinciFo
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_CfkoldsCBinciFo

.L_16_blocks_overflow_CfkoldsCBinciFo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_CfkoldsCBinciFo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_uCBrertBEjqDqiA





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_uCBrertBEjqDqiA
.L_small_initial_partial_block_uCBrertBEjqDqiA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_uCBrertBEjqDqiA:

	orq	%r8,%r8
	je	.L_after_reduction_uCBrertBEjqDqiA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_uCBrertBEjqDqiA:
	jmp	.L_last_blocks_done_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_9_GsmpGmtvdqCfkBm:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_mdzEkihgFfoDvei
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_mdzEkihgFfoDvei

.L_16_blocks_overflow_mdzEkihgFfoDvei:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_mdzEkihgFfoDvei:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bGmpxheDbnuAluw





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bGmpxheDbnuAluw
.L_small_initial_partial_block_bGmpxheDbnuAluw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bGmpxheDbnuAluw:

	orq	%r8,%r8
	je	.L_after_reduction_bGmpxheDbnuAluw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bGmpxheDbnuAluw:
	jmp	.L_last_blocks_done_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_10_GsmpGmtvdqCfkBm:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_lfgzqpGmAszfuzm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_lfgzqpGmAszfuzm

.L_16_blocks_overflow_lfgzqpGmAszfuzm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_lfgzqpGmAszfuzm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zemCfmtcGsmwwdF





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zemCfmtcGsmwwdF
.L_small_initial_partial_block_zemCfmtcGsmwwdF:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zemCfmtcGsmwwdF:

	orq	%r8,%r8
	je	.L_after_reduction_zemCfmtcGsmwwdF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zemCfmtcGsmwwdF:
	jmp	.L_last_blocks_done_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_11_GsmpGmtvdqCfkBm:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_eiFleesjktxxzcB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_eiFleesjktxxzcB

.L_16_blocks_overflow_eiFleesjktxxzcB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_eiFleesjktxxzcB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yekzkgtayjrbxmw





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yekzkgtayjrbxmw
.L_small_initial_partial_block_yekzkgtayjrbxmw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yekzkgtayjrbxmw:

	orq	%r8,%r8
	je	.L_after_reduction_yekzkgtayjrbxmw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_yekzkgtayjrbxmw:
	jmp	.L_last_blocks_done_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_12_GsmpGmtvdqCfkBm:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_AkybfbxDsGwqafx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_AkybfbxDsGwqafx

.L_16_blocks_overflow_AkybfbxDsGwqafx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_AkybfbxDsGwqafx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ahgihpusFisgxaE





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ahgihpusFisgxaE
.L_small_initial_partial_block_ahgihpusFisgxaE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ahgihpusFisgxaE:

	orq	%r8,%r8
	je	.L_after_reduction_ahgihpusFisgxaE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ahgihpusFisgxaE:
	jmp	.L_last_blocks_done_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_13_GsmpGmtvdqCfkBm:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_yxyfDaBGBjwjolm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_yxyfDaBGBjwjolm

.L_16_blocks_overflow_yxyfDaBGBjwjolm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_yxyfDaBGBjwjolm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dusqjgxpastGane





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dusqjgxpastGane
.L_small_initial_partial_block_dusqjgxpastGane:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dusqjgxpastGane:

	orq	%r8,%r8
	je	.L_after_reduction_dusqjgxpastGane
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dusqjgxpastGane:
	jmp	.L_last_blocks_done_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_14_GsmpGmtvdqCfkBm:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_rjvrCtayEcnzEFr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_rjvrCtayEcnzEFr

.L_16_blocks_overflow_rjvrCtayEcnzEFr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_rjvrCtayEcnzEFr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vwffogaFFzochal





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vwffogaFFzochal
.L_small_initial_partial_block_vwffogaFFzochal:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vwffogaFFzochal:

	orq	%r8,%r8
	je	.L_after_reduction_vwffogaFFzochal
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vwffogaFFzochal:
	jmp	.L_last_blocks_done_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_15_GsmpGmtvdqCfkBm:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_qwvqvBEeBvFnqoe
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_qwvqvBEeBvFnqoe

.L_16_blocks_overflow_qwvqvBEeBvFnqoe:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_qwvqvBEeBvFnqoe:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tqxjEhwnommfvkh





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tqxjEhwnommfvkh
.L_small_initial_partial_block_tqxjEhwnommfvkh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tqxjEhwnommfvkh:

	orq	%r8,%r8
	je	.L_after_reduction_tqxjEhwnommfvkh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tqxjEhwnommfvkh:
	jmp	.L_last_blocks_done_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_16_GsmpGmtvdqCfkBm:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_uzigczBrrCotpeb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_uzigczBrrCotpeb

.L_16_blocks_overflow_uzigczBrrCotpeb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_uzigczBrrCotpeb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_cDiEuDwoDCvjyhv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cDiEuDwoDCvjyhv:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cDiEuDwoDCvjyhv:
	jmp	.L_last_blocks_done_GsmpGmtvdqCfkBm
.L_last_num_blocks_is_0_GsmpGmtvdqCfkBm:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_GsmpGmtvdqCfkBm:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_ioflBkhvfsafcEu

.L_message_below_32_blocks_ioflBkhvfsafcEu:


	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_nGoqmGsjsedFbol
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
.L_skip_hkeys_precomputation_nGoqmGsjsedFbol:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_ccEbGBqFvbfmhAo

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_ccEbGBqFvbfmhAo
	jb	.L_last_num_blocks_is_7_1_ccEbGBqFvbfmhAo


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_ccEbGBqFvbfmhAo
	jb	.L_last_num_blocks_is_11_9_ccEbGBqFvbfmhAo


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_ccEbGBqFvbfmhAo
	ja	.L_last_num_blocks_is_16_ccEbGBqFvbfmhAo
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_ccEbGBqFvbfmhAo
	jmp	.L_last_num_blocks_is_13_ccEbGBqFvbfmhAo

.L_last_num_blocks_is_11_9_ccEbGBqFvbfmhAo:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_ccEbGBqFvbfmhAo
	ja	.L_last_num_blocks_is_11_ccEbGBqFvbfmhAo
	jmp	.L_last_num_blocks_is_9_ccEbGBqFvbfmhAo

.L_last_num_blocks_is_7_1_ccEbGBqFvbfmhAo:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_ccEbGBqFvbfmhAo
	jb	.L_last_num_blocks_is_3_1_ccEbGBqFvbfmhAo

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_ccEbGBqFvbfmhAo
	je	.L_last_num_blocks_is_6_ccEbGBqFvbfmhAo
	jmp	.L_last_num_blocks_is_5_ccEbGBqFvbfmhAo

.L_last_num_blocks_is_3_1_ccEbGBqFvbfmhAo:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_ccEbGBqFvbfmhAo
	je	.L_last_num_blocks_is_2_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_1_ccEbGBqFvbfmhAo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_GscdgyEwjrclfjp
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_GscdgyEwjrclfjp

.L_16_blocks_overflow_GscdgyEwjrclfjp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_GscdgyEwjrclfjp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cbkeoakBGvutdAc





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cbkeoakBGvutdAc
.L_small_initial_partial_block_cbkeoakBGvutdAc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_cbkeoakBGvutdAc
.L_small_initial_compute_done_cbkeoakBGvutdAc:
.L_after_reduction_cbkeoakBGvutdAc:
	jmp	.L_last_blocks_done_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_2_ccEbGBqFvbfmhAo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_hfgFvutBdACiGtz
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_hfgFvutBdACiGtz

.L_16_blocks_overflow_hfgFvutBdACiGtz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_hfgFvutBdACiGtz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pdjGiBmapgcqico





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pdjGiBmapgcqico
.L_small_initial_partial_block_pdjGiBmapgcqico:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pdjGiBmapgcqico:

	orq	%r8,%r8
	je	.L_after_reduction_pdjGiBmapgcqico
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_pdjGiBmapgcqico:
	jmp	.L_last_blocks_done_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_3_ccEbGBqFvbfmhAo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_cBffoFfiCvzvifs
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_cBffoFfiCvzvifs

.L_16_blocks_overflow_cBffoFfiCvzvifs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_cBffoFfiCvzvifs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zGeCEuFGDEwnhnq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zGeCEuFGDEwnhnq
.L_small_initial_partial_block_zGeCEuFGDEwnhnq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zGeCEuFGDEwnhnq:

	orq	%r8,%r8
	je	.L_after_reduction_zGeCEuFGDEwnhnq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zGeCEuFGDEwnhnq:
	jmp	.L_last_blocks_done_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_4_ccEbGBqFvbfmhAo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_CxhynxGBCraCmcu
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_CxhynxGBCraCmcu

.L_16_blocks_overflow_CxhynxGBCraCmcu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_CxhynxGBCraCmcu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_hddsjqizCegzGex





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_hddsjqizCegzGex
.L_small_initial_partial_block_hddsjqizCegzGex:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_hddsjqizCegzGex:

	orq	%r8,%r8
	je	.L_after_reduction_hddsjqizCegzGex
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_hddsjqizCegzGex:
	jmp	.L_last_blocks_done_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_5_ccEbGBqFvbfmhAo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_wvapeiBGaaDEkvq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_wvapeiBGaaDEkvq

.L_16_blocks_overflow_wvapeiBGaaDEkvq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_wvapeiBGaaDEkvq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iDAwEAcwrwzFbpp





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iDAwEAcwrwzFbpp
.L_small_initial_partial_block_iDAwEAcwrwzFbpp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iDAwEAcwrwzFbpp:

	orq	%r8,%r8
	je	.L_after_reduction_iDAwEAcwrwzFbpp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_iDAwEAcwrwzFbpp:
	jmp	.L_last_blocks_done_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_6_ccEbGBqFvbfmhAo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_GhaAoBcybyaFfbn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_GhaAoBcybyaFfbn

.L_16_blocks_overflow_GhaAoBcybyaFfbn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_GhaAoBcybyaFfbn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lEudimznCcmddmD





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lEudimznCcmddmD
.L_small_initial_partial_block_lEudimznCcmddmD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lEudimznCcmddmD:

	orq	%r8,%r8
	je	.L_after_reduction_lEudimznCcmddmD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lEudimznCcmddmD:
	jmp	.L_last_blocks_done_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_7_ccEbGBqFvbfmhAo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_zCoauBpkuFixsjp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_zCoauBpkuFixsjp

.L_16_blocks_overflow_zCoauBpkuFixsjp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_zCoauBpkuFixsjp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ubhmAArDoxmnEkq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ubhmAArDoxmnEkq
.L_small_initial_partial_block_ubhmAArDoxmnEkq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ubhmAArDoxmnEkq:

	orq	%r8,%r8
	je	.L_after_reduction_ubhmAArDoxmnEkq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ubhmAArDoxmnEkq:
	jmp	.L_last_blocks_done_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_8_ccEbGBqFvbfmhAo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_fzllbCxrjbCdBeG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_fzllbCxrjbCdBeG

.L_16_blocks_overflow_fzllbCxrjbCdBeG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_fzllbCxrjbCdBeG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xcxeoslceiiBgbp





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xcxeoslceiiBgbp
.L_small_initial_partial_block_xcxeoslceiiBgbp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xcxeoslceiiBgbp:

	orq	%r8,%r8
	je	.L_after_reduction_xcxeoslceiiBgbp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xcxeoslceiiBgbp:
	jmp	.L_last_blocks_done_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_9_ccEbGBqFvbfmhAo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_hBbFpdEBqtwkDdy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_hBbFpdEBqtwkDdy

.L_16_blocks_overflow_hBbFpdEBqtwkDdy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_hBbFpdEBqtwkDdy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_frnGqudGaqakhdr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_frnGqudGaqakhdr
.L_small_initial_partial_block_frnGqudGaqakhdr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_frnGqudGaqakhdr:

	orq	%r8,%r8
	je	.L_after_reduction_frnGqudGaqakhdr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_frnGqudGaqakhdr:
	jmp	.L_last_blocks_done_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_10_ccEbGBqFvbfmhAo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_oDimegtwwEAzsqe
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_oDimegtwwEAzsqe

.L_16_blocks_overflow_oDimegtwwEAzsqe:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_oDimegtwwEAzsqe:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tiEqFgvouiqzeeu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tiEqFgvouiqzeeu
.L_small_initial_partial_block_tiEqFgvouiqzeeu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tiEqFgvouiqzeeu:

	orq	%r8,%r8
	je	.L_after_reduction_tiEqFgvouiqzeeu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tiEqFgvouiqzeeu:
	jmp	.L_last_blocks_done_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_11_ccEbGBqFvbfmhAo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_uiGukqfBqonhqrg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_uiGukqfBqonhqrg

.L_16_blocks_overflow_uiGukqfBqonhqrg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_uiGukqfBqonhqrg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_knogGCGrEnyCyrs





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_knogGCGrEnyCyrs
.L_small_initial_partial_block_knogGCGrEnyCyrs:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_knogGCGrEnyCyrs:

	orq	%r8,%r8
	je	.L_after_reduction_knogGCGrEnyCyrs
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_knogGCGrEnyCyrs:
	jmp	.L_last_blocks_done_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_12_ccEbGBqFvbfmhAo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_ijFdEksmDwsxija
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_ijFdEksmDwsxija

.L_16_blocks_overflow_ijFdEksmDwsxija:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_ijFdEksmDwsxija:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_aCndrjfAdcmolux





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_aCndrjfAdcmolux
.L_small_initial_partial_block_aCndrjfAdcmolux:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_aCndrjfAdcmolux:

	orq	%r8,%r8
	je	.L_after_reduction_aCndrjfAdcmolux
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_aCndrjfAdcmolux:
	jmp	.L_last_blocks_done_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_13_ccEbGBqFvbfmhAo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_rkzlChjomlfjkDF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_rkzlChjomlfjkDF

.L_16_blocks_overflow_rkzlChjomlfjkDF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_rkzlChjomlfjkDF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ydEejbmnyuzyhly





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ydEejbmnyuzyhly
.L_small_initial_partial_block_ydEejbmnyuzyhly:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ydEejbmnyuzyhly:

	orq	%r8,%r8
	je	.L_after_reduction_ydEejbmnyuzyhly
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ydEejbmnyuzyhly:
	jmp	.L_last_blocks_done_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_14_ccEbGBqFvbfmhAo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_tzffhAoBrDeilmi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_tzffhAoBrDeilmi

.L_16_blocks_overflow_tzffhAoBrDeilmi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_tzffhAoBrDeilmi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ngkqebvmECgBlgw





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ngkqebvmECgBlgw
.L_small_initial_partial_block_ngkqebvmECgBlgw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ngkqebvmECgBlgw:

	orq	%r8,%r8
	je	.L_after_reduction_ngkqebvmECgBlgw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ngkqebvmECgBlgw:
	jmp	.L_last_blocks_done_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_15_ccEbGBqFvbfmhAo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_FtuwjtuoFptbCsq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_FtuwjtuoFptbCsq

.L_16_blocks_overflow_FtuwjtuoFptbCsq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_FtuwjtuoFptbCsq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xFqpFxmmaofmeqc





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xFqpFxmmaofmeqc
.L_small_initial_partial_block_xFqpFxmmaofmeqc:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xFqpFxmmaofmeqc:

	orq	%r8,%r8
	je	.L_after_reduction_xFqpFxmmaofmeqc
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xFqpFxmmaofmeqc:
	jmp	.L_last_blocks_done_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_16_ccEbGBqFvbfmhAo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_vDevjCidEsrFDBE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_vDevjCidEsrFDBE

.L_16_blocks_overflow_vDevjCidEsrFDBE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_vDevjCidEsrFDBE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_ewyqehpaFfyfEhG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ewyqehpaFfyfEhG:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ewyqehpaFfyfEhG:
	jmp	.L_last_blocks_done_ccEbGBqFvbfmhAo
.L_last_num_blocks_is_0_ccEbGBqFvbfmhAo:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_ccEbGBqFvbfmhAo:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_ioflBkhvfsafcEu

.L_message_below_equal_16_blocks_ioflBkhvfsafcEu:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	.L_small_initial_num_blocks_is_8_tipBECssyqzbbgA
	jl	.L_small_initial_num_blocks_is_7_1_tipBECssyqzbbgA


	cmpq	$12,%r12
	je	.L_small_initial_num_blocks_is_12_tipBECssyqzbbgA
	jl	.L_small_initial_num_blocks_is_11_9_tipBECssyqzbbgA


	cmpq	$16,%r12
	je	.L_small_initial_num_blocks_is_16_tipBECssyqzbbgA
	cmpq	$15,%r12
	je	.L_small_initial_num_blocks_is_15_tipBECssyqzbbgA
	cmpq	$14,%r12
	je	.L_small_initial_num_blocks_is_14_tipBECssyqzbbgA
	jmp	.L_small_initial_num_blocks_is_13_tipBECssyqzbbgA

.L_small_initial_num_blocks_is_11_9_tipBECssyqzbbgA:

	cmpq	$11,%r12
	je	.L_small_initial_num_blocks_is_11_tipBECssyqzbbgA
	cmpq	$10,%r12
	je	.L_small_initial_num_blocks_is_10_tipBECssyqzbbgA
	jmp	.L_small_initial_num_blocks_is_9_tipBECssyqzbbgA

.L_small_initial_num_blocks_is_7_1_tipBECssyqzbbgA:
	cmpq	$4,%r12
	je	.L_small_initial_num_blocks_is_4_tipBECssyqzbbgA
	jl	.L_small_initial_num_blocks_is_3_1_tipBECssyqzbbgA

	cmpq	$7,%r12
	je	.L_small_initial_num_blocks_is_7_tipBECssyqzbbgA
	cmpq	$6,%r12
	je	.L_small_initial_num_blocks_is_6_tipBECssyqzbbgA
	jmp	.L_small_initial_num_blocks_is_5_tipBECssyqzbbgA

.L_small_initial_num_blocks_is_3_1_tipBECssyqzbbgA:

	cmpq	$3,%r12
	je	.L_small_initial_num_blocks_is_3_tipBECssyqzbbgA
	cmpq	$2,%r12
	je	.L_small_initial_num_blocks_is_2_tipBECssyqzbbgA





.L_small_initial_num_blocks_is_1_tipBECssyqzbbgA:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%r11,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm6,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EEcdcbljhiihjDp





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EEcdcbljhiihjDp
.L_small_initial_partial_block_EEcdcbljhiihjDp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	.L_after_reduction_EEcdcbljhiihjDp
.L_small_initial_compute_done_EEcdcbljhiihjDp:
.L_after_reduction_EEcdcbljhiihjDp:
	jmp	.L_small_initial_blocks_encrypted_tipBECssyqzbbgA
.L_small_initial_num_blocks_is_2_tipBECssyqzbbgA:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%r11,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm6,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cuCnpDsgEfhszng





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cuCnpDsgEfhszng
.L_small_initial_partial_block_cuCnpDsgEfhszng:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cuCnpDsgEfhszng:

	orq	%r8,%r8
	je	.L_after_reduction_cuCnpDsgEfhszng
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_cuCnpDsgEfhszng:
	jmp	.L_small_initial_blocks_encrypted_tipBECssyqzbbgA
.L_small_initial_num_blocks_is_3_tipBECssyqzbbgA:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yFgaflqBFvlggxa





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yFgaflqBFvlggxa
.L_small_initial_partial_block_yFgaflqBFvlggxa:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yFgaflqBFvlggxa:

	orq	%r8,%r8
	je	.L_after_reduction_yFgaflqBFvlggxa
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_yFgaflqBFvlggxa:
	jmp	.L_small_initial_blocks_encrypted_tipBECssyqzbbgA
.L_small_initial_num_blocks_is_4_tipBECssyqzbbgA:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_roaFtsCavjenmBE





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_roaFtsCavjenmBE
.L_small_initial_partial_block_roaFtsCavjenmBE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_roaFtsCavjenmBE:

	orq	%r8,%r8
	je	.L_after_reduction_roaFtsCavjenmBE
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_roaFtsCavjenmBE:
	jmp	.L_small_initial_blocks_encrypted_tipBECssyqzbbgA
.L_small_initial_num_blocks_is_5_tipBECssyqzbbgA:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%xmm29,%xmm7,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bxGdgvivDiymCjx





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bxGdgvivDiymCjx
.L_small_initial_partial_block_bxGdgvivDiymCjx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bxGdgvivDiymCjx:

	orq	%r8,%r8
	je	.L_after_reduction_bxGdgvivDiymCjx
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_bxGdgvivDiymCjx:
	jmp	.L_small_initial_blocks_encrypted_tipBECssyqzbbgA
.L_small_initial_num_blocks_is_6_tipBECssyqzbbgA:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%ymm29,%ymm7,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ecegijCpbdgllzn





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ecegijCpbdgllzn
.L_small_initial_partial_block_ecegijCpbdgllzn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ecegijCpbdgllzn:

	orq	%r8,%r8
	je	.L_after_reduction_ecegijCpbdgllzn
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_ecegijCpbdgllzn:
	jmp	.L_small_initial_blocks_encrypted_tipBECssyqzbbgA
.L_small_initial_num_blocks_is_7_tipBECssyqzbbgA:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_srlumwddqFznBcv





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_srlumwddqFznBcv
.L_small_initial_partial_block_srlumwddqFznBcv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_srlumwddqFznBcv:

	orq	%r8,%r8
	je	.L_after_reduction_srlumwddqFznBcv
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_srlumwddqFznBcv:
	jmp	.L_small_initial_blocks_encrypted_tipBECssyqzbbgA
.L_small_initial_num_blocks_is_8_tipBECssyqzbbgA:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jgaGFaEAxeedDda





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jgaGFaEAxeedDda
.L_small_initial_partial_block_jgaGFaEAxeedDda:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jgaGFaEAxeedDda:

	orq	%r8,%r8
	je	.L_after_reduction_jgaGFaEAxeedDda
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_jgaGFaEAxeedDda:
	jmp	.L_small_initial_blocks_encrypted_tipBECssyqzbbgA
.L_small_initial_num_blocks_is_9_tipBECssyqzbbgA:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%xmm29,%xmm10,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AdEshCFihDfipyy





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AdEshCFihDfipyy
.L_small_initial_partial_block_AdEshCFihDfipyy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AdEshCFihDfipyy:

	orq	%r8,%r8
	je	.L_after_reduction_AdEshCFihDfipyy
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_AdEshCFihDfipyy:
	jmp	.L_small_initial_blocks_encrypted_tipBECssyqzbbgA
.L_small_initial_num_blocks_is_10_tipBECssyqzbbgA:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%ymm29,%ymm10,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_oehyGfwmwABfflf





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_oehyGfwmwABfflf
.L_small_initial_partial_block_oehyGfwmwABfflf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_oehyGfwmwABfflf:

	orq	%r8,%r8
	je	.L_after_reduction_oehyGfwmwABfflf
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_oehyGfwmwABfflf:
	jmp	.L_small_initial_blocks_encrypted_tipBECssyqzbbgA
.L_small_initial_num_blocks_is_11_tipBECssyqzbbgA:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bgrwieoxdeojtvn





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bgrwieoxdeojtvn
.L_small_initial_partial_block_bgrwieoxdeojtvn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bgrwieoxdeojtvn:

	orq	%r8,%r8
	je	.L_after_reduction_bgrwieoxdeojtvn
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_bgrwieoxdeojtvn:
	jmp	.L_small_initial_blocks_encrypted_tipBECssyqzbbgA
.L_small_initial_num_blocks_is_12_tipBECssyqzbbgA:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dmhyboFdlblAfmb





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dmhyboFdlblAfmb
.L_small_initial_partial_block_dmhyboFdlblAfmb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dmhyboFdlblAfmb:

	orq	%r8,%r8
	je	.L_after_reduction_dmhyboFdlblAfmb
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_dmhyboFdlblAfmb:
	jmp	.L_small_initial_blocks_encrypted_tipBECssyqzbbgA
.L_small_initial_num_blocks_is_13_tipBECssyqzbbgA:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%xmm29,%xmm11,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_soopEuzukCylfyy





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_soopEuzukCylfyy
.L_small_initial_partial_block_soopEuzukCylfyy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_soopEuzukCylfyy:

	orq	%r8,%r8
	je	.L_after_reduction_soopEuzukCylfyy
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_soopEuzukCylfyy:
	jmp	.L_small_initial_blocks_encrypted_tipBECssyqzbbgA
.L_small_initial_num_blocks_is_14_tipBECssyqzbbgA:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%ymm29,%ymm11,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qoFDpfDshBjrpzG





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qoFDpfDshBjrpzG
.L_small_initial_partial_block_qoFDpfDshBjrpzG:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qoFDpfDshBjrpzG:

	orq	%r8,%r8
	je	.L_after_reduction_qoFDpfDshBjrpzG
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_qoFDpfDshBjrpzG:
	jmp	.L_small_initial_blocks_encrypted_tipBECssyqzbbgA
.L_small_initial_num_blocks_is_15_tipBECssyqzbbgA:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_hmkkFljggEiDouz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_hmkkFljggEiDouz
.L_small_initial_partial_block_hmkkFljggEiDouz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_hmkkFljggEiDouz:

	orq	%r8,%r8
	je	.L_after_reduction_hmkkFljggEiDouz
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_hmkkFljggEiDouz:
	jmp	.L_small_initial_blocks_encrypted_tipBECssyqzbbgA
.L_small_initial_num_blocks_is_16_tipBECssyqzbbgA:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_dvvyGtbkzAFtFdu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dvvyGtbkzAFtFdu:
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_dvvyGtbkzAFtFdu:
.L_small_initial_blocks_encrypted_tipBECssyqzbbgA:
.L_ghash_done_ioflBkhvfsafcEu:
	vmovdqu64	%xmm2,0(%rsi)
	vmovdqu64	%xmm14,64(%rsi)
.L_enc_dec_done_ioflBkhvfsafcEu:
	jmp	.Lexit_gcm_decrypt
.align	32
.Laes_gcm_decrypt_256_avx512:
	orq	%r8,%r8
	je	.L_enc_dec_done_cDwywBGkADyfvEf
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14

	movq	(%rdx),%r11
	orq	%r11,%r11
	je	.L_partial_block_done_rfdtprAkrlgjrcs
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3
	vmovdqu64	336(%rsi),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%r11,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3

	vmovdqa64	%xmm0,%xmm6
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%r11,1),%r13
	subq	$16,%r13
	jge	.L_no_extra_mask_rfdtprAkrlgjrcs
	subq	%r13,%r12
.L_no_extra_mask_rfdtprAkrlgjrcs:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpand	%xmm0,%xmm6,%xmm6
	vpshufb	SHUF_MASK(%rip),%xmm6,%xmm6
	vpshufb	%xmm5,%xmm6,%xmm6
	vpxorq	%xmm6,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	.L_partial_incomplete_rfdtprAkrlgjrcs

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movq	$0,(%rdx)

	movq	%r11,%r12
	movq	$16,%r11
	subq	%r12,%r11
	jmp	.L_enc_dec_done_rfdtprAkrlgjrcs

.L_partial_incomplete_rfdtprAkrlgjrcs:
	addq	%r8,(%rdx)
	movq	%r8,%r11

.L_enc_dec_done_rfdtprAkrlgjrcs:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%r11,2),%k1
	vmovdqu64	%xmm14,64(%rsi)
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
.L_partial_block_done_rfdtprAkrlgjrcs:
	vmovdqu64	0(%rsi),%xmm2
	subq	%r11,%r8
	je	.L_enc_dec_done_cDwywBGkADyfvEf
	cmpq	$256,%r8
	jbe	.L_message_below_equal_16_blocks_cDwywBGkADyfvEf

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_hCmlxbFvpmbtgEr
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_hCmlxbFvpmbtgEr
.L_next_16_overflow_hCmlxbFvpmbtgEr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_hCmlxbFvpmbtgEr:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%r11,1),%zmm0
	vmovdqu8	64(%rcx,%r11,1),%zmm3
	vmovdqu8	128(%rcx,%r11,1),%zmm4
	vmovdqu8	192(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	208(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	224(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%r11,1)
	vmovdqu8	%zmm10,64(%r10,%r11,1)
	vmovdqu8	%zmm11,128(%r10,%r11,1)
	vmovdqu8	%zmm12,192(%r10,%r11,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_eDdmsjbmbfgeAuf

	vmovdqu64	288(%rsi),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	224(%rsi),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	160(%rsi),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	96(%rsi),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
.L_skip_hkeys_precomputation_eDdmsjbmbfgeAuf:
	cmpq	$512,%r8
	jb	.L_message_below_32_blocks_cDwywBGkADyfvEf



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_xzvbpbjjamneuxk
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_xzvbpbjjamneuxk
.L_next_16_overflow_xzvbpbjjamneuxk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_xzvbpbjjamneuxk:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%r11,1),%zmm0
	vmovdqu8	320(%rcx,%r11,1),%zmm3
	vmovdqu8	384(%rcx,%r11,1),%zmm4
	vmovdqu8	448(%rcx,%r11,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	208(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	224(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%r11,1)
	vmovdqu8	%zmm10,320(%r10,%r11,1)
	vmovdqu8	%zmm11,384(%r10,%r11,1)
	vmovdqu8	%zmm12,448(%r10,%r11,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_nyBCdptuiqAyxwi
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
.L_skip_hkeys_precomputation_nyBCdptuiqAyxwi:
	movq	$1,%r14
	addq	$512,%r11
	subq	$512,%r8

	cmpq	$768,%r8
	jb	.L_no_more_big_nblocks_cDwywBGkADyfvEf
.L_encrypt_big_nblocks_cDwywBGkADyfvEf:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_BvybAbcEGnbAdpw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_BvybAbcEGnbAdpw
.L_16_blocks_overflow_BvybAbcEGnbAdpw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_BvybAbcEGnbAdpw:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_fitCixcuBsDmcus
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_fitCixcuBsDmcus
.L_16_blocks_overflow_fitCixcuBsDmcus:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_fitCixcuBsDmcus:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_lcmaceDxlBgsaly
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_lcmaceDxlBgsaly
.L_16_blocks_overflow_lcmaceDxlBgsaly:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_lcmaceDxlBgsaly:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%r11,1),%zmm17
	vmovdqu8	576(%rcx,%r11,1),%zmm19
	vmovdqu8	640(%rcx,%r11,1),%zmm20
	vmovdqu8	704(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%r11,1)
	vmovdqu8	%zmm3,576(%r10,%r11,1)
	vmovdqu8	%zmm4,640(%r10,%r11,1)
	vmovdqu8	%zmm5,704(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%r11
	subq	$768,%r8
	cmpq	$768,%r8
	jae	.L_encrypt_big_nblocks_cDwywBGkADyfvEf

.L_no_more_big_nblocks_cDwywBGkADyfvEf:

	cmpq	$512,%r8
	jae	.L_encrypt_32_blocks_cDwywBGkADyfvEf

	cmpq	$256,%r8
	jae	.L_encrypt_16_blocks_cDwywBGkADyfvEf
.L_encrypt_0_blocks_ghash_32_cDwywBGkADyfvEf:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_wtBibnAfFctmwsA

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_wtBibnAfFctmwsA
	jb	.L_last_num_blocks_is_7_1_wtBibnAfFctmwsA


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_wtBibnAfFctmwsA
	jb	.L_last_num_blocks_is_11_9_wtBibnAfFctmwsA


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_wtBibnAfFctmwsA
	ja	.L_last_num_blocks_is_16_wtBibnAfFctmwsA
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_wtBibnAfFctmwsA
	jmp	.L_last_num_blocks_is_13_wtBibnAfFctmwsA

.L_last_num_blocks_is_11_9_wtBibnAfFctmwsA:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_wtBibnAfFctmwsA
	ja	.L_last_num_blocks_is_11_wtBibnAfFctmwsA
	jmp	.L_last_num_blocks_is_9_wtBibnAfFctmwsA

.L_last_num_blocks_is_7_1_wtBibnAfFctmwsA:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_wtBibnAfFctmwsA
	jb	.L_last_num_blocks_is_3_1_wtBibnAfFctmwsA

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_wtBibnAfFctmwsA
	je	.L_last_num_blocks_is_6_wtBibnAfFctmwsA
	jmp	.L_last_num_blocks_is_5_wtBibnAfFctmwsA

.L_last_num_blocks_is_3_1_wtBibnAfFctmwsA:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_wtBibnAfFctmwsA
	je	.L_last_num_blocks_is_2_wtBibnAfFctmwsA
.L_last_num_blocks_is_1_wtBibnAfFctmwsA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_hnBuErflmjwcCjc
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_hnBuErflmjwcCjc

.L_16_blocks_overflow_hnBuErflmjwcCjc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_hnBuErflmjwcCjc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dGlxruyalDpwDio





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dGlxruyalDpwDio
.L_small_initial_partial_block_dGlxruyalDpwDio:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_dGlxruyalDpwDio
.L_small_initial_compute_done_dGlxruyalDpwDio:
.L_after_reduction_dGlxruyalDpwDio:
	jmp	.L_last_blocks_done_wtBibnAfFctmwsA
.L_last_num_blocks_is_2_wtBibnAfFctmwsA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_xDdgBcgwouFbDov
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_xDdgBcgwouFbDov

.L_16_blocks_overflow_xDdgBcgwouFbDov:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_xDdgBcgwouFbDov:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_oesEsDayxDtqcju





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_oesEsDayxDtqcju
.L_small_initial_partial_block_oesEsDayxDtqcju:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_oesEsDayxDtqcju:

	orq	%r8,%r8
	je	.L_after_reduction_oesEsDayxDtqcju
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_oesEsDayxDtqcju:
	jmp	.L_last_blocks_done_wtBibnAfFctmwsA
.L_last_num_blocks_is_3_wtBibnAfFctmwsA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_gdAxxDzxvpFwfpA
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_gdAxxDzxvpFwfpA

.L_16_blocks_overflow_gdAxxDzxvpFwfpA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_gdAxxDzxvpFwfpA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gbwhsyAopnrfyAA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gbwhsyAopnrfyAA
.L_small_initial_partial_block_gbwhsyAopnrfyAA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gbwhsyAopnrfyAA:

	orq	%r8,%r8
	je	.L_after_reduction_gbwhsyAopnrfyAA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gbwhsyAopnrfyAA:
	jmp	.L_last_blocks_done_wtBibnAfFctmwsA
.L_last_num_blocks_is_4_wtBibnAfFctmwsA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_mphyxygoEFilrGj
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_mphyxygoEFilrGj

.L_16_blocks_overflow_mphyxygoEFilrGj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_mphyxygoEFilrGj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EoqraFaFkAqgypa





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EoqraFaFkAqgypa
.L_small_initial_partial_block_EoqraFaFkAqgypa:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EoqraFaFkAqgypa:

	orq	%r8,%r8
	je	.L_after_reduction_EoqraFaFkAqgypa
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_EoqraFaFkAqgypa:
	jmp	.L_last_blocks_done_wtBibnAfFctmwsA
.L_last_num_blocks_is_5_wtBibnAfFctmwsA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_upDGEvDreEuxGEa
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_upDGEvDreEuxGEa

.L_16_blocks_overflow_upDGEvDreEuxGEa:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_upDGEvDreEuxGEa:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rFuiyzzzgnclFga





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rFuiyzzzgnclFga
.L_small_initial_partial_block_rFuiyzzzgnclFga:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rFuiyzzzgnclFga:

	orq	%r8,%r8
	je	.L_after_reduction_rFuiyzzzgnclFga
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rFuiyzzzgnclFga:
	jmp	.L_last_blocks_done_wtBibnAfFctmwsA
.L_last_num_blocks_is_6_wtBibnAfFctmwsA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_sBozhmimhFaifBr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_sBozhmimhFaifBr

.L_16_blocks_overflow_sBozhmimhFaifBr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_sBozhmimhFaifBr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_givhcgqenrbBliz





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_givhcgqenrbBliz
.L_small_initial_partial_block_givhcgqenrbBliz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_givhcgqenrbBliz:

	orq	%r8,%r8
	je	.L_after_reduction_givhcgqenrbBliz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_givhcgqenrbBliz:
	jmp	.L_last_blocks_done_wtBibnAfFctmwsA
.L_last_num_blocks_is_7_wtBibnAfFctmwsA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_BjcpsfjszxiyvDq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_BjcpsfjszxiyvDq

.L_16_blocks_overflow_BjcpsfjszxiyvDq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_BjcpsfjszxiyvDq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_oviDuGFDgzgtAEy





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_oviDuGFDgzgtAEy
.L_small_initial_partial_block_oviDuGFDgzgtAEy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_oviDuGFDgzgtAEy:

	orq	%r8,%r8
	je	.L_after_reduction_oviDuGFDgzgtAEy
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_oviDuGFDgzgtAEy:
	jmp	.L_last_blocks_done_wtBibnAfFctmwsA
.L_last_num_blocks_is_8_wtBibnAfFctmwsA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_rpFxkroFEBFwrat
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_rpFxkroFEBFwrat

.L_16_blocks_overflow_rpFxkroFEBFwrat:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_rpFxkroFEBFwrat:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_auuBEegbojtknkm





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_auuBEegbojtknkm
.L_small_initial_partial_block_auuBEegbojtknkm:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_auuBEegbojtknkm:

	orq	%r8,%r8
	je	.L_after_reduction_auuBEegbojtknkm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_auuBEegbojtknkm:
	jmp	.L_last_blocks_done_wtBibnAfFctmwsA
.L_last_num_blocks_is_9_wtBibnAfFctmwsA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_gvurapkEmvdEjrg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_gvurapkEmvdEjrg

.L_16_blocks_overflow_gvurapkEmvdEjrg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_gvurapkEmvdEjrg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ydGEznxieACzwkq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ydGEznxieACzwkq
.L_small_initial_partial_block_ydGEznxieACzwkq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ydGEznxieACzwkq:

	orq	%r8,%r8
	je	.L_after_reduction_ydGEznxieACzwkq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ydGEznxieACzwkq:
	jmp	.L_last_blocks_done_wtBibnAfFctmwsA
.L_last_num_blocks_is_10_wtBibnAfFctmwsA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_dAqyxvmruuguDna
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_dAqyxvmruuguDna

.L_16_blocks_overflow_dAqyxvmruuguDna:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_dAqyxvmruuguDna:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ABtqdgxdqGkaazx





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ABtqdgxdqGkaazx
.L_small_initial_partial_block_ABtqdgxdqGkaazx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ABtqdgxdqGkaazx:

	orq	%r8,%r8
	je	.L_after_reduction_ABtqdgxdqGkaazx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ABtqdgxdqGkaazx:
	jmp	.L_last_blocks_done_wtBibnAfFctmwsA
.L_last_num_blocks_is_11_wtBibnAfFctmwsA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_AzEADkpzibyarqC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_AzEADkpzibyarqC

.L_16_blocks_overflow_AzEADkpzibyarqC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_AzEADkpzibyarqC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yDeqBzxoahDxhwr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yDeqBzxoahDxhwr
.L_small_initial_partial_block_yDeqBzxoahDxhwr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yDeqBzxoahDxhwr:

	orq	%r8,%r8
	je	.L_after_reduction_yDeqBzxoahDxhwr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_yDeqBzxoahDxhwr:
	jmp	.L_last_blocks_done_wtBibnAfFctmwsA
.L_last_num_blocks_is_12_wtBibnAfFctmwsA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_nrBGiirAiGfDazb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_nrBGiirAiGfDazb

.L_16_blocks_overflow_nrBGiirAiGfDazb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_nrBGiirAiGfDazb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iizxnEGxlnjCcyA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iizxnEGxlnjCcyA
.L_small_initial_partial_block_iizxnEGxlnjCcyA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iizxnEGxlnjCcyA:

	orq	%r8,%r8
	je	.L_after_reduction_iizxnEGxlnjCcyA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_iizxnEGxlnjCcyA:
	jmp	.L_last_blocks_done_wtBibnAfFctmwsA
.L_last_num_blocks_is_13_wtBibnAfFctmwsA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_zGgCpqqzCqajeEj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_zGgCpqqzCqajeEj

.L_16_blocks_overflow_zGgCpqqzCqajeEj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_zGgCpqqzCqajeEj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rhcFnnsedmwfyck





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rhcFnnsedmwfyck
.L_small_initial_partial_block_rhcFnnsedmwfyck:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rhcFnnsedmwfyck:

	orq	%r8,%r8
	je	.L_after_reduction_rhcFnnsedmwfyck
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rhcFnnsedmwfyck:
	jmp	.L_last_blocks_done_wtBibnAfFctmwsA
.L_last_num_blocks_is_14_wtBibnAfFctmwsA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_FvcmkglqDjwEqcz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_FvcmkglqDjwEqcz

.L_16_blocks_overflow_FvcmkglqDjwEqcz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_FvcmkglqDjwEqcz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_eaibxAfqraFzgdr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_eaibxAfqraFzgdr
.L_small_initial_partial_block_eaibxAfqraFzgdr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_eaibxAfqraFzgdr:

	orq	%r8,%r8
	je	.L_after_reduction_eaibxAfqraFzgdr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_eaibxAfqraFzgdr:
	jmp	.L_last_blocks_done_wtBibnAfFctmwsA
.L_last_num_blocks_is_15_wtBibnAfFctmwsA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_GgACbDhrrpFrrwm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_GgACbDhrrpFrrwm

.L_16_blocks_overflow_GgACbDhrrpFrrwm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_GgACbDhrrpFrrwm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pArbGhzGzAilDku





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pArbGhzGzAilDku
.L_small_initial_partial_block_pArbGhzGzAilDku:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pArbGhzGzAilDku:

	orq	%r8,%r8
	je	.L_after_reduction_pArbGhzGzAilDku
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_pArbGhzGzAilDku:
	jmp	.L_last_blocks_done_wtBibnAfFctmwsA
.L_last_num_blocks_is_16_wtBibnAfFctmwsA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_axzgCBpfBqrCjyd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_axzgCBpfBqrCjyd

.L_16_blocks_overflow_axzgCBpfBqrCjyd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_axzgCBpfBqrCjyd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_dnyofbqthsuhpDr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dnyofbqthsuhpDr:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dnyofbqthsuhpDr:
	jmp	.L_last_blocks_done_wtBibnAfFctmwsA
.L_last_num_blocks_is_0_wtBibnAfFctmwsA:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_wtBibnAfFctmwsA:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_cDwywBGkADyfvEf
.L_encrypt_32_blocks_cDwywBGkADyfvEf:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_cnypxikCqfwuhiD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_cnypxikCqfwuhiD
.L_16_blocks_overflow_cnypxikCqfwuhiD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_cnypxikCqfwuhiD:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_iFcsdkAisxigloy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_iFcsdkAisxigloy
.L_16_blocks_overflow_iFcsdkAisxigloy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_iFcsdkAisxigloy:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%r11,1),%zmm17
	vmovdqu8	320(%rcx,%r11,1),%zmm19
	vmovdqu8	384(%rcx,%r11,1),%zmm20
	vmovdqu8	448(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%r11,1)
	vmovdqu8	%zmm3,320(%r10,%r11,1)
	vmovdqu8	%zmm4,384(%r10,%r11,1)
	vmovdqu8	%zmm5,448(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%r11
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_BgdtDlbsjFjbfFC

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_BgdtDlbsjFjbfFC
	jb	.L_last_num_blocks_is_7_1_BgdtDlbsjFjbfFC


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_BgdtDlbsjFjbfFC
	jb	.L_last_num_blocks_is_11_9_BgdtDlbsjFjbfFC


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_BgdtDlbsjFjbfFC
	ja	.L_last_num_blocks_is_16_BgdtDlbsjFjbfFC
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_BgdtDlbsjFjbfFC
	jmp	.L_last_num_blocks_is_13_BgdtDlbsjFjbfFC

.L_last_num_blocks_is_11_9_BgdtDlbsjFjbfFC:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_BgdtDlbsjFjbfFC
	ja	.L_last_num_blocks_is_11_BgdtDlbsjFjbfFC
	jmp	.L_last_num_blocks_is_9_BgdtDlbsjFjbfFC

.L_last_num_blocks_is_7_1_BgdtDlbsjFjbfFC:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_BgdtDlbsjFjbfFC
	jb	.L_last_num_blocks_is_3_1_BgdtDlbsjFjbfFC

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_BgdtDlbsjFjbfFC
	je	.L_last_num_blocks_is_6_BgdtDlbsjFjbfFC
	jmp	.L_last_num_blocks_is_5_BgdtDlbsjFjbfFC

.L_last_num_blocks_is_3_1_BgdtDlbsjFjbfFC:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_BgdtDlbsjFjbfFC
	je	.L_last_num_blocks_is_2_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_1_BgdtDlbsjFjbfFC:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_jcbbwlAjmnryegG
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_jcbbwlAjmnryegG

.L_16_blocks_overflow_jcbbwlAjmnryegG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_jcbbwlAjmnryegG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fglfrECvEiEbEas





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fglfrECvEiEbEas
.L_small_initial_partial_block_fglfrECvEiEbEas:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_fglfrECvEiEbEas
.L_small_initial_compute_done_fglfrECvEiEbEas:
.L_after_reduction_fglfrECvEiEbEas:
	jmp	.L_last_blocks_done_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_2_BgdtDlbsjFjbfFC:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_qklptyjdvxzqgug
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_qklptyjdvxzqgug

.L_16_blocks_overflow_qklptyjdvxzqgug:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_qklptyjdvxzqgug:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kvnsntjoiFbstsh





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kvnsntjoiFbstsh
.L_small_initial_partial_block_kvnsntjoiFbstsh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kvnsntjoiFbstsh:

	orq	%r8,%r8
	je	.L_after_reduction_kvnsntjoiFbstsh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kvnsntjoiFbstsh:
	jmp	.L_last_blocks_done_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_3_BgdtDlbsjFjbfFC:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_aBpacfkvkhyyyAu
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_aBpacfkvkhyyyAu

.L_16_blocks_overflow_aBpacfkvkhyyyAu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_aBpacfkvkhyyyAu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_mifurCAjmbspvwu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_mifurCAjmbspvwu
.L_small_initial_partial_block_mifurCAjmbspvwu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mifurCAjmbspvwu:

	orq	%r8,%r8
	je	.L_after_reduction_mifurCAjmbspvwu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_mifurCAjmbspvwu:
	jmp	.L_last_blocks_done_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_4_BgdtDlbsjFjbfFC:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_utnuerosqBktwBo
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_utnuerosqBktwBo

.L_16_blocks_overflow_utnuerosqBktwBo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_utnuerosqBktwBo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_eeEFoAborihyEyD





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_eeEFoAborihyEyD
.L_small_initial_partial_block_eeEFoAborihyEyD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_eeEFoAborihyEyD:

	orq	%r8,%r8
	je	.L_after_reduction_eeEFoAborihyEyD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_eeEFoAborihyEyD:
	jmp	.L_last_blocks_done_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_5_BgdtDlbsjFjbfFC:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_cGgAsGlqwjyFCCE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_cGgAsGlqwjyFCCE

.L_16_blocks_overflow_cGgAsGlqwjyFCCE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_cGgAsGlqwjyFCCE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nhBsAmsBjvdEpAr





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nhBsAmsBjvdEpAr
.L_small_initial_partial_block_nhBsAmsBjvdEpAr:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nhBsAmsBjvdEpAr:

	orq	%r8,%r8
	je	.L_after_reduction_nhBsAmsBjvdEpAr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_nhBsAmsBjvdEpAr:
	jmp	.L_last_blocks_done_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_6_BgdtDlbsjFjbfFC:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_mzyGeFkaCmGhaxx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_mzyGeFkaCmGhaxx

.L_16_blocks_overflow_mzyGeFkaCmGhaxx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_mzyGeFkaCmGhaxx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FvDxiorBkmCyrin





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FvDxiorBkmCyrin
.L_small_initial_partial_block_FvDxiorBkmCyrin:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FvDxiorBkmCyrin:

	orq	%r8,%r8
	je	.L_after_reduction_FvDxiorBkmCyrin
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FvDxiorBkmCyrin:
	jmp	.L_last_blocks_done_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_7_BgdtDlbsjFjbfFC:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_tGwqsskerqkuDle
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_tGwqsskerqkuDle

.L_16_blocks_overflow_tGwqsskerqkuDle:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_tGwqsskerqkuDle:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jDpAzoEwyCwhayE





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jDpAzoEwyCwhayE
.L_small_initial_partial_block_jDpAzoEwyCwhayE:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jDpAzoEwyCwhayE:

	orq	%r8,%r8
	je	.L_after_reduction_jDpAzoEwyCwhayE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jDpAzoEwyCwhayE:
	jmp	.L_last_blocks_done_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_8_BgdtDlbsjFjbfFC:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_unohvdDsepjiFji
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_unohvdDsepjiFji

.L_16_blocks_overflow_unohvdDsepjiFji:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_unohvdDsepjiFji:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ryFGhvAvmGejDqu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ryFGhvAvmGejDqu
.L_small_initial_partial_block_ryFGhvAvmGejDqu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ryFGhvAvmGejDqu:

	orq	%r8,%r8
	je	.L_after_reduction_ryFGhvAvmGejDqu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ryFGhvAvmGejDqu:
	jmp	.L_last_blocks_done_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_9_BgdtDlbsjFjbfFC:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_dChbFkBBdFuCmza
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_dChbFkBBdFuCmza

.L_16_blocks_overflow_dChbFkBBdFuCmza:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_dChbFkBBdFuCmza:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tkoEAcwrtsluhwt





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tkoEAcwrtsluhwt
.L_small_initial_partial_block_tkoEAcwrtsluhwt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tkoEAcwrtsluhwt:

	orq	%r8,%r8
	je	.L_after_reduction_tkoEAcwrtsluhwt
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tkoEAcwrtsluhwt:
	jmp	.L_last_blocks_done_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_10_BgdtDlbsjFjbfFC:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_shxzvlrfkuotBCk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_shxzvlrfkuotBCk

.L_16_blocks_overflow_shxzvlrfkuotBCk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_shxzvlrfkuotBCk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kCwhlgxmrCibCpq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kCwhlgxmrCibCpq
.L_small_initial_partial_block_kCwhlgxmrCibCpq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kCwhlgxmrCibCpq:

	orq	%r8,%r8
	je	.L_after_reduction_kCwhlgxmrCibCpq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kCwhlgxmrCibCpq:
	jmp	.L_last_blocks_done_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_11_BgdtDlbsjFjbfFC:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_uciwnpExhBBkygq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_uciwnpExhBBkygq

.L_16_blocks_overflow_uciwnpExhBBkygq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_uciwnpExhBBkygq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ssixciAghhnzipx





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ssixciAghhnzipx
.L_small_initial_partial_block_ssixciAghhnzipx:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ssixciAghhnzipx:

	orq	%r8,%r8
	je	.L_after_reduction_ssixciAghhnzipx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ssixciAghhnzipx:
	jmp	.L_last_blocks_done_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_12_BgdtDlbsjFjbfFC:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_DawFvAaGpzExbas
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_DawFvAaGpzExbas

.L_16_blocks_overflow_DawFvAaGpzExbas:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_DawFvAaGpzExbas:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DmnvfmxcuvAylAq





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DmnvfmxcuvAylAq
.L_small_initial_partial_block_DmnvfmxcuvAylAq:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DmnvfmxcuvAylAq:

	orq	%r8,%r8
	je	.L_after_reduction_DmnvfmxcuvAylAq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DmnvfmxcuvAylAq:
	jmp	.L_last_blocks_done_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_13_BgdtDlbsjFjbfFC:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_eoaBCuguDwtjCqv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_eoaBCuguDwtjCqv

.L_16_blocks_overflow_eoaBCuguDwtjCqv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_eoaBCuguDwtjCqv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_idwwfdDfvpidFbp





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_idwwfdDfvpidFbp
.L_small_initial_partial_block_idwwfdDfvpidFbp:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_idwwfdDfvpidFbp:

	orq	%r8,%r8
	je	.L_after_reduction_idwwfdDfvpidFbp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_idwwfdDfvpidFbp:
	jmp	.L_last_blocks_done_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_14_BgdtDlbsjFjbfFC:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_DqbnbgGiryBojvi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_DqbnbgGiryBojvi

.L_16_blocks_overflow_DqbnbgGiryBojvi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_DqbnbgGiryBojvi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CCjqFdwlpgGBBbA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CCjqFdwlpgGBBbA
.L_small_initial_partial_block_CCjqFdwlpgGBBbA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CCjqFdwlpgGBBbA:

	orq	%r8,%r8
	je	.L_after_reduction_CCjqFdwlpgGBBbA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CCjqFdwlpgGBBbA:
	jmp	.L_last_blocks_done_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_15_BgdtDlbsjFjbfFC:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_AAeGyzxtwskdwhb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_AAeGyzxtwskdwhb

.L_16_blocks_overflow_AAeGyzxtwskdwhb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_AAeGyzxtwskdwhb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nrgwFepgAuGyDsu





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nrgwFepgAuGyDsu
.L_small_initial_partial_block_nrgwFepgAuGyDsu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nrgwFepgAuGyDsu:

	orq	%r8,%r8
	je	.L_after_reduction_nrgwFepgAuGyDsu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_nrgwFepgAuGyDsu:
	jmp	.L_last_blocks_done_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_16_BgdtDlbsjFjbfFC:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_qnAEekGvFcAcdcd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_qnAEekGvFcAcdcd

.L_16_blocks_overflow_qnAEekGvFcAcdcd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_qnAEekGvFcAcdcd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_ygypssGaselvnAs:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ygypssGaselvnAs:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ygypssGaselvnAs:
	jmp	.L_last_blocks_done_BgdtDlbsjFjbfFC
.L_last_num_blocks_is_0_BgdtDlbsjFjbfFC:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_BgdtDlbsjFjbfFC:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_cDwywBGkADyfvEf
.L_encrypt_16_blocks_cDwywBGkADyfvEf:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_cxjrpvssmrrjFpd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_cxjrpvssmrrjFpd
.L_16_blocks_overflow_cxjrpvssmrrjFpd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_cxjrpvssmrrjFpd:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_CkolwfeBymDxAnh

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_CkolwfeBymDxAnh
	jb	.L_last_num_blocks_is_7_1_CkolwfeBymDxAnh


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_CkolwfeBymDxAnh
	jb	.L_last_num_blocks_is_11_9_CkolwfeBymDxAnh


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_CkolwfeBymDxAnh
	ja	.L_last_num_blocks_is_16_CkolwfeBymDxAnh
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_CkolwfeBymDxAnh
	jmp	.L_last_num_blocks_is_13_CkolwfeBymDxAnh

.L_last_num_blocks_is_11_9_CkolwfeBymDxAnh:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_CkolwfeBymDxAnh
	ja	.L_last_num_blocks_is_11_CkolwfeBymDxAnh
	jmp	.L_last_num_blocks_is_9_CkolwfeBymDxAnh

.L_last_num_blocks_is_7_1_CkolwfeBymDxAnh:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_CkolwfeBymDxAnh
	jb	.L_last_num_blocks_is_3_1_CkolwfeBymDxAnh

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_CkolwfeBymDxAnh
	je	.L_last_num_blocks_is_6_CkolwfeBymDxAnh
	jmp	.L_last_num_blocks_is_5_CkolwfeBymDxAnh

.L_last_num_blocks_is_3_1_CkolwfeBymDxAnh:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_CkolwfeBymDxAnh
	je	.L_last_num_blocks_is_2_CkolwfeBymDxAnh
.L_last_num_blocks_is_1_CkolwfeBymDxAnh:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_Goqwwnrbghtpfdj
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_Goqwwnrbghtpfdj

.L_16_blocks_overflow_Goqwwnrbghtpfdj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_Goqwwnrbghtpfdj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BAgAtfkxCulejgn





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BAgAtfkxCulejgn
.L_small_initial_partial_block_BAgAtfkxCulejgn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_BAgAtfkxCulejgn
.L_small_initial_compute_done_BAgAtfkxCulejgn:
.L_after_reduction_BAgAtfkxCulejgn:
	jmp	.L_last_blocks_done_CkolwfeBymDxAnh
.L_last_num_blocks_is_2_CkolwfeBymDxAnh:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_zGnkgfxwwaczCwo
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_zGnkgfxwwaczCwo

.L_16_blocks_overflow_zGnkgfxwwaczCwo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_zGnkgfxwwaczCwo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GipAjCzrwxjilah





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GipAjCzrwxjilah
.L_small_initial_partial_block_GipAjCzrwxjilah:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GipAjCzrwxjilah:

	orq	%r8,%r8
	je	.L_after_reduction_GipAjCzrwxjilah
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GipAjCzrwxjilah:
	jmp	.L_last_blocks_done_CkolwfeBymDxAnh
.L_last_num_blocks_is_3_CkolwfeBymDxAnh:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_CDFGdrgkyjjohwD
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_CDFGdrgkyjjohwD

.L_16_blocks_overflow_CDFGdrgkyjjohwD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_CDFGdrgkyjjohwD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cGlBrohqnvEsCpy





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cGlBrohqnvEsCpy
.L_small_initial_partial_block_cGlBrohqnvEsCpy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cGlBrohqnvEsCpy:

	orq	%r8,%r8
	je	.L_after_reduction_cGlBrohqnvEsCpy
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cGlBrohqnvEsCpy:
	jmp	.L_last_blocks_done_CkolwfeBymDxAnh
.L_last_num_blocks_is_4_CkolwfeBymDxAnh:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_boysaojnzpDAhzn
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_boysaojnzpDAhzn

.L_16_blocks_overflow_boysaojnzpDAhzn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_boysaojnzpDAhzn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lcDDowbiyEwzzwj





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lcDDowbiyEwzzwj
.L_small_initial_partial_block_lcDDowbiyEwzzwj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lcDDowbiyEwzzwj:

	orq	%r8,%r8
	je	.L_after_reduction_lcDDowbiyEwzzwj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lcDDowbiyEwzzwj:
	jmp	.L_last_blocks_done_CkolwfeBymDxAnh
.L_last_num_blocks_is_5_CkolwfeBymDxAnh:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_FszFbEnGcpvtytw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_FszFbEnGcpvtytw

.L_16_blocks_overflow_FszFbEnGcpvtytw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_FszFbEnGcpvtytw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ivwergbtGvDhlbt





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ivwergbtGvDhlbt
.L_small_initial_partial_block_ivwergbtGvDhlbt:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ivwergbtGvDhlbt:

	orq	%r8,%r8
	je	.L_after_reduction_ivwergbtGvDhlbt
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ivwergbtGvDhlbt:
	jmp	.L_last_blocks_done_CkolwfeBymDxAnh
.L_last_num_blocks_is_6_CkolwfeBymDxAnh:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_joxujmmCbBkdczs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_joxujmmCbBkdczs

.L_16_blocks_overflow_joxujmmCbBkdczs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_joxujmmCbBkdczs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nbBufbknabnpveo





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nbBufbknabnpveo
.L_small_initial_partial_block_nbBufbknabnpveo:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nbBufbknabnpveo:

	orq	%r8,%r8
	je	.L_after_reduction_nbBufbknabnpveo
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_nbBufbknabnpveo:
	jmp	.L_last_blocks_done_CkolwfeBymDxAnh
.L_last_num_blocks_is_7_CkolwfeBymDxAnh:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_tpijBdqBknxavzf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_tpijBdqBknxavzf

.L_16_blocks_overflow_tpijBdqBknxavzf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_tpijBdqBknxavzf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_umwAazDmrqncdik





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_umwAazDmrqncdik
.L_small_initial_partial_block_umwAazDmrqncdik:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_umwAazDmrqncdik:

	orq	%r8,%r8
	je	.L_after_reduction_umwAazDmrqncdik
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_umwAazDmrqncdik:
	jmp	.L_last_blocks_done_CkolwfeBymDxAnh
.L_last_num_blocks_is_8_CkolwfeBymDxAnh:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_pjDesnCilyiuigy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_pjDesnCilyiuigy

.L_16_blocks_overflow_pjDesnCilyiuigy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_pjDesnCilyiuigy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CEzFqtdifarstdD





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CEzFqtdifarstdD
.L_small_initial_partial_block_CEzFqtdifarstdD:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CEzFqtdifarstdD:

	orq	%r8,%r8
	je	.L_after_reduction_CEzFqtdifarstdD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CEzFqtdifarstdD:
	jmp	.L_last_blocks_done_CkolwfeBymDxAnh
.L_last_num_blocks_is_9_CkolwfeBymDxAnh:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_EqDuqAsoFzmsxvv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_EqDuqAsoFzmsxvv

.L_16_blocks_overflow_EqDuqAsoFzmsxvv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_EqDuqAsoFzmsxvv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FBezfFeffcxkvcA





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FBezfFeffcxkvcA
.L_small_initial_partial_block_FBezfFeffcxkvcA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FBezfFeffcxkvcA:

	orq	%r8,%r8
	je	.L_after_reduction_FBezfFeffcxkvcA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FBezfFeffcxkvcA:
	jmp	.L_last_blocks_done_CkolwfeBymDxAnh
.L_last_num_blocks_is_10_CkolwfeBymDxAnh:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_huGmojCAjbhkghb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_huGmojCAjbhkghb

.L_16_blocks_overflow_huGmojCAjbhkghb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_huGmojCAjbhkghb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rDpksGuehrilAiz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rDpksGuehrilAiz
.L_small_initial_partial_block_rDpksGuehrilAiz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rDpksGuehrilAiz:

	orq	%r8,%r8
	je	.L_after_reduction_rDpksGuehrilAiz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rDpksGuehrilAiz:
	jmp	.L_last_blocks_done_CkolwfeBymDxAnh
.L_last_num_blocks_is_11_CkolwfeBymDxAnh:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_aykAFgfqwgdheCg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_aykAFgfqwgdheCg

.L_16_blocks_overflow_aykAFgfqwgdheCg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_aykAFgfqwgdheCg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ttdgjvFFCmtbniz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ttdgjvFFCmtbniz
.L_small_initial_partial_block_ttdgjvFFCmtbniz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ttdgjvFFCmtbniz:

	orq	%r8,%r8
	je	.L_after_reduction_ttdgjvFFCmtbniz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ttdgjvFFCmtbniz:
	jmp	.L_last_blocks_done_CkolwfeBymDxAnh
.L_last_num_blocks_is_12_CkolwfeBymDxAnh:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_sphDvEsfxhieyua
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_sphDvEsfxhieyua

.L_16_blocks_overflow_sphDvEsfxhieyua:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_sphDvEsfxhieyua:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dlrcbxepaEhFwxb





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dlrcbxepaEhFwxb
.L_small_initial_partial_block_dlrcbxepaEhFwxb:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dlrcbxepaEhFwxb:

	orq	%r8,%r8
	je	.L_after_reduction_dlrcbxepaEhFwxb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dlrcbxepaEhFwxb:
	jmp	.L_last_blocks_done_CkolwfeBymDxAnh
.L_last_num_blocks_is_13_CkolwfeBymDxAnh:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_FsssGjqnfvzzjmr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_FsssGjqnfvzzjmr

.L_16_blocks_overflow_FsssGjqnfvzzjmr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_FsssGjqnfvzzjmr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_otwbdqyloDayqty





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_otwbdqyloDayqty
.L_small_initial_partial_block_otwbdqyloDayqty:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_otwbdqyloDayqty:

	orq	%r8,%r8
	je	.L_after_reduction_otwbdqyloDayqty
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_otwbdqyloDayqty:
	jmp	.L_last_blocks_done_CkolwfeBymDxAnh
.L_last_num_blocks_is_14_CkolwfeBymDxAnh:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_fFftFprqGfzvEwy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_fFftFprqGfzvEwy

.L_16_blocks_overflow_fFftFprqGfzvEwy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_fFftFprqGfzvEwy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_eGmxpcoqfqBFDeu





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_eGmxpcoqfqBFDeu
.L_small_initial_partial_block_eGmxpcoqfqBFDeu:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_eGmxpcoqfqBFDeu:

	orq	%r8,%r8
	je	.L_after_reduction_eGmxpcoqfqBFDeu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_eGmxpcoqfqBFDeu:
	jmp	.L_last_blocks_done_CkolwfeBymDxAnh
.L_last_num_blocks_is_15_CkolwfeBymDxAnh:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_DklyCyofFDbbnwA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_DklyCyofFDbbnwA

.L_16_blocks_overflow_DklyCyofFDbbnwA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_DklyCyofFDbbnwA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ynuwbGtFnFDCwoj





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ynuwbGtFnFDCwoj
.L_small_initial_partial_block_ynuwbGtFnFDCwoj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ynuwbGtFnFDCwoj:

	orq	%r8,%r8
	je	.L_after_reduction_ynuwbGtFnFDCwoj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ynuwbGtFnFDCwoj:
	jmp	.L_last_blocks_done_CkolwfeBymDxAnh
.L_last_num_blocks_is_16_CkolwfeBymDxAnh:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_DsiAmcjshvbfxub
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_DsiAmcjshvbfxub

.L_16_blocks_overflow_DsiAmcjshvbfxub:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_DsiAmcjshvbfxub:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_yFffqpuyyBlcdvi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yFffqpuyyBlcdvi:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_yFffqpuyyBlcdvi:
	jmp	.L_last_blocks_done_CkolwfeBymDxAnh
.L_last_num_blocks_is_0_CkolwfeBymDxAnh:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_CkolwfeBymDxAnh:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_cDwywBGkADyfvEf

.L_message_below_32_blocks_cDwywBGkADyfvEf:


	subq	$256,%r8
	addq	$256,%r11
	movl	%r8d,%r10d
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_tutzmfoDrulDodj
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
.L_skip_hkeys_precomputation_tutzmfoDrulDodj:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_DEcfkajCuyGflBl

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_DEcfkajCuyGflBl
	jb	.L_last_num_blocks_is_7_1_DEcfkajCuyGflBl


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_DEcfkajCuyGflBl
	jb	.L_last_num_blocks_is_11_9_DEcfkajCuyGflBl


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_DEcfkajCuyGflBl
	ja	.L_last_num_blocks_is_16_DEcfkajCuyGflBl
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_DEcfkajCuyGflBl
	jmp	.L_last_num_blocks_is_13_DEcfkajCuyGflBl

.L_last_num_blocks_is_11_9_DEcfkajCuyGflBl:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_DEcfkajCuyGflBl
	ja	.L_last_num_blocks_is_11_DEcfkajCuyGflBl
	jmp	.L_last_num_blocks_is_9_DEcfkajCuyGflBl

.L_last_num_blocks_is_7_1_DEcfkajCuyGflBl:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_DEcfkajCuyGflBl
	jb	.L_last_num_blocks_is_3_1_DEcfkajCuyGflBl

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_DEcfkajCuyGflBl
	je	.L_last_num_blocks_is_6_DEcfkajCuyGflBl
	jmp	.L_last_num_blocks_is_5_DEcfkajCuyGflBl

.L_last_num_blocks_is_3_1_DEcfkajCuyGflBl:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_DEcfkajCuyGflBl
	je	.L_last_num_blocks_is_2_DEcfkajCuyGflBl
.L_last_num_blocks_is_1_DEcfkajCuyGflBl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_rhqhleoBAadeEzC
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_rhqhleoBAadeEzC

.L_16_blocks_overflow_rhqhleoBAadeEzC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_rhqhleoBAadeEzC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qdnjqdkDnDhmaEA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qdnjqdkDnDhmaEA
.L_small_initial_partial_block_qdnjqdkDnDhmaEA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_qdnjqdkDnDhmaEA
.L_small_initial_compute_done_qdnjqdkDnDhmaEA:
.L_after_reduction_qdnjqdkDnDhmaEA:
	jmp	.L_last_blocks_done_DEcfkajCuyGflBl
.L_last_num_blocks_is_2_DEcfkajCuyGflBl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_CwxphwFmjusiiwg
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_CwxphwFmjusiiwg

.L_16_blocks_overflow_CwxphwFmjusiiwg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_CwxphwFmjusiiwg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dkrbnuFwDnqgxrA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dkrbnuFwDnqgxrA
.L_small_initial_partial_block_dkrbnuFwDnqgxrA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dkrbnuFwDnqgxrA:

	orq	%r8,%r8
	je	.L_after_reduction_dkrbnuFwDnqgxrA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dkrbnuFwDnqgxrA:
	jmp	.L_last_blocks_done_DEcfkajCuyGflBl
.L_last_num_blocks_is_3_DEcfkajCuyGflBl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_miwqsqqbcEEAbdd
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_miwqsqqbcEEAbdd

.L_16_blocks_overflow_miwqsqqbcEEAbdd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_miwqsqqbcEEAbdd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ipwDhrhsrxxcimh





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ipwDhrhsrxxcimh
.L_small_initial_partial_block_ipwDhrhsrxxcimh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ipwDhrhsrxxcimh:

	orq	%r8,%r8
	je	.L_after_reduction_ipwDhrhsrxxcimh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ipwDhrhsrxxcimh:
	jmp	.L_last_blocks_done_DEcfkajCuyGflBl
.L_last_num_blocks_is_4_DEcfkajCuyGflBl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_DxsuntvfjjDgvnu
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_DxsuntvfjjDgvnu

.L_16_blocks_overflow_DxsuntvfjjDgvnu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_DxsuntvfjjDgvnu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_awhbyckquyngyri





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_awhbyckquyngyri
.L_small_initial_partial_block_awhbyckquyngyri:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_awhbyckquyngyri:

	orq	%r8,%r8
	je	.L_after_reduction_awhbyckquyngyri
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_awhbyckquyngyri:
	jmp	.L_last_blocks_done_DEcfkajCuyGflBl
.L_last_num_blocks_is_5_DEcfkajCuyGflBl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_vAGdkCtFgxDtCsq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_vAGdkCtFgxDtCsq

.L_16_blocks_overflow_vAGdkCtFgxDtCsq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_vAGdkCtFgxDtCsq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_oGbzbwFCcytwzFy





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_oGbzbwFCcytwzFy
.L_small_initial_partial_block_oGbzbwFCcytwzFy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_oGbzbwFCcytwzFy:

	orq	%r8,%r8
	je	.L_after_reduction_oGbzbwFCcytwzFy
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_oGbzbwFCcytwzFy:
	jmp	.L_last_blocks_done_DEcfkajCuyGflBl
.L_last_num_blocks_is_6_DEcfkajCuyGflBl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_pCByziGBaaaqaby
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_pCByziGBaaaqaby

.L_16_blocks_overflow_pCByziGBaaaqaby:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_pCByziGBaaaqaby:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pvthipijfpChewj





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pvthipijfpChewj
.L_small_initial_partial_block_pvthipijfpChewj:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pvthipijfpChewj:

	orq	%r8,%r8
	je	.L_after_reduction_pvthipijfpChewj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_pvthipijfpChewj:
	jmp	.L_last_blocks_done_DEcfkajCuyGflBl
.L_last_num_blocks_is_7_DEcfkajCuyGflBl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_GkhsyfzDuwcjxxh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_GkhsyfzDuwcjxxh

.L_16_blocks_overflow_GkhsyfzDuwcjxxh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_GkhsyfzDuwcjxxh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_aAluukpCAjwunyg





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_aAluukpCAjwunyg
.L_small_initial_partial_block_aAluukpCAjwunyg:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_aAluukpCAjwunyg:

	orq	%r8,%r8
	je	.L_after_reduction_aAluukpCAjwunyg
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_aAluukpCAjwunyg:
	jmp	.L_last_blocks_done_DEcfkajCuyGflBl
.L_last_num_blocks_is_8_DEcfkajCuyGflBl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$64,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_osDwlyscwcFjmvs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_osDwlyscwcFjmvs

.L_16_blocks_overflow_osDwlyscwcFjmvs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_osDwlyscwcFjmvs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GoGnuDhcwhnBbtz





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GoGnuDhcwhnBbtz
.L_small_initial_partial_block_GoGnuDhcwhnBbtz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GoGnuDhcwhnBbtz:

	orq	%r8,%r8
	je	.L_after_reduction_GoGnuDhcwhnBbtz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GoGnuDhcwhnBbtz:
	jmp	.L_last_blocks_done_DEcfkajCuyGflBl
.L_last_num_blocks_is_9_DEcfkajCuyGflBl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_vBblnkwDGkBoBrg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_vBblnkwDGkBoBrg

.L_16_blocks_overflow_vBblnkwDGkBoBrg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_vBblnkwDGkBoBrg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bqFybuAfEBDffEv





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bqFybuAfEBDffEv
.L_small_initial_partial_block_bqFybuAfEBDffEv:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bqFybuAfEBDffEv:

	orq	%r8,%r8
	je	.L_after_reduction_bqFybuAfEBDffEv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bqFybuAfEBDffEv:
	jmp	.L_last_blocks_done_DEcfkajCuyGflBl
.L_last_num_blocks_is_10_DEcfkajCuyGflBl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_xkzqnwxdrDfhGap
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_xkzqnwxdrDfhGap

.L_16_blocks_overflow_xkzqnwxdrDfhGap:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_xkzqnwxdrDfhGap:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jpxEwnEmgFEtvyn





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jpxEwnEmgFEtvyn
.L_small_initial_partial_block_jpxEwnEmgFEtvyn:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jpxEwnEmgFEtvyn:

	orq	%r8,%r8
	je	.L_after_reduction_jpxEwnEmgFEtvyn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jpxEwnEmgFEtvyn:
	jmp	.L_last_blocks_done_DEcfkajCuyGflBl
.L_last_num_blocks_is_11_DEcfkajCuyGflBl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_suBbsqesElkclmC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_suBbsqesElkclmC

.L_16_blocks_overflow_suBbsqesElkclmC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_suBbsqesElkclmC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_oDlhapEGrpaznvA





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_oDlhapEGrpaznvA
.L_small_initial_partial_block_oDlhapEGrpaznvA:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_oDlhapEGrpaznvA:

	orq	%r8,%r8
	je	.L_after_reduction_oDlhapEGrpaznvA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_oDlhapEGrpaznvA:
	jmp	.L_last_blocks_done_DEcfkajCuyGflBl
.L_last_num_blocks_is_12_DEcfkajCuyGflBl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$128,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_esivbdotGEnrenv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_esivbdotGEnrenv

.L_16_blocks_overflow_esivbdotGEnrenv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_esivbdotGEnrenv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_retwmqqxsBbfdGy





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_retwmqqxsBbfdGy
.L_small_initial_partial_block_retwmqqxsBbfdGy:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_retwmqqxsBbfdGy:

	orq	%r8,%r8
	je	.L_after_reduction_retwmqqxsBbfdGy
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_retwmqqxsBbfdGy:
	jmp	.L_last_blocks_done_DEcfkajCuyGflBl
.L_last_num_blocks_is_13_DEcfkajCuyGflBl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_DinrgfnubBqBguF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_DinrgfnubBqBguF

.L_16_blocks_overflow_DinrgfnubBqBguF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_DinrgfnubBqBguF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gbafyshjBAtvCBB





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gbafyshjBAtvCBB
.L_small_initial_partial_block_gbafyshjBAtvCBB:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	224(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	288(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gbafyshjBAtvCBB:

	orq	%r8,%r8
	je	.L_after_reduction_gbafyshjBAtvCBB
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gbafyshjBAtvCBB:
	jmp	.L_last_blocks_done_DEcfkajCuyGflBl
.L_last_num_blocks_is_14_DEcfkajCuyGflBl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_FyxBaClfGxhmqFo
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_FyxBaClfGxhmqFo

.L_16_blocks_overflow_FyxBaClfGxhmqFo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_FyxBaClfGxhmqFo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lEDBnbGhxlDDljs





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lEDBnbGhxlDDljs
.L_small_initial_partial_block_lEDBnbGhxlDDljs:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	208(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	272(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	336(%rsi),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lEDBnbGhxlDDljs:

	orq	%r8,%r8
	je	.L_after_reduction_lEDBnbGhxlDDljs
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lEDBnbGhxlDDljs:
	jmp	.L_last_blocks_done_DEcfkajCuyGflBl
.L_last_num_blocks_is_15_DEcfkajCuyGflBl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_DGCocFGqkyEFqyv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_DGCocFGqkyEFqyv

.L_16_blocks_overflow_DGCocFGqkyEFqyv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_DGCocFGqkyEFqyv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vsFDAmltkkwfvzl





	subq	$16,%r8
	movq	$0,(%rdx)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vsFDAmltkkwfvzl
.L_small_initial_partial_block_vsFDAmltkkwfvzl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	256(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	320(%rsi),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vsFDAmltkkwfvzl:

	orq	%r8,%r8
	je	.L_after_reduction_vsFDAmltkkwfvzl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vsFDAmltkkwfvzl:
	jmp	.L_last_blocks_done_DEcfkajCuyGflBl
.L_last_num_blocks_is_16_DEcfkajCuyGflBl:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%rax
	subq	$192,%rax
	kmovq	(%r10,%rax,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_AbFoFozuhnjdwxj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_AbFoFozuhnjdwxj

.L_16_blocks_overflow_AbFoFozuhnjdwxj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_AbFoFozuhnjdwxj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%r11,1),%zmm17
	vmovdqu8	64(%rcx,%r11,1),%zmm19
	vmovdqu8	128(%rcx,%r11,1),%zmm20
	vmovdqu8	192(%rcx,%r11,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_EAGDFntGAsFplgk:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	240(%rsi),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	304(%rsi),%ymm1
	vinserti64x2	$2,336(%rsi),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EAGDFntGAsFplgk:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_EAGDFntGAsFplgk:
	jmp	.L_last_blocks_done_DEcfkajCuyGflBl
.L_last_num_blocks_is_0_DEcfkajCuyGflBl:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_DEcfkajCuyGflBl:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_cDwywBGkADyfvEf

.L_message_below_equal_16_blocks_cDwywBGkADyfvEf:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	.L_small_initial_num_blocks_is_8_ekxzgamDsfossgm
	jl	.L_small_initial_num_blocks_is_7_1_ekxzgamDsfossgm


	cmpq	$12,%r12
	je	.L_small_initial_num_blocks_is_12_ekxzgamDsfossgm
	jl	.L_small_initial_num_blocks_is_11_9_ekxzgamDsfossgm


	cmpq	$16,%r12
	je	.L_small_initial_num_blocks_is_16_ekxzgamDsfossgm
	cmpq	$15,%r12
	je	.L_small_initial_num_blocks_is_15_ekxzgamDsfossgm
	cmpq	$14,%r12
	je	.L_small_initial_num_blocks_is_14_ekxzgamDsfossgm
	jmp	.L_small_initial_num_blocks_is_13_ekxzgamDsfossgm

.L_small_initial_num_blocks_is_11_9_ekxzgamDsfossgm:

	cmpq	$11,%r12
	je	.L_small_initial_num_blocks_is_11_ekxzgamDsfossgm
	cmpq	$10,%r12
	je	.L_small_initial_num_blocks_is_10_ekxzgamDsfossgm
	jmp	.L_small_initial_num_blocks_is_9_ekxzgamDsfossgm

.L_small_initial_num_blocks_is_7_1_ekxzgamDsfossgm:
	cmpq	$4,%r12
	je	.L_small_initial_num_blocks_is_4_ekxzgamDsfossgm
	jl	.L_small_initial_num_blocks_is_3_1_ekxzgamDsfossgm

	cmpq	$7,%r12
	je	.L_small_initial_num_blocks_is_7_ekxzgamDsfossgm
	cmpq	$6,%r12
	je	.L_small_initial_num_blocks_is_6_ekxzgamDsfossgm
	jmp	.L_small_initial_num_blocks_is_5_ekxzgamDsfossgm

.L_small_initial_num_blocks_is_3_1_ekxzgamDsfossgm:

	cmpq	$3,%r12
	je	.L_small_initial_num_blocks_is_3_ekxzgamDsfossgm
	cmpq	$2,%r12
	je	.L_small_initial_num_blocks_is_2_ekxzgamDsfossgm





.L_small_initial_num_blocks_is_1_ekxzgamDsfossgm:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%r11,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm6,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_eGConwexrilzioh





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_eGConwexrilzioh
.L_small_initial_partial_block_eGConwexrilzioh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	.L_after_reduction_eGConwexrilzioh
.L_small_initial_compute_done_eGConwexrilzioh:
.L_after_reduction_eGConwexrilzioh:
	jmp	.L_small_initial_blocks_encrypted_ekxzgamDsfossgm
.L_small_initial_num_blocks_is_2_ekxzgamDsfossgm:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%r11,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm6,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zgFFlBdhpllnEef





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zgFFlBdhpllnEef
.L_small_initial_partial_block_zgFFlBdhpllnEef:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zgFFlBdhpllnEef:

	orq	%r8,%r8
	je	.L_after_reduction_zgFFlBdhpllnEef
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_zgFFlBdhpllnEef:
	jmp	.L_small_initial_blocks_encrypted_ekxzgamDsfossgm
.L_small_initial_num_blocks_is_3_ekxzgamDsfossgm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bpgkcGkxbdcvjqa





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bpgkcGkxbdcvjqa
.L_small_initial_partial_block_bpgkcGkxbdcvjqa:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bpgkcGkxbdcvjqa:

	orq	%r8,%r8
	je	.L_after_reduction_bpgkcGkxbdcvjqa
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_bpgkcGkxbdcvjqa:
	jmp	.L_small_initial_blocks_encrypted_ekxzgamDsfossgm
.L_small_initial_num_blocks_is_4_ekxzgamDsfossgm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%r11,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nGhDytspiDvAxkh





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nGhDytspiDvAxkh
.L_small_initial_partial_block_nGhDytspiDvAxkh:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nGhDytspiDvAxkh:

	orq	%r8,%r8
	je	.L_after_reduction_nGhDytspiDvAxkh
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_nGhDytspiDvAxkh:
	jmp	.L_small_initial_blocks_encrypted_ekxzgamDsfossgm
.L_small_initial_num_blocks_is_5_ekxzgamDsfossgm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%xmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%xmm29,%xmm7,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ygohGCifsokBwoi





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ygohGCifsokBwoi
.L_small_initial_partial_block_ygohGCifsokBwoi:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ygohGCifsokBwoi:

	orq	%r8,%r8
	je	.L_after_reduction_ygohGCifsokBwoi
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_ygohGCifsokBwoi:
	jmp	.L_small_initial_blocks_encrypted_ekxzgamDsfossgm
.L_small_initial_num_blocks_is_6_ekxzgamDsfossgm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%ymm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%ymm29,%ymm7,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yhmgoduFtfpDzwa





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yhmgoduFtfpDzwa
.L_small_initial_partial_block_yhmgoduFtfpDzwa:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yhmgoduFtfpDzwa:

	orq	%r8,%r8
	je	.L_after_reduction_yhmgoduFtfpDzwa
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_yhmgoduFtfpDzwa:
	jmp	.L_small_initial_blocks_encrypted_ekxzgamDsfossgm
.L_small_initial_num_blocks_is_7_ekxzgamDsfossgm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jxcfryjAkjuknzf





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jxcfryjAkjuknzf
.L_small_initial_partial_block_jxcfryjAkjuknzf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jxcfryjAkjuknzf:

	orq	%r8,%r8
	je	.L_after_reduction_jxcfryjAkjuknzf
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_jxcfryjAkjuknzf:
	jmp	.L_small_initial_blocks_encrypted_ekxzgamDsfossgm
.L_small_initial_num_blocks_is_8_ekxzgamDsfossgm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BFyftBCCtqbmhBa





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BFyftBCCtqbmhBa
.L_small_initial_partial_block_BFyftBCCtqbmhBa:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BFyftBCCtqbmhBa:

	orq	%r8,%r8
	je	.L_after_reduction_BFyftBCCtqbmhBa
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_BFyftBCCtqbmhBa:
	jmp	.L_small_initial_blocks_encrypted_ekxzgamDsfossgm
.L_small_initial_num_blocks_is_9_ekxzgamDsfossgm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%xmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%xmm29,%xmm10,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qaFxqDAAqwmlofz





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qaFxqDAAqwmlofz
.L_small_initial_partial_block_qaFxqDAAqwmlofz:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qaFxqDAAqwmlofz:

	orq	%r8,%r8
	je	.L_after_reduction_qaFxqDAAqwmlofz
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_qaFxqDAAqwmlofz:
	jmp	.L_small_initial_blocks_encrypted_ekxzgamDsfossgm
.L_small_initial_num_blocks_is_10_ekxzgamDsfossgm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%ymm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%ymm29,%ymm10,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_Amrntehyylndjze





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_Amrntehyylndjze
.L_small_initial_partial_block_Amrntehyylndjze:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_Amrntehyylndjze:

	orq	%r8,%r8
	je	.L_after_reduction_Amrntehyylndjze
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_Amrntehyylndjze:
	jmp	.L_small_initial_blocks_encrypted_ekxzgamDsfossgm
.L_small_initial_num_blocks_is_11_ekxzgamDsfossgm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_whBcqDaDwavtGnw





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_whBcqDaDwavtGnw
.L_small_initial_partial_block_whBcqDaDwavtGnw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_whBcqDaDwavtGnw:

	orq	%r8,%r8
	je	.L_after_reduction_whBcqDaDwavtGnw
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_whBcqDaDwavtGnw:
	jmp	.L_small_initial_blocks_encrypted_ekxzgamDsfossgm
.L_small_initial_num_blocks_is_12_ekxzgamDsfossgm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lyhcmgvmfBaerhw





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lyhcmgvmfBaerhw
.L_small_initial_partial_block_lyhcmgvmfBaerhw:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lyhcmgvmfBaerhw:

	orq	%r8,%r8
	je	.L_after_reduction_lyhcmgvmfBaerhw
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_lyhcmgvmfBaerhw:
	jmp	.L_small_initial_blocks_encrypted_ekxzgamDsfossgm
.L_small_initial_num_blocks_is_13_ekxzgamDsfossgm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%xmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%xmm29,%xmm11,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FDjFjanFnxBbyho





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FDjFjanFnxBbyho
.L_small_initial_partial_block_FDjFjanFnxBbyho:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	224(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	288(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FDjFjanFnxBbyho:

	orq	%r8,%r8
	je	.L_after_reduction_FDjFjanFnxBbyho
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_FDjFjanFnxBbyho:
	jmp	.L_small_initial_blocks_encrypted_ekxzgamDsfossgm
.L_small_initial_num_blocks_is_14_ekxzgamDsfossgm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%ymm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%ymm29,%ymm11,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rDddrvxkrBxfkcf





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rDddrvxkrBxfkcf
.L_small_initial_partial_block_rDddrvxkrBxfkcf:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	208(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	272(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	336(%rsi),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rDddrvxkrBxfkcf:

	orq	%r8,%r8
	je	.L_after_reduction_rDddrvxkrBxfkcf
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_rDddrvxkrBxfkcf:
	jmp	.L_small_initial_blocks_encrypted_ekxzgamDsfossgm
.L_small_initial_num_blocks_is_15_ekxzgamDsfossgm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lvdhbzAjawpfabl





	subq	$16,%r8
	movq	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lvdhbzAjawpfabl
.L_small_initial_partial_block_lvdhbzAjawpfabl:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	256(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	320(%rsi),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lvdhbzAjawpfabl:

	orq	%r8,%r8
	je	.L_after_reduction_lvdhbzAjawpfabl
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_lvdhbzAjawpfabl:
	jmp	.L_small_initial_blocks_encrypted_ekxzgamDsfossgm
.L_small_initial_num_blocks_is_16_ekxzgamDsfossgm:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%r11,1),%zmm6
	vmovdqu8	64(%rcx,%r11,1),%zmm7
	vmovdqu8	128(%rcx,%r11,1),%zmm10
	vmovdqu8	192(%rcx,%r11,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%r11,1)
	vmovdqu8	%zmm3,64(%r10,%r11,1)
	vmovdqu8	%zmm4,128(%r10,%r11,1)
	vmovdqu8	%zmm5,192(%r10,%r11,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_oqFpyyjuqEvBtax:








	movq	%r8,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	240(%rsi),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	304(%rsi),%ymm20
	vinserti64x2	$2,336(%rsi),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_oqFpyyjuqEvBtax:
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_oqFpyyjuqEvBtax:
.L_small_initial_blocks_encrypted_ekxzgamDsfossgm:
.L_ghash_done_cDwywBGkADyfvEf:
	vmovdqu64	%xmm2,0(%rsi)
	vmovdqu64	%xmm14,64(%rsi)
.L_enc_dec_done_cDwywBGkADyfvEf:
	jmp	.Lexit_gcm_decrypt
.Lexit_gcm_decrypt:
	cmpq	$256,%r8
	jbe	.Lskip_hkeys_cleanup_mBEdrmojAEkopnn
	vpxor	%xmm0,%xmm0,%xmm0
	vmovdqa64	%zmm0,0(%rsp)
	vmovdqa64	%zmm0,64(%rsp)
	vmovdqa64	%zmm0,128(%rsp)
	vmovdqa64	%zmm0,192(%rsp)
	vmovdqa64	%zmm0,256(%rsp)
	vmovdqa64	%zmm0,320(%rsp)
	vmovdqa64	%zmm0,384(%rsp)
	vmovdqa64	%zmm0,448(%rsp)
	vmovdqa64	%zmm0,512(%rsp)
	vmovdqa64	%zmm0,576(%rsp)
	vmovdqa64	%zmm0,640(%rsp)
	vmovdqa64	%zmm0,704(%rsp)
.Lskip_hkeys_cleanup_mBEdrmojAEkopnn:
	vzeroupper
	leaq	(%rbp),%rsp
.cfi_def_cfa_register	%rsp
	popq	%r15
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r15
	popq	%r14
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r14
	popq	%r13
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r13
	popq	%r12
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r12
	popq	%rbp
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbp
	popq	%rbx
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbx
	.byte	0xf3,0xc3
.Ldecrypt_seh_end:
.cfi_endproc	
.size	ossl_aes_gcm_decrypt_avx512, .-ossl_aes_gcm_decrypt_avx512
.globl	ossl_aes_gcm_finalize_avx512
.type	ossl_aes_gcm_finalize_avx512,@function
.align	32
ossl_aes_gcm_finalize_avx512:
.cfi_startproc	
.byte	243,15,30,250
	vmovdqu	336(%rdi),%xmm2
	vmovdqu	32(%rdi),%xmm3
	vmovdqu	64(%rdi),%xmm4


	cmpq	$0,%rsi
	je	.L_partial_done_iszBirFkxbjCfsd

	vpclmulqdq	$0x11,%xmm2,%xmm4,%xmm0
	vpclmulqdq	$0x00,%xmm2,%xmm4,%xmm16
	vpclmulqdq	$0x01,%xmm2,%xmm4,%xmm17
	vpclmulqdq	$0x10,%xmm2,%xmm4,%xmm4
	vpxorq	%xmm17,%xmm4,%xmm4

	vpsrldq	$8,%xmm4,%xmm17
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm17,%xmm0,%xmm0
	vpxorq	%xmm16,%xmm4,%xmm4



	vmovdqu64	POLY2(%rip),%xmm17

	vpclmulqdq	$0x01,%xmm4,%xmm17,%xmm16
	vpslldq	$8,%xmm16,%xmm16
	vpxorq	%xmm16,%xmm4,%xmm4



	vpclmulqdq	$0x00,%xmm4,%xmm17,%xmm16
	vpsrldq	$4,%xmm16,%xmm16
	vpclmulqdq	$0x10,%xmm4,%xmm17,%xmm4
	vpslldq	$4,%xmm4,%xmm4

	vpternlogq	$0x96,%xmm16,%xmm0,%xmm4

.L_partial_done_iszBirFkxbjCfsd:
	vmovq	56(%rdi),%xmm5
	vpinsrq	$1,48(%rdi),%xmm5,%xmm5
	vpsllq	$3,%xmm5,%xmm5

	vpxor	%xmm5,%xmm4,%xmm4

	vpclmulqdq	$0x11,%xmm2,%xmm4,%xmm0
	vpclmulqdq	$0x00,%xmm2,%xmm4,%xmm16
	vpclmulqdq	$0x01,%xmm2,%xmm4,%xmm17
	vpclmulqdq	$0x10,%xmm2,%xmm4,%xmm4
	vpxorq	%xmm17,%xmm4,%xmm4

	vpsrldq	$8,%xmm4,%xmm17
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm17,%xmm0,%xmm0
	vpxorq	%xmm16,%xmm4,%xmm4



	vmovdqu64	POLY2(%rip),%xmm17

	vpclmulqdq	$0x01,%xmm4,%xmm17,%xmm16
	vpslldq	$8,%xmm16,%xmm16
	vpxorq	%xmm16,%xmm4,%xmm4



	vpclmulqdq	$0x00,%xmm4,%xmm17,%xmm16
	vpsrldq	$4,%xmm16,%xmm16
	vpclmulqdq	$0x10,%xmm4,%xmm17,%xmm4
	vpslldq	$4,%xmm4,%xmm4

	vpternlogq	$0x96,%xmm16,%xmm0,%xmm4

	vpshufb	SHUF_MASK(%rip),%xmm4,%xmm4
	vpxor	%xmm4,%xmm3,%xmm3

.L_return_T_iszBirFkxbjCfsd:
	vmovdqu	%xmm3,64(%rdi)
.Labort_finalize:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	ossl_aes_gcm_finalize_avx512, .-ossl_aes_gcm_finalize_avx512
.globl	ossl_gcm_gmult_avx512
.hidden	ossl_gcm_gmult_avx512
.type	ossl_gcm_gmult_avx512,@function
.align	32
ossl_gcm_gmult_avx512:
.cfi_startproc	
.byte	243,15,30,250
	vmovdqu64	(%rdi),%xmm1
	vmovdqu64	336(%rsi),%xmm2

	vpclmulqdq	$0x11,%xmm2,%xmm1,%xmm3
	vpclmulqdq	$0x00,%xmm2,%xmm1,%xmm4
	vpclmulqdq	$0x01,%xmm2,%xmm1,%xmm5
	vpclmulqdq	$0x10,%xmm2,%xmm1,%xmm1
	vpxorq	%xmm5,%xmm1,%xmm1

	vpsrldq	$8,%xmm1,%xmm5
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm4,%xmm1,%xmm1



	vmovdqu64	POLY2(%rip),%xmm5

	vpclmulqdq	$0x01,%xmm1,%xmm5,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm1,%xmm1



	vpclmulqdq	$0x00,%xmm1,%xmm5,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm5,%xmm1
	vpslldq	$4,%xmm1,%xmm1

	vpternlogq	$0x96,%xmm4,%xmm3,%xmm1

	vmovdqu64	%xmm1,(%rdi)
	vzeroupper
.Labort_gmult:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	ossl_gcm_gmult_avx512, .-ossl_gcm_gmult_avx512
.data	
.align	16
POLY:.quad	0x0000000000000001, 0xC200000000000000

.align	64
POLY2:
.quad	0x00000001C2000000, 0xC200000000000000
.quad	0x00000001C2000000, 0xC200000000000000
.quad	0x00000001C2000000, 0xC200000000000000
.quad	0x00000001C2000000, 0xC200000000000000

.align	16
TWOONE:.quad	0x0000000000000001, 0x0000000100000000



.align	64
SHUF_MASK:
.quad	0x08090A0B0C0D0E0F, 0x0001020304050607
.quad	0x08090A0B0C0D0E0F, 0x0001020304050607
.quad	0x08090A0B0C0D0E0F, 0x0001020304050607
.quad	0x08090A0B0C0D0E0F, 0x0001020304050607

.align	16
SHIFT_MASK:
.quad	0x0706050403020100, 0x0f0e0d0c0b0a0908

ALL_F:
.quad	0xffffffffffffffff, 0xffffffffffffffff

ZERO:
.quad	0x0000000000000000, 0x0000000000000000

.align	16
ONE:
.quad	0x0000000000000001, 0x0000000000000000

.align	16
ONEf:
.quad	0x0000000000000000, 0x0100000000000000

.align	64
ddq_add_1234:
.quad	0x0000000000000001, 0x0000000000000000
.quad	0x0000000000000002, 0x0000000000000000
.quad	0x0000000000000003, 0x0000000000000000
.quad	0x0000000000000004, 0x0000000000000000

.align	64
ddq_add_5678:
.quad	0x0000000000000005, 0x0000000000000000
.quad	0x0000000000000006, 0x0000000000000000
.quad	0x0000000000000007, 0x0000000000000000
.quad	0x0000000000000008, 0x0000000000000000

.align	64
ddq_add_4444:
.quad	0x0000000000000004, 0x0000000000000000
.quad	0x0000000000000004, 0x0000000000000000
.quad	0x0000000000000004, 0x0000000000000000
.quad	0x0000000000000004, 0x0000000000000000

.align	64
ddq_add_8888:
.quad	0x0000000000000008, 0x0000000000000000
.quad	0x0000000000000008, 0x0000000000000000
.quad	0x0000000000000008, 0x0000000000000000
.quad	0x0000000000000008, 0x0000000000000000

.align	64
ddq_addbe_1234:
.quad	0x0000000000000000, 0x0100000000000000
.quad	0x0000000000000000, 0x0200000000000000
.quad	0x0000000000000000, 0x0300000000000000
.quad	0x0000000000000000, 0x0400000000000000

.align	64
ddq_addbe_4444:
.quad	0x0000000000000000, 0x0400000000000000
.quad	0x0000000000000000, 0x0400000000000000
.quad	0x0000000000000000, 0x0400000000000000
.quad	0x0000000000000000, 0x0400000000000000

.align	64
byte_len_to_mask_table:
.value	0x0000, 0x0001, 0x0003, 0x0007
.value	0x000f, 0x001f, 0x003f, 0x007f
.value	0x00ff, 0x01ff, 0x03ff, 0x07ff
.value	0x0fff, 0x1fff, 0x3fff, 0x7fff
.value	0xffff

.align	64
byte64_len_to_mask_table:
.quad	0x0000000000000000, 0x0000000000000001
.quad	0x0000000000000003, 0x0000000000000007
.quad	0x000000000000000f, 0x000000000000001f
.quad	0x000000000000003f, 0x000000000000007f
.quad	0x00000000000000ff, 0x00000000000001ff
.quad	0x00000000000003ff, 0x00000000000007ff
.quad	0x0000000000000fff, 0x0000000000001fff
.quad	0x0000000000003fff, 0x0000000000007fff
.quad	0x000000000000ffff, 0x000000000001ffff
.quad	0x000000000003ffff, 0x000000000007ffff
.quad	0x00000000000fffff, 0x00000000001fffff
.quad	0x00000000003fffff, 0x00000000007fffff
.quad	0x0000000000ffffff, 0x0000000001ffffff
.quad	0x0000000003ffffff, 0x0000000007ffffff
.quad	0x000000000fffffff, 0x000000001fffffff
.quad	0x000000003fffffff, 0x000000007fffffff
.quad	0x00000000ffffffff, 0x00000001ffffffff
.quad	0x00000003ffffffff, 0x00000007ffffffff
.quad	0x0000000fffffffff, 0x0000001fffffffff
.quad	0x0000003fffffffff, 0x0000007fffffffff
.quad	0x000000ffffffffff, 0x000001ffffffffff
.quad	0x000003ffffffffff, 0x000007ffffffffff
.quad	0x00000fffffffffff, 0x00001fffffffffff
.quad	0x00003fffffffffff, 0x00007fffffffffff
.quad	0x0000ffffffffffff, 0x0001ffffffffffff
.quad	0x0003ffffffffffff, 0x0007ffffffffffff
.quad	0x000fffffffffffff, 0x001fffffffffffff
.quad	0x003fffffffffffff, 0x007fffffffffffff
.quad	0x00ffffffffffffff, 0x01ffffffffffffff
.quad	0x03ffffffffffffff, 0x07ffffffffffffff
.quad	0x0fffffffffffffff, 0x1fffffffffffffff
.quad	0x3fffffffffffffff, 0x7fffffffffffffff
.quad	0xffffffffffffffff
	.section ".note.gnu.property", "a"
	.p2align 3
	.long 1f - 0f
	.long 4f - 1f
	.long 5
0:
	# "GNU" encoded with .byte, since .asciz isn't supported
	# on Solaris.
	.byte 0x47
	.byte 0x4e
	.byte 0x55
	.byte 0
1:
	.p2align 3
	.long 0xc0000002
	.long 3f - 2f
2:
	.long 3
3:
	.p2align 3
4:
